{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":""},{"location":"#getting-started-with-pyiceberg","title":"Getting started with PyIceberg","text":"<p>PyIceberg is a Python implementation for accessing Iceberg tables, without the need of a JVM.</p>"},{"location":"#installation","title":"Installation","text":"<p>Before installing PyIceberg, make sure that you're on an up-to-date version of <code>pip</code>:</p> <pre><code>pip install --upgrade pip\n</code></pre> <p>You can install the latest release version from pypi:</p> <pre><code>pip install \"pyiceberg[s3fs,hive]\"\n</code></pre> <p>You can mix and match optional dependencies depending on your needs:</p> Key Description: hive Support for the Hive metastore hive-kerberos Support for Hive metastore in Kerberos environment glue Support for AWS Glue dynamodb Support for AWS DynamoDB sql-postgres Support for SQL Catalog backed by Postgresql sql-sqlite Support for SQL Catalog backed by SQLite pyarrow PyArrow as a FileIO implementation to interact with the object store pandas Installs both PyArrow and Pandas duckdb Installs both PyArrow and DuckDB ray Installs PyArrow, Pandas, and Ray daft Installs Daft polars Installs Polars s3fs S3FS as a FileIO implementation to interact with the object store adlfs ADLFS as a FileIO implementation to interact with the object store snappy Support for snappy Avro compression gcsfs GCSFS as a FileIO implementation to interact with the object store rest-sigv4 Support for generating AWS SIGv4 authentication headers for REST Catalogs <p>You either need to install <code>s3fs</code>, <code>adlfs</code>, <code>gcsfs</code>, or <code>pyarrow</code> to be able to fetch files from an object store.</p>"},{"location":"#connecting-to-a-catalog","title":"Connecting to a catalog","text":"<p>Iceberg leverages the catalog to have one centralized place to organize the tables. This can be a traditional Hive catalog to store your Iceberg tables next to the rest, a vendor solution like the AWS Glue catalog, or an implementation of Icebergs' own REST protocol. Checkout the configuration page to find all the configuration details.</p> <p>For the sake of demonstration, we'll configure the catalog to use the <code>SqlCatalog</code> implementation, which will store information in a local <code>sqlite</code> database. We'll also configure the catalog to store data files in the local filesystem instead of an object store. This should not be used in production due to the limited scalability.</p> <p>Create a temporary location for Iceberg:</p> <pre><code>mkdir /tmp/warehouse\n</code></pre> <p>Open a Python 3 REPL to set up the catalog:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\nwarehouse_path = \"/tmp/warehouse\"\ncatalog = load_catalog(\n    \"default\",\n    **{\n        'type': 'sql',\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\",\n    },\n)\n</code></pre> <p>The <code>sql</code> catalog works for testing locally without needing another service. If you want to try out another catalog, please check out the configuration.</p>"},{"location":"#write-a-pyarrow-dataframe","title":"Write a PyArrow dataframe","text":"<p>Let's take the Taxi dataset, and write this to an Iceberg table.</p> <p>First download one month of data:</p> <pre><code>curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o /tmp/yellow_tripdata_2023-01.parquet\n</code></pre> <p>Load it into your PyArrow dataframe:</p> <pre><code>import pyarrow.parquet as pq\n\ndf = pq.read_table(\"/tmp/yellow_tripdata_2023-01.parquet\")\n</code></pre> <p>Create a new Iceberg table:</p> <pre><code>catalog.create_namespace(\"default\")\n\ntable = catalog.create_table(\n    \"default.taxi_dataset\",\n    schema=df.schema,\n)\n</code></pre> <p>Append the dataframe to the table:</p> <pre><code>table.append(df)\nlen(table.scan().to_arrow())\n</code></pre> <p>3066766 rows have been written to the table.</p> <p>Now generate a tip-per-mile feature to train the model on:</p> <pre><code>import pyarrow.compute as pc\n\ndf = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n</code></pre> <p>Evolve the schema of the table with the new column:</p> <pre><code>with table.update_schema() as update_schema:\n    update_schema.union_by_name(df.schema)\n</code></pre> <p>And now we can write the new dataframe to the Iceberg table:</p> <pre><code>table.overwrite(df)\nprint(table.scan().to_arrow())\n</code></pre> <p>And the new column is there:</p> <pre><code>taxi_dataset(\n  1: VendorID: optional long,\n  2: tpep_pickup_datetime: optional timestamp,\n  3: tpep_dropoff_datetime: optional timestamp,\n  4: passenger_count: optional double,\n  5: trip_distance: optional double,\n  6: RatecodeID: optional double,\n  7: store_and_fwd_flag: optional string,\n  8: PULocationID: optional long,\n  9: DOLocationID: optional long,\n  10: payment_type: optional long,\n  11: fare_amount: optional double,\n  12: extra: optional double,\n  13: mta_tax: optional double,\n  14: tip_amount: optional double,\n  15: tolls_amount: optional double,\n  16: improvement_surcharge: optional double,\n  17: total_amount: optional double,\n  18: congestion_surcharge: optional double,\n  19: airport_fee: optional double,\n  20: tip_per_mile: optional double\n),\n</code></pre> <p>And we can see that 2371784 rows have a tip-per-mile:</p> <pre><code>df = table.scan(row_filter=\"tip_per_mile &gt; 0\").to_arrow()\nlen(df)\n</code></pre>"},{"location":"#explore-iceberg-data-and-metadata-files","title":"Explore Iceberg data and metadata files","text":"<p>Since the catalog was configured to use the local filesystem, we can explore how Iceberg saved data and metadata files from the above operations.</p> <pre><code>find /tmp/warehouse/\n</code></pre>"},{"location":"#more-details","title":"More details","text":"<p>For the details, please check the CLI or Python API page.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":""},{"location":"SUMMARY/#summary","title":"Summary","text":"<ul> <li>Getting started</li> <li>Configuration</li> <li>CLI</li> <li>API</li> <li>Contributing</li> <li>Community</li> <li>Releases<ul> <li>Verify a release</li> <li>How to release</li> <li>Release Notes</li> <li>Nightly Build</li> </ul> </li> <li>Code Reference</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#python-api","title":"Python API","text":"<p>PyIceberg is based around catalogs to load tables. First step is to instantiate a catalog that loads tables. Let's use the following configuration to define a catalog called <code>prod</code>:</p> <pre><code>catalog:\n  prod:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n</code></pre> <p>Note that multiple catalogs can be defined in the same <code>.pyiceberg.yaml</code>:</p> <pre><code>catalog:\n  hive:\n    uri: thrift://127.0.0.1:9083\n    s3.endpoint: http://127.0.0.1:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n  rest:\n    uri: https://rest-server:8181/\n    warehouse: my-warehouse\n</code></pre> <p>and loaded in python by calling <code>load_catalog(name=\"hive\")</code> and <code>load_catalog(name=\"rest\")</code>.</p> <p>This information must be placed inside a file called <code>.pyiceberg.yaml</code> located either in the <code>$HOME</code> or <code>%USERPROFILE%</code> directory (depending on whether the operating system is Unix-based or Windows-based, respectively), in the current working directory, or in the <code>$PYICEBERG_HOME</code> directory (if the corresponding environment variable is set).</p> <p>For more details on possible configurations refer to the specific page.</p> <p>Then load the <code>prod</code> catalog:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\n    \"docs\",\n    **{\n        \"uri\": \"http://127.0.0.1:8181\",\n        \"s3.endpoint\": \"http://127.0.0.1:9000\",\n        \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n        \"s3.access-key-id\": \"admin\",\n        \"s3.secret-access-key\": \"password\",\n    }\n)\n</code></pre> <p>Let's create a namespace:</p> <pre><code>catalog.create_namespace(\"docs_example\")\n</code></pre> <p>And then list them:</p> <pre><code>ns = catalog.list_namespaces()\n\nassert ns == [(\"docs_example\",)]\n</code></pre> <p>And then list tables in the namespace:</p> <pre><code>catalog.list_tables(\"docs_example\")\n</code></pre>"},{"location":"api/#create-a-table","title":"Create a table","text":"<p>To create a table from a catalog:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import (\n    TimestampType,\n    FloatType,\n    DoubleType,\n    StringType,\n    NestedField,\n    StructType,\n)\n\nschema = Schema(\n    NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),\n    NestedField(field_id=2, name=\"symbol\", field_type=StringType(), required=True),\n    NestedField(field_id=3, name=\"bid\", field_type=FloatType(), required=False),\n    NestedField(field_id=4, name=\"ask\", field_type=DoubleType(), required=False),\n    NestedField(\n        field_id=5,\n        name=\"details\",\n        field_type=StructType(\n            NestedField(\n                field_id=4, name=\"created_by\", field_type=StringType(), required=False\n            ),\n        ),\n        required=False,\n    ),\n)\n\nfrom pyiceberg.partitioning import PartitionSpec, PartitionField\nfrom pyiceberg.transforms import DayTransform\n\npartition_spec = PartitionSpec(\n    PartitionField(\n        source_id=1, field_id=1000, transform=DayTransform(), name=\"datetime_day\"\n    )\n)\n\nfrom pyiceberg.table.sorting import SortOrder, SortField\nfrom pyiceberg.transforms import IdentityTransform\n\n# Sort on the symbol\nsort_order = SortOrder(SortField(source_id=2, transform=IdentityTransform()))\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n    location=\"s3://pyiceberg\",\n    partition_spec=partition_spec,\n    sort_order=sort_order,\n)\n</code></pre> <p>When the table is created, all IDs in the schema are re-assigned to ensure uniqueness.</p> <p>To create a table using a pyarrow schema:</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"foo\", pa.string(), nullable=True),\n        pa.field(\"bar\", pa.int32(), nullable=False),\n        pa.field(\"baz\", pa.bool_(), nullable=True),\n    ]\n)\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n)\n</code></pre> <p>To create a table with some subsequent changes atomically in a transaction:</p> <pre><code>with catalog.create_table_transaction(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n    location=\"s3://pyiceberg\",\n    partition_spec=partition_spec,\n    sort_order=sort_order,\n) as txn:\n    with txn.update_schema() as update_schema:\n        update_schema.add_column(path=\"new_column\", field_type=StringType())\n\n    with txn.update_spec() as update_spec:\n        update_spec.add_identity(\"symbol\")\n\n    txn.set_properties(test_a=\"test_aa\", test_b=\"test_b\", test_c=\"test_c\")\n</code></pre>"},{"location":"api/#load-a-table","title":"Load a table","text":""},{"location":"api/#catalog-table","title":"Catalog table","text":"<p>Loading the <code>bids</code> table:</p> <pre><code>table = catalog.load_table(\"docs_example.bids\")\n# Equivalent to:\ntable = catalog.load_table((\"docs_example\", \"bids\"))\n# The tuple syntax can be used if the namespace or table contains a dot.\n</code></pre> <p>This returns a <code>Table</code> that represents an Iceberg table that can be queried and altered.</p>"},{"location":"api/#static-table","title":"Static table","text":"<p>To load a table directly from a metadata file (i.e., without using a catalog), you can use a <code>StaticTable</code> as follows:</p> <pre><code>from pyiceberg.table import StaticTable\n\nstatic_table = StaticTable.from_metadata(\n    \"s3://warehouse/wh/nyc.db/taxis/metadata/00002-6ea51ce3-62aa-4197-9cf8-43d07c3440ca.metadata.json\"\n)\n</code></pre> <p>The static-table is considered read-only.</p>"},{"location":"api/#check-if-a-table-exists","title":"Check if a table exists","text":"<p>To check whether the <code>bids</code> table exists:</p> <pre><code>catalog.table_exists(\"docs_example.bids\")\n</code></pre> <p>Returns <code>True</code> if the table already exists.</p>"},{"location":"api/#write-support","title":"Write support","text":"<p>With PyIceberg 0.6.0 write support is added through Arrow. Let's consider an Arrow Table:</p> <pre><code>import pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\n</code></pre> <p>Next, create a table based on the schema:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n</code></pre> <p>Now write the data to the table:</p> <p>Fast append</p> <p>PyIceberg default to the fast append to minimize the amount of data written. This enables quick writes, reducing the possibility of conflicts. The downside of the fast append is that it creates more metadata than a normal commit. Compaction is planned and will automatically rewrite all the metadata when a threshold is hit, to maintain performant reads.</p> <pre><code>tbl.append(df)\n\n# or\n\ntbl.overwrite(df)\n</code></pre> <p>The data is written to the table, and when the table is read using <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\",\"Paris\"]]\nlat: [[52.371807,37.773972,53.11254,48.864716]]\nlong: [[4.896029,-122.431297,6.0989,2.349014]]\n</code></pre> <p>You both can use <code>append(df)</code> or <code>overwrite(df)</code> since there is no data yet. If we want to add more data, we can use <code>.append()</code> again:</p> <pre><code>df = pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}],\n)\n\ntbl.append(df)\n</code></pre> <p>When reading the table <code>tbl.scan().to_arrow()</code> you can see that <code>Groningen</code> is now also part of the table:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\",\"Paris\"],[\"Groningen\"]]\nlat: [[52.371807,37.773972,53.11254,48.864716],[53.21917]]\nlong: [[4.896029,-122.431297,6.0989,2.349014],[6.56667]]\n</code></pre> <p>The nested lists indicate the different Arrow buffers, where the first write results into a buffer, and the second append in a separate buffer. This is expected since it will read two parquet files.</p> <p>To avoid any type errors during writing, you can enforce the PyArrow table types using the Iceberg table schema:</p> <pre><code>from pyiceberg.catalog import load_catalog\nimport pyarrow as pa\n\ncatalog = load_catalog(\"default\")\ntable = catalog.load_table(\"default.cities\")\nschema = table.schema().as_arrow()\n\ndf = pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}], schema=schema\n)\n\ntable.append(df)\n</code></pre> <p>You can delete some of the data from the table by calling <code>tbl.delete()</code> with a desired <code>delete_filter</code>.</p> <pre><code>tbl.delete(delete_filter=\"city == 'Paris'\")\n</code></pre> <p>In the above example, any records where the city field value equals to <code>Paris</code> will be deleted. Running <code>tbl.scan().to_arrow()</code> will now yield:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\"],[\"Groningen\"]]\nlat: [[52.371807,37.773972,53.11254],[53.21917]]\nlong: [[4.896029,-122.431297,6.0989],[6.56667]]\n</code></pre>"},{"location":"api/#partial-overwrites","title":"Partial overwrites","text":"<p>When using the <code>overwrite</code> API, you can use an <code>overwrite_filter</code> to delete data that matches the filter before appending new data into the table.</p> <p>For example, with an iceberg table created as:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\n\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n</code></pre> <p>And with initial data populating the table:</p> <pre><code>import pyarrow as pa\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\ntbl.append(df)\n</code></pre> <p>You can overwrite the record of <code>Paris</code> with a record of <code>New York</code>:</p> <pre><code>from pyiceberg.expressions import EqualTo\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"New York\", \"lat\": 40.7128, \"long\": 74.0060},\n    ]\n)\ntbl.overwrite(df, overwrite_filter=EqualTo('city', \"Paris\"))\n</code></pre> <p>This produces the following result with <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: large_string\nlat: double\nlong: double\n----\ncity: [[\"New York\"],[\"Amsterdam\",\"San Francisco\",\"Drachten\"]]\nlat: [[40.7128],[52.371807,37.773972,53.11254]]\nlong: [[74.006],[4.896029,-122.431297,6.0989]]\n</code></pre> <p>If the PyIceberg table is partitioned, you can use <code>tbl.dynamic_partition_overwrite(df)</code> to replace the existing partitions with new ones provided in the dataframe. The partitions to be replaced are detected automatically from the provided arrow table. For example, with an iceberg table with a partition specified on <code>\"city\"</code> field:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import DoubleType, NestedField, StringType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\n    \"default.cities\",\n    schema=schema,\n    partition_spec=PartitionSpec(PartitionField(source_id=1, field_id=1001, transform=IdentityTransform(), name=\"city_identity\"))\n)\n</code></pre> <p>And we want to overwrite the data for the partition of <code>\"Paris\"</code>:</p> <pre><code>import pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": -48.864716, \"long\": -2.349014},\n    ],\n)\ntbl.append(df)\n</code></pre> <p>Then we can call <code>dynamic_partition_overwrite</code> with this arrow table:</p> <pre><code>df_corrected = pa.Table.from_pylist([\n    {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014}\n])\ntbl.dynamic_partition_overwrite(df_corrected)\n</code></pre> <p>This produces the following result with <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: large_string\nlat: double\nlong: double\n----\ncity: [[\"Paris\"],[\"Amsterdam\"],[\"Drachten\"],[\"San Francisco\"]]\nlat: [[48.864716],[52.371807],[53.11254],[37.773972]]\nlong: [[2.349014],[4.896029],[6.0989],[-122.431297]]\n</code></pre>"},{"location":"api/#upsert","title":"Upsert","text":"<p>PyIceberg supports upsert operations, meaning that it is able to merge an Arrow table into an Iceberg table. Rows are considered the same based on the identifier field. If a row is already in the table, it will update that row. If a row cannot be found, it will insert that new row.</p> <p>Consider the following table, with some data:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import IntegerType, NestedField, StringType\n\nimport pyarrow as pa\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=True),\n    NestedField(2, \"inhabitants\", IntegerType(), required=True),\n    # Mark City as the identifier field, also known as the primary-key\n    identifier_field_ids=[1]\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n\narrow_schema = pa.schema(\n    [\n        pa.field(\"city\", pa.string(), nullable=False),\n        pa.field(\"inhabitants\", pa.int32(), nullable=False),\n    ]\n)\n\n# Write some data\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"inhabitants\": 921402},\n        {\"city\": \"San Francisco\", \"inhabitants\": 808988},\n        {\"city\": \"Drachten\", \"inhabitants\": 45019},\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\ntbl.append(df)\n</code></pre> <p>Next, we'll upsert a table into the Iceberg table:</p> <pre><code>df = pa.Table.from_pylist(\n    [\n        # Will be updated, the inhabitants has been updated\n        {\"city\": \"Drachten\", \"inhabitants\": 45505},\n\n        # New row, will be inserted\n        {\"city\": \"Berlin\", \"inhabitants\": 3432000},\n\n        # Ignored, already exists in the table\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\nupd = tbl.upsert(df)\n\nassert upd.rows_updated == 1\nassert upd.rows_inserted == 1\n</code></pre> <p>PyIceberg will automatically detect which rows need to be updated, inserted or can simply be ignored.</p>"},{"location":"api/#inspecting-tables","title":"Inspecting tables","text":"<p>To explore the table metadata, tables can be inspected.</p> <p>Time Travel</p> <p>To inspect a tables's metadata with the time travel feature, call the inspect table method with the <code>snapshot_id</code> argument. Time travel is supported on all metadata tables except <code>snapshots</code> and <code>refs</code>. <pre><code>table.inspect.entries(snapshot_id=805611270568163028)\n</code></pre></p>"},{"location":"api/#snapshots","title":"Snapshots","text":"<p>Inspect the snapshots of the table:</p> <pre><code>table.inspect.snapshots()\n</code></pre> <pre><code>pyarrow.Table\ncommitted_at: timestamp[ms] not null\nsnapshot_id: int64 not null\nparent_id: int64\noperation: string\nmanifest_list: string not null\nsummary: map&lt;string, string&gt;\n  child 0, entries: struct&lt;key: string not null, value: string&gt; not null\n      child 0, key: string not null\n      child 1, value: string\n----\ncommitted_at: [[2024-03-15 15:01:25.682,2024-03-15 15:01:25.730,2024-03-15 15:01:25.772]]\nsnapshot_id: [[805611270568163028,3679426539959220963,5588071473139865870]]\nparent_id: [[null,805611270568163028,3679426539959220963]]\noperation: [[\"append\",\"overwrite\",\"append\"]]\nmanifest_list: [[\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-805611270568163028-0-43637daf-ea4b-4ceb-b096-a60c25481eb5.avro\",\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-3679426539959220963-0-8be81019-adf1-4bb6-a127-e15217bd50b3.avro\",\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-5588071473139865870-0-1382dd7e-5fbc-4c51-9776-a832d7d0984e.avro\"]]\nsummary: [[keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-delete-files\",\"total-records\",\"total-files-size\",\"total-position-deletes\",\"total-equality-deletes\"]values:[\"5459\",\"1\",\"3\",\"1\",\"0\",\"3\",\"5459\",\"0\",\"0\"],keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-records\",...,\"total-equality-deletes\",\"total-files-size\",\"deleted-data-files\",\"deleted-records\",\"removed-files-size\"]values:[\"5459\",\"1\",\"3\",\"1\",\"3\",...,\"0\",\"5459\",\"1\",\"3\",\"5459\"],keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-delete-files\",\"total-records\",\"total-files-size\",\"total-position-deletes\",\"total-equality-deletes\"]values:[\"5459\",\"1\",\"3\",\"2\",\"0\",\"6\",\"10918\",\"0\",\"0\"]]]\n</code></pre>"},{"location":"api/#partitions","title":"Partitions","text":"<p>Inspect the partitions of the table:</p> <pre><code>table.inspect.partitions()\n</code></pre> <pre><code>pyarrow.Table\npartition: struct&lt;dt_month: int32, dt_day: date32[day]&gt; not null\n  child 0, dt_month: int32\n  child 1, dt_day: date32[day]\nspec_id: int32 not null\nrecord_count: int64 not null\nfile_count: int32 not null\ntotal_data_file_size_in_bytes: int64 not null\nposition_delete_record_count: int64 not null\nposition_delete_file_count: int32 not null\nequality_delete_record_count: int64 not null\nequality_delete_file_count: int32 not null\nlast_updated_at: timestamp[ms]\nlast_updated_snapshot_id: int64\n----\npartition: [\n  -- is_valid: all not null\n  -- child 0 type: int32\n[null,null,612]\n  -- child 1 type: date32[day]\n[null,2021-02-01,null]]\nspec_id: [[2,1,0]]\nrecord_count: [[1,1,2]]\nfile_count: [[1,1,2]]\ntotal_data_file_size_in_bytes: [[641,641,1260]]\nposition_delete_record_count: [[0,0,0]]\nposition_delete_file_count: [[0,0,0]]\nequality_delete_record_count: [[0,0,0]]\nequality_delete_file_count: [[0,0,0]]\nlast_updated_at: [[2024-04-13 18:59:35.981,2024-04-13 18:59:35.465,2024-04-13 18:59:35.003]]\n</code></pre>"},{"location":"api/#entries","title":"Entries","text":"<p>To show all the table's current manifest entries for both data and delete files.</p> <pre><code>table.inspect.entries()\n</code></pre> <pre><code>pyarrow.Table\nstatus: int8 not null\nsnapshot_id: int64 not null\nsequence_number: int64 not null\nfile_sequence_number: int64 not null\ndata_file: struct&lt;content: int8 not null, file_path: string not null, file_format: string not null, partition: struct&lt;&gt; not null, record_count: int64 not null, file_size_in_bytes: int64 not null, column_sizes: map&lt;int32, int64&gt;, value_counts: map&lt;int32, int64&gt;, null_value_counts: map&lt;int32, int64&gt;, nan_value_counts: map&lt;int32, int64&gt;, lower_bounds: map&lt;int32, binary&gt;, upper_bounds: map&lt;int32, binary&gt;, key_metadata: binary, split_offsets: list&lt;item: int64&gt;, equality_ids: list&lt;item: int32&gt;, sort_order_id: int32&gt; not null\n  child 0, content: int8 not null\n  child 1, file_path: string not null\n  child 2, file_format: string not null\n  child 3, partition: struct&lt;&gt; not null\n  child 4, record_count: int64 not null\n  child 5, file_size_in_bytes: int64 not null\n  child 6, column_sizes: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 7, value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 8, null_value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 9, nan_value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 10, lower_bounds: map&lt;int32, binary&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n          child 0, key: int32 not null\n          child 1, value: binary\n  child 11, upper_bounds: map&lt;int32, binary&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n          child 0, key: int32 not null\n          child 1, value: binary\n  child 12, key_metadata: binary\n  child 13, split_offsets: list&lt;item: int64&gt;\n      child 0, item: int64\n  child 14, equality_ids: list&lt;item: int32&gt;\n      child 0, item: int32\n  child 15, sort_order_id: int32\nreadable_metrics: struct&lt;city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null&gt;\n  child 0, city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: string\n      child 5, upper_bound: string\n  child 1, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n  child 2, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n----\nstatus: [[1]]\nsnapshot_id: [[6245626162224016531]]\nsequence_number: [[1]]\nfile_sequence_number: [[1]]\ndata_file: [\n  -- is_valid: all not null\n  -- child 0 type: int8\n[0]\n  -- child 1 type: string\n[\"s3://warehouse/default/cities/data/00000-0-80766b66-e558-4150-a5cf-85e4c609b9fe.parquet\"]\n  -- child 2 type: string\n[\"PARQUET\"]\n  -- child 3 type: struct&lt;&gt;\n    -- is_valid: all not null\n  -- child 4 type: int64\n[4]\n  -- child 5 type: int64\n[1656]\n  -- child 6 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[140,135,135]]\n  -- child 7 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[4,4,4]]\n  -- child 8 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[0,0,0]]\n  -- child 9 type: map&lt;int32, int64&gt;\n[keys:[]values:[]]\n  -- child 10 type: map&lt;int32, binary&gt;\n[keys:[1,2,3]values:[416D7374657264616D,8602B68311E34240,3A77BB5E9A9B5EC0]]\n  -- child 11 type: map&lt;int32, binary&gt;\n[keys:[1,2,3]values:[53616E204672616E636973636F,F5BEF1B5678E4A40,304CA60A46651840]]\n  -- child 12 type: binary\n[null]\n  -- child 13 type: list&lt;item: int64&gt;\n[[4]]\n  -- child 14 type: list&lt;item: int32&gt;\n[null]\n  -- child 15 type: int32\n[null]]\nreadable_metrics: [\n  -- is_valid: all not null\n  -- child 0 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[140]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: string\n[\"Amsterdam\"]\n    -- child 5 type: string\n[\"San Francisco\"]\n  -- child 1 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[37.773972]\n    -- child 5 type: double\n[53.11254]\n  -- child 2 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[-122.431297]\n    -- child 5 type: double\n[6.0989]]\n</code></pre>"},{"location":"api/#references","title":"References","text":"<p>To show a table's known snapshot references:</p> <pre><code>table.inspect.refs()\n</code></pre> <pre><code>pyarrow.Table\nname: string not null\ntype: string not null\nsnapshot_id: int64 not null\nmax_reference_age_in_ms: int64\nmin_snapshots_to_keep: int32\nmax_snapshot_age_in_ms: int64\n----\nname: [[\"main\",\"testTag\"]]\ntype: [[\"BRANCH\",\"TAG\"]]\nsnapshot_id: [[2278002651076891950,2278002651076891950]]\nmax_reference_age_in_ms: [[null,604800000]]\nmin_snapshots_to_keep: [[null,10]]\nmax_snapshot_age_in_ms: [[null,604800000]]\n</code></pre>"},{"location":"api/#manifests","title":"Manifests","text":"<p>To show a table's current file manifests:</p> <pre><code>table.inspect.manifests()\n</code></pre> <pre><code>pyarrow.Table\ncontent: int8 not null\npath: string not null\nlength: int64 not null\npartition_spec_id: int32 not null\nadded_snapshot_id: int64 not null\nadded_data_files_count: int32 not null\nexisting_data_files_count: int32 not null\ndeleted_data_files_count: int32 not null\nadded_delete_files_count: int32 not null\nexisting_delete_files_count: int32 not null\ndeleted_delete_files_count: int32 not null\npartition_summaries: list&lt;item: struct&lt;contains_null: bool not null, contains_nan: bool, lower_bound: string, upper_bound: string&gt;&gt; not null\n  child 0, item: struct&lt;contains_null: bool not null, contains_nan: bool, lower_bound: string, upper_bound: string&gt;\n      child 0, contains_null: bool not null\n      child 1, contains_nan: bool\n      child 2, lower_bound: string\n      child 3, upper_bound: string\n----\ncontent: [[0]]\npath: [[\"s3://warehouse/default/table_metadata_manifests/metadata/3bf5b4c6-a7a4-4b43-a6ce-ca2b4887945a-m0.avro\"]]\nlength: [[6886]]\npartition_spec_id: [[0]]\nadded_snapshot_id: [[3815834705531553721]]\nadded_data_files_count: [[1]]\nexisting_data_files_count: [[0]]\ndeleted_data_files_count: [[0]]\nadded_delete_files_count: [[0]]\nexisting_delete_files_count: [[0]]\ndeleted_delete_files_count: [[0]]\npartition_summaries: [[    -- is_valid: all not null\n    -- child 0 type: bool\n[false]\n    -- child 1 type: bool\n[false]\n    -- child 2 type: string\n[\"test\"]\n    -- child 3 type: string\n[\"test\"]]]\n</code></pre>"},{"location":"api/#metadata-log-entries","title":"Metadata Log Entries","text":"<p>To show table metadata log entries:</p> <pre><code>table.inspect.metadata_log_entries()\n</code></pre> <pre><code>pyarrow.Table\ntimestamp: timestamp[ms] not null\nfile: string not null\nlatest_snapshot_id: int64\nlatest_schema_id: int32\nlatest_sequence_number: int64\n----\ntimestamp: [[2024-04-28 17:03:00.214,2024-04-28 17:03:00.352,2024-04-28 17:03:00.445,2024-04-28 17:03:00.498]]\nfile: [[\"s3://warehouse/default/table_metadata_log_entries/metadata/00000-0b3b643b-0f3a-4787-83ad-601ba57b7319.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00001-f74e4b2c-0f89-4f55-822d-23d099fd7d54.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00002-97e31507-e4d9-4438-aff1-3c0c5304d271.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00003-6c8b7033-6ad8-4fe4-b64d-d70381aeaddc.metadata.json\"]]\nlatest_snapshot_id: [[null,3958871664825505738,1289234307021405706,7640277914614648349]]\nlatest_schema_id: [[null,0,0,0]]\nlatest_sequence_number: [[null,0,0,0]]\n</code></pre>"},{"location":"api/#history","title":"History","text":"<p>To show a table's history:</p> <pre><code>table.inspect.history()\n</code></pre> <pre><code>pyarrow.Table\nmade_current_at: timestamp[ms] not null\nsnapshot_id: int64 not null\nparent_id: int64\nis_current_ancestor: bool not null\n----\nmade_current_at: [[2024-06-18 16:17:48.768,2024-06-18 16:17:49.240,2024-06-18 16:17:49.343,2024-06-18 16:17:49.511]]\nsnapshot_id: [[4358109269873137077,3380769165026943338,4358109269873137077,3089420140651211776]]\nparent_id: [[null,4358109269873137077,null,4358109269873137077]]\nis_current_ancestor: [[true,false,true,true]]\n</code></pre>"},{"location":"api/#files","title":"Files","text":"<p>Inspect the data files in the current snapshot of the table:</p> <pre><code>table.inspect.files()\n</code></pre> <pre><code>pyarrow.Table\ncontent: int8 not null\nfile_path: string not null\nfile_format: dictionary&lt;values=string, indices=int32, ordered=0&gt; not null\nspec_id: int32 not null\nrecord_count: int64 not null\nfile_size_in_bytes: int64 not null\ncolumn_sizes: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nvalue_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nnull_value_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nnan_value_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nlower_bounds: map&lt;int32, binary&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n      child 0, key: int32 not null\n      child 1, value: binary\nupper_bounds: map&lt;int32, binary&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n      child 0, key: int32 not null\n      child 1, value: binary\nkey_metadata: binary\nsplit_offsets: list&lt;item: int64&gt;\n  child 0, item: int64\nequality_ids: list&lt;item: int32&gt;\n  child 0, item: int32\nsort_order_id: int32\nreadable_metrics: struct&lt;city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: large_string, upper_bound: large_string&gt; not null, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null&gt;\n  child 0, city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: large_string\n      child 5, upper_bound: large_string\n  child 1, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n  child 2, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n----\ncontent: [[0,0]]\nfile_path: [[\"s3://warehouse/default/table_metadata_files/data/00000-0-9ea7d222-6457-467f-bad5-6fb125c9aa5f.parquet\",\"s3://warehouse/default/table_metadata_files/data/00000-0-afa8893c-de71-4710-97c9-6b01590d0c44.parquet\"]]\nfile_format: [[\"PARQUET\",\"PARQUET\"]]\nspec_id: [[0,0]]\nrecord_count: [[3,3]]\nfile_size_in_bytes: [[5459,5459]]\ncolumn_sizes: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[49,78,128,94,118,...,118,118,94,78,109],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[49,78,128,94,118,...,118,118,94,78,109]]]\nvalue_counts: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[3,3,3,3,3,...,3,3,3,3,3],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[3,3,3,3,3,...,3,3,3,3,3]]]\nnull_value_counts: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[1,1,1,1,1,...,1,1,1,1,1],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[1,1,1,1,1,...,1,1,1,1,1]]]\nnan_value_counts: [[keys:[]values:[],keys:[]values:[]]]\nlower_bounds: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000]]]\nupper_bounds:[[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000]]]\nkey_metadata: [[0100,0100]]\nsplit_offsets:[[[],[]]]\nequality_ids:[[[],[]]]\nsort_order_id:[[[],[]]]\nreadable_metrics: [\n  -- is_valid: all not null\n  -- child 0 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: large_string, upper_bound: large_string&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[140]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: large_string\n[\"Amsterdam\"]\n    -- child 5 type: large_string\n[\"San Francisco\"]\n  -- child 1 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[37.773972]\n    -- child 5 type: double\n[53.11254]\n  -- child 2 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[-122.431297]\n    -- child 5 type: double\n[6.0989]]\n</code></pre> <p>Info</p> <p>Content refers to type of content stored by the data file: <code>0</code> - <code>Data</code>, <code>1</code> - <code>Position Deletes</code>, <code>2</code> - <code>Equality Deletes</code></p> <p>To show only data files or delete files in the current snapshot, use <code>table.inspect.data_files()</code> and <code>table.inspect.delete_files()</code> respectively.</p>"},{"location":"api/#add-files","title":"Add Files","text":"<p>Expert Iceberg users may choose to commit existing parquet files to the Iceberg table as data files, without rewriting them.</p> <pre><code># Given that these parquet files have schema consistent with the Iceberg table\n\nfile_paths = [\n    \"s3a://warehouse/default/existing-1.parquet\",\n    \"s3a://warehouse/default/existing-2.parquet\",\n]\n\n# They can be added to the table without rewriting them\n\ntbl.add_files(file_paths=file_paths)\n\n# A new snapshot is committed to the table with manifests pointing to the existing parquet files\n</code></pre> <p>Name Mapping</p> <p>Because <code>add_files</code> uses existing files without writing new parquet files that are aware of the Iceberg's schema, it requires the Iceberg's table to have a Name Mapping (The Name mapping maps the field names within the parquet files to the Iceberg field IDs). Hence, <code>add_files</code> requires that there are no field IDs in the parquet file's metadata, and creates a new Name Mapping based on the table's current schema if the table doesn't already have one.</p> <p>Partitions</p> <p><code>add_files</code> only requires the client to read the existing parquet files' metadata footer to infer the partition value of each file. This implementation also supports adding files to Iceberg tables with partition transforms like <code>MonthTransform</code>, and <code>TruncateTransform</code> which preserve the order of the values after the transformation (Any Transform that has the <code>preserves_order</code> property set to True is supported). Please note that if the column statistics of the <code>PartitionField</code>'s source column are not present in the parquet metadata, the partition value is inferred as <code>None</code>.</p> <p>Maintenance Operations</p> <p>Because <code>add_files</code> commits the existing parquet files to the Iceberg Table as any other data file, destructive maintenance operations like expiring snapshots will remove them.</p>"},{"location":"api/#schema-evolution","title":"Schema evolution","text":"<p>PyIceberg supports full schema evolution through the Python API. It takes care of setting the field-IDs and makes sure that only non-breaking changes are done (can be overridden).</p> <p>In the examples below, the <code>.update_schema()</code> is called from the table itself.</p> <pre><code>with table.update_schema() as update:\n    update.add_column(\"some_field\", IntegerType(), \"doc\")\n</code></pre> <p>You can also initiate a transaction if you want to make more changes than just evolving the schema:</p> <pre><code>with table.transaction() as transaction:\n    with transaction.update_schema() as update_schema:\n        update.add_column(\"some_other_field\", IntegerType(), \"doc\")\n    # ... Update properties etc\n</code></pre>"},{"location":"api/#union-by-name","title":"Union by Name","text":"<p>Using <code>.union_by_name()</code> you can merge another schema into an existing schema without having to worry about field-IDs:</p> <pre><code>from pyiceberg.catalog import load_catalog\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType, LongType\n\ncatalog = load_catalog()\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntable = catalog.create_table(\"default.locations\", schema)\n\nnew_schema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n    NestedField(10, \"population\", LongType(), required=False),\n)\n\nwith table.update_schema() as update:\n    update.union_by_name(new_schema)\n</code></pre> <p>Now the table has the union of the two schemas <code>print(table.schema())</code>:</p> <pre><code>table {\n  1: city: optional string\n  2: lat: optional double\n  3: long: optional double\n  4: population: optional long\n}\n</code></pre>"},{"location":"api/#add-column","title":"Add column","text":"<p>Using <code>add_column</code> you can add a column, without having to worry about the field-id:</p> <pre><code>with table.update_schema() as update:\n    update.add_column(\"retries\", IntegerType(), \"Number of retries to place the bid\")\n    # In a struct\n    update.add_column(\"details\", StructType())\n\nwith table.update_schema() as update:\n    update.add_column((\"details\", \"confirmed_by\"), StringType(), \"Name of the exchange\")\n</code></pre> <p>A complex type must exist before columns can be added to it. Fields in complex types are added in a tuple.</p>"},{"location":"api/#rename-column","title":"Rename column","text":"<p>Renaming a field in an Iceberg table is simple:</p> <pre><code>with table.update_schema() as update:\n    update.rename_column(\"retries\", \"num_retries\")\n    # This will rename `confirmed_by` to `processed_by` in the `details` struct\n    update.rename_column((\"details\", \"confirmed_by\"), \"processed_by\")\n</code></pre>"},{"location":"api/#move-column","title":"Move column","text":"<p>Move order of fields:</p> <pre><code>with table.update_schema() as update:\n    update.move_first(\"symbol\")\n    # This will move `bid` after `ask`\n    update.move_after(\"bid\", \"ask\")\n    # This will move `confirmed_by` before `exchange` in the `details` struct\n    update.move_before((\"details\", \"confirmed_by\"), (\"details\", \"exchange\"))\n</code></pre>"},{"location":"api/#update-column","title":"Update column","text":"<p>Update a fields' type, description or required.</p> <pre><code>with table.update_schema() as update:\n    # Promote a float to a double\n    update.update_column(\"bid\", field_type=DoubleType())\n    # Make a field optional\n    update.update_column(\"symbol\", required=False)\n    # Update the documentation\n    update.update_column(\"symbol\", doc=\"Name of the share on the exchange\")\n</code></pre> <p>Be careful, some operations are not compatible, but can still be done at your own risk by setting <code>allow_incompatible_changes</code>:</p> <pre><code>with table.update_schema(allow_incompatible_changes=True) as update:\n    # Incompatible change, cannot require an optional field\n    update.update_column(\"symbol\", required=True)\n</code></pre>"},{"location":"api/#delete-column","title":"Delete column","text":"<p>Delete a field, careful this is a incompatible change (readers/writers might expect this field):</p> <pre><code>with table.update_schema(allow_incompatible_changes=True) as update:\n    update.delete_column(\"some_field\")\n    # In a struct\n    update.delete_column((\"details\", \"confirmed_by\"))\n</code></pre>"},{"location":"api/#partition-evolution","title":"Partition evolution","text":"<p>PyIceberg supports partition evolution. See the partition evolution for more details.</p> <p>The API to use when evolving partitions is the <code>update_spec</code> API on the table.</p> <pre><code>with table.update_spec() as update:\n    update.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n    update.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n</code></pre> <p>Updating the partition spec can also be done as part of a transaction with other operations.</p> <pre><code>with table.transaction() as transaction:\n    with transaction.update_spec() as update_spec:\n        update_spec.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n        update_spec.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n    # ... Update properties etc\n</code></pre>"},{"location":"api/#add-fields","title":"Add fields","text":"<p>New partition fields can be added via the <code>add_field</code> API which takes in the field name to partition on, the partition transform, and an optional partition name. If the partition name is not specified, one will be created.</p> <pre><code>with table.update_spec() as update:\n    update.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n    update.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n    # identity is a shortcut API for adding an IdentityTransform\n    update.identity(\"some_field\")\n</code></pre>"},{"location":"api/#remove-fields","title":"Remove fields","text":"<p>Partition fields can also be removed via the <code>remove_field</code> API if it no longer makes sense to partition on those fields.</p> <pre><code>with table.update_spec() as update:\n    # Remove the partition field with the name\n    update.remove_field(\"some_partition_name\")\n</code></pre>"},{"location":"api/#rename-fields","title":"Rename fields","text":"<p>Partition fields can also be renamed via the <code>rename_field</code> API.</p> <pre><code>with table.update_spec() as update:\n    # Rename the partition field with the name bucketed_id to sharded_id\n    update.rename_field(\"bucketed_id\", \"sharded_id\")\n</code></pre>"},{"location":"api/#table-properties","title":"Table properties","text":"<p>Set and remove properties through the <code>Transaction</code> API:</p> <pre><code>with table.transaction() as transaction:\n    transaction.set_properties(abc=\"def\")\n\nassert table.properties == {\"abc\": \"def\"}\n\nwith table.transaction() as transaction:\n    transaction.remove_properties(\"abc\")\n\nassert table.properties == {}\n</code></pre> <p>Or, without context manager:</p> <pre><code>table = table.transaction().set_properties(abc=\"def\").commit_transaction()\n\nassert table.properties == {\"abc\": \"def\"}\n\ntable = table.transaction().remove_properties(\"abc\").commit_transaction()\n\nassert table.properties == {}\n</code></pre>"},{"location":"api/#snapshot-properties","title":"Snapshot properties","text":"<p>Optionally, Snapshot properties can be set while writing to a table using <code>append</code> or <code>overwrite</code> API:</p> <pre><code>tbl.append(df, snapshot_properties={\"abc\": \"def\"})\n\n# or\n\ntbl.overwrite(df, snapshot_properties={\"abc\": \"def\"})\n\nassert tbl.metadata.snapshots[-1].summary[\"abc\"] == \"def\"\n</code></pre>"},{"location":"api/#snapshot-management","title":"Snapshot Management","text":"<p>Manage snapshots with operations through the <code>Table</code> API:</p> <pre><code># To run a specific operation\ntable.manage_snapshots().create_tag(snapshot_id, \"tag123\").commit()\n# To run multiple operations\ntable.manage_snapshots()\n    .create_tag(snapshot_id1, \"tag123\")\n    .create_tag(snapshot_id2, \"tag456\")\n    .commit()\n# Operations are applied on commit.\n</code></pre> <p>You can also use context managers to make more changes:</p> <pre><code>with table.manage_snapshots() as ms:\n    ms.create_branch(snapshot_id1, \"Branch_A\").create_tag(snapshot_id2, \"tag789\")\n</code></pre>"},{"location":"api/#views","title":"Views","text":"<p>PyIceberg supports view operations.</p>"},{"location":"api/#check-if-a-view-exists","title":"Check if a view exists","text":"<pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\ncatalog.view_exists(\"default.bar\")\n</code></pre>"},{"location":"api/#table-statistics-management","title":"Table Statistics Management","text":"<p>Manage table statistics with operations through the <code>Table</code> API:</p> <pre><code># To run a specific operation\ntable.update_statistics().set_statistics(statistics_file=statistics_file).commit()\n# To run multiple operations\ntable.update_statistics()\n  .set_statistics(statistics_file1)\n  .remove_statistics(snapshot_id2)\n  .commit()\n# Operations are applied on commit.\n</code></pre> <p>You can also use context managers to make more changes:</p> <pre><code>with table.update_statistics() as update:\n    update.set_statistics(statistics_file)\n    update.remove_statistics(snapshot_id2)\n</code></pre>"},{"location":"api/#query-the-data","title":"Query the data","text":"<p>To query a table, a table scan is needed. A table scan accepts a filter, columns, optionally a limit and a snapshot ID:</p> <pre><code>from pyiceberg.catalog import load_catalog\nfrom pyiceberg.expressions import GreaterThanOrEqual\n\ncatalog = load_catalog(\"default\")\ntable = catalog.load_table(\"nyc.taxis\")\n\nscan = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n    limit=100,\n)\n\n# Or filter using a string predicate\nscan = table.scan(\n    row_filter=\"trip_distance &gt; 10.0\",\n)\n\n[task.file.file_path for task in scan.plan_files()]\n</code></pre> <p>The low level API <code>plan_files</code> methods returns a set of tasks that provide the files that might contain matching rows:</p> <pre><code>[\n  \"s3://warehouse/wh/nyc/taxis/data/00003-4-42464649-92dd-41ad-b83b-dea1a2fe4b58-00001.parquet\"\n]\n</code></pre> <p>In this case it is up to the engine itself to filter the file itself. Below, <code>to_arrow()</code> and <code>to_duckdb()</code> that already do this for you.</p>"},{"location":"api/#apache-arrow","title":"Apache Arrow","text":"<p>Requirements</p> <p>This requires <code>pyarrow</code> to be installed.</p> <p>Using PyIceberg it is filter out data from a huge table and pull it into a PyArrow table:</p> <pre><code>table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow()\n</code></pre> <p>This will return a PyArrow table:</p> <pre><code>pyarrow.Table\nVendorID: int64\ntpep_pickup_datetime: timestamp[us, tz=+00:00]\ntpep_dropoff_datetime: timestamp[us, tz=+00:00]\n----\nVendorID: [[2,1,2,1,1,...,2,2,2,2,2],[2,1,1,1,2,...,1,1,2,1,2],...,[2,2,2,2,2,...,2,6,6,2,2],[2,2,2,2,2,...,2,2,2,2,2]]\ntpep_pickup_datetime: [[2021-04-01 00:28:05.000000,...,2021-04-30 23:44:25.000000]]\ntpep_dropoff_datetime: [[2021-04-01 00:47:59.000000,...,2021-05-01 00:14:47.000000]]\n</code></pre> <p>This will only pull in the files that that might contain matching rows.</p> <p>One can also return a PyArrow RecordBatchReader, if reading one record batch at a time is preferred:</p> <pre><code>table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow_batch_reader()\n</code></pre>"},{"location":"api/#pandas","title":"Pandas","text":"<p>Requirements</p> <p>This requires <code>pandas</code> to be installed.</p> <p>PyIceberg makes it easy to filter out data from a huge table and pull it into a Pandas dataframe locally. This will only fetch the relevant Parquet files for the query and apply the filter. This will reduce IO and therefore improve performance and reduce cost.</p> <pre><code>table.scan(\n    row_filter=\"trip_distance &gt;= 10.0\",\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_pandas()\n</code></pre> <p>This will return a Pandas dataframe:</p> <pre><code>        VendorID      tpep_pickup_datetime     tpep_dropoff_datetime\n0              2 2021-04-01 00:28:05+00:00 2021-04-01 00:47:59+00:00\n1              1 2021-04-01 00:39:01+00:00 2021-04-01 00:57:39+00:00\n2              2 2021-04-01 00:14:42+00:00 2021-04-01 00:42:59+00:00\n3              1 2021-04-01 00:17:17+00:00 2021-04-01 00:43:38+00:00\n4              1 2021-04-01 00:24:04+00:00 2021-04-01 00:56:20+00:00\n...          ...                       ...                       ...\n116976         2 2021-04-30 23:56:18+00:00 2021-05-01 00:29:13+00:00\n116977         2 2021-04-30 23:07:41+00:00 2021-04-30 23:37:18+00:00\n116978         2 2021-04-30 23:38:28+00:00 2021-05-01 00:12:04+00:00\n116979         2 2021-04-30 23:33:00+00:00 2021-04-30 23:59:00+00:00\n116980         2 2021-04-30 23:44:25+00:00 2021-05-01 00:14:47+00:00\n\n[116981 rows x 3 columns]\n</code></pre> <p>It is recommended to use Pandas 2 or later, because it stores the data in an Apache Arrow backend which avoids copies of data.</p>"},{"location":"api/#duckdb","title":"DuckDB","text":"<p>Requirements</p> <p>This requires DuckDB to be installed.</p> <p>A table scan can also be converted into a in-memory DuckDB table:</p> <pre><code>con = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_duckdb(table_name=\"distant_taxi_trips\")\n</code></pre> <p>Using the cursor that we can run queries on the DuckDB table:</p> <pre><code>print(\n    con.execute(\n        \"SELECT tpep_dropoff_datetime - tpep_pickup_datetime AS duration FROM distant_taxi_trips LIMIT 4\"\n    ).fetchall()\n)\n[\n    (datetime.timedelta(seconds=1194),),\n    (datetime.timedelta(seconds=1118),),\n    (datetime.timedelta(seconds=1697),),\n    (datetime.timedelta(seconds=1581),),\n]\n</code></pre>"},{"location":"api/#ray","title":"Ray","text":"<p>Requirements</p> <p>This requires Ray to be installed.</p> <p>A table scan can also be converted into a Ray dataset:</p> <pre><code>ray_dataset = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_ray()\n</code></pre> <p>This will return a Ray dataset:</p> <pre><code>Dataset(\n    num_blocks=1,\n    num_rows=1168798,\n    schema={\n        VendorID: int64,\n        tpep_pickup_datetime: timestamp[us, tz=UTC],\n        tpep_dropoff_datetime: timestamp[us, tz=UTC]\n    }\n)\n</code></pre> <p>Using Ray Dataset API to interact with the dataset:</p> <pre><code>print(ray_dataset.take(2))\n[\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 23, 50),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 0, 34, 31),\n    },\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 5, 3),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 16, 10, 18),\n    },\n]\n</code></pre>"},{"location":"api/#daft","title":"Daft","text":"<p>PyIceberg interfaces closely with Daft Dataframes (see also: Daft integration with Iceberg) which provides a full lazily optimized query engine interface on top of PyIceberg tables.</p> <p>Requirements</p> <p>This requires Daft to be installed.</p> <p>A table can be read easily into a Daft Dataframe:</p> <pre><code>df = table.to_daft()  # equivalent to `daft.read_iceberg(table)`\ndf = df.where(df[\"trip_distance\"] &gt;= 10.0)\ndf = df.select(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n</code></pre> <p>This returns a Daft Dataframe which is lazily materialized. Printing <code>df</code> will display the schema:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 VendorID \u2506 tpep_pickup_datetime          \u2506 tpep_dropoff_datetime         \u2502\n\u2502 ---      \u2506 ---                           \u2506 ---                           \u2502\n\u2502 Int64    \u2506 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(No data to display: Dataframe not materialized)\n</code></pre> <p>We can execute the Dataframe to preview the first few rows of the query with <code>df.show()</code>.</p> <p>This is correctly optimized to take advantage of Iceberg features such as hidden partitioning and file-level statistics for efficient reads.</p> <pre><code>df.show(2)\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 VendorID \u2506 tpep_pickup_datetime          \u2506 tpep_dropoff_datetime         \u2502\n\u2502 ---      \u2506 ---                           \u2506 ---                           \u2502\n\u2502 Int64    \u2506 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2        \u2506 2008-12-31T23:23:50.000000    \u2506 2009-01-01T00:34:31.000000    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2        \u2506 2008-12-31T23:05:03.000000    \u2506 2009-01-01T16:10:18.000000    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 rows)\n</code></pre>"},{"location":"api/#polars","title":"Polars","text":"<p>PyIceberg interfaces closely with Polars Dataframes and LazyFrame which provides a full lazily optimized query engine interface on top of PyIceberg tables.</p> <p>Requirements</p> <p>This requires <code>polars</code> to be installed.</p> <pre><code>pip install pyiceberg['polars']\n</code></pre> <p>PyIceberg data can be analyzed and accessed through Polars using either DataFrame or LazyFrame. If your code utilizes the Apache Iceberg data scanning and retrieval API and then analyzes the resulting DataFrame in Polars, use the <code>table.scan().to_polars()</code> API. If the intent is to utilize Polars' high-performance filtering and retrieval functionalities, use LazyFrame exported from the Iceberg table with the <code>table.to_polars()</code> API.</p> <pre><code># Get LazyFrame\niceberg_table.to_polars()\n\n# Get Data Frame\niceberg_table.scan().to_polars()\n</code></pre>"},{"location":"api/#working-with-polars-dataframe","title":"Working with Polars DataFrame","text":"<p>PyIceberg makes it easy to filter out data from a huge table and pull it into a Polars dataframe locally. This will only fetch the relevant Parquet files for the query and apply the filter. This will reduce IO and therefore improve performance and reduce cost.</p> <pre><code>schema = Schema(\n    NestedField(field_id=1, name='ticket_id', field_type=LongType(), required=True),\n    NestedField(field_id=2, name='customer_id', field_type=LongType(), required=True),\n    NestedField(field_id=3, name='issue', field_type=StringType(), required=False),\n    NestedField(field_id=4, name='created_at', field_type=TimestampType(), required=True),\n  required=True\n)\n\niceberg_table = catalog.create_table(\n    identifier='default.product_support_issues',\n    schema=schema\n)\n\npa_table_data = pa.Table.from_pylist(\n    [\n        {'ticket_id': 1, 'customer_id': 546, 'issue': 'User Login issue', 'created_at': 1650020000000000},\n        {'ticket_id': 2, 'customer_id': 547, 'issue': 'Payment not going through', 'created_at': 1650028640000000},\n        {'ticket_id': 3, 'customer_id': 548, 'issue': 'Error on checkout', 'created_at': 1650037280000000},\n        {'ticket_id': 4, 'customer_id': 549, 'issue': 'Unable to reset password', 'created_at': 1650045920000000},\n        {'ticket_id': 5, 'customer_id': 550, 'issue': 'Account locked', 'created_at': 1650054560000000},\n        {'ticket_id': 6, 'customer_id': 551, 'issue': 'Order not received', 'created_at': 1650063200000000},\n        {'ticket_id': 7, 'customer_id': 552, 'issue': 'Refund not processed', 'created_at': 1650071840000000},\n        {'ticket_id': 8, 'customer_id': 553, 'issue': 'Shipping address issue', 'created_at': 1650080480000000},\n        {'ticket_id': 9, 'customer_id': 554, 'issue': 'Product damaged', 'created_at': 1650089120000000},\n        {'ticket_id': 10, 'customer_id': 555, 'issue': 'Unable to apply discount code', 'created_at': 1650097760000000},\n        {'ticket_id': 11, 'customer_id': 556, 'issue': 'Website not loading', 'created_at': 1650106400000000},\n        {'ticket_id': 12, 'customer_id': 557, 'issue': 'Incorrect order received', 'created_at': 1650115040000000},\n        {'ticket_id': 13, 'customer_id': 558, 'issue': 'Unable to track order', 'created_at': 1650123680000000},\n        {'ticket_id': 14, 'customer_id': 559, 'issue': 'Order delayed', 'created_at': 1650132320000000},\n        {'ticket_id': 15, 'customer_id': 560, 'issue': 'Product not as described', 'created_at': 1650140960000000},\n        {'ticket_id': 16, 'customer_id': 561, 'issue': 'Unable to contact support', 'created_at': 1650149600000000},\n        {'ticket_id': 17, 'customer_id': 562, 'issue': 'Duplicate charge', 'created_at': 1650158240000000},\n        {'ticket_id': 18, 'customer_id': 563, 'issue': 'Unable to update profile', 'created_at': 1650166880000000},\n        {'ticket_id': 19, 'customer_id': 564, 'issue': 'App crashing', 'created_at': 1650175520000000},\n        {'ticket_id': 20, 'customer_id': 565, 'issue': 'Unable to download invoice', 'created_at': 1650184160000000},\n        {'ticket_id': 21, 'customer_id': 566, 'issue': 'Incorrect billing amount', 'created_at': 1650192800000000},\n    ], schema=iceberg_table.schema().as_arrow()\n)\n\niceberg_table.append(\n    df=pa_table_data\n)\n\ntable.scan(\n    row_filter=\"ticket_id &gt; 10\",\n).to_polars()\n</code></pre> <p>This will return a Polars DataFrame:</p> <pre><code>shape: (11, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticket_id \u2506 customer_id \u2506 issue                      \u2506 created_at          \u2502\n\u2502 ---       \u2506 ---         \u2506 ---                        \u2506 ---                 \u2502\n\u2502 i64       \u2506 i64         \u2506 str                        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11        \u2506 556         \u2506 Website not loading        \u2506 2022-04-16 10:53:20 \u2502\n\u2502 12        \u2506 557         \u2506 Incorrect order received   \u2506 2022-04-16 13:17:20 \u2502\n\u2502 13        \u2506 558         \u2506 Unable to track order      \u2506 2022-04-16 15:41:20 \u2502\n\u2502 14        \u2506 559         \u2506 Order delayed              \u2506 2022-04-16 18:05:20 \u2502\n\u2502 15        \u2506 560         \u2506 Product not as described   \u2506 2022-04-16 20:29:20 \u2502\n\u2502 \u2026         \u2506 \u2026           \u2506 \u2026                          \u2506 \u2026                   \u2502\n\u2502 17        \u2506 562         \u2506 Duplicate charge           \u2506 2022-04-17 01:17:20 \u2502\n\u2502 18        \u2506 563         \u2506 Unable to update profile   \u2506 2022-04-17 03:41:20 \u2502\n\u2502 19        \u2506 564         \u2506 App crashing               \u2506 2022-04-17 06:05:20 \u2502\n\u2502 20        \u2506 565         \u2506 Unable to download invoice \u2506 2022-04-17 08:29:20 \u2502\n\u2502 21        \u2506 566         \u2506 Incorrect billing amount   \u2506 2022-04-17 10:53:20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/#working-with-polars-lazyframe","title":"Working with Polars LazyFrame","text":"<p>PyIceberg supports creation of a Polars LazyFrame based on an Iceberg Table.</p> <p>using the above code example:</p> <pre><code>lf = iceberg_table.to_polars().filter(pl.col(\"ticket_id\") &gt; 10)\nprint(lf.collect())\n</code></pre> <p>This above code snippet returns a Polars LazyFrame and defines a filter to be executed by Polars:</p> <pre><code>shape: (11, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticket_id \u2506 customer_id \u2506 issue                      \u2506 created_at          \u2502\n\u2502 ---       \u2506 ---         \u2506 ---                        \u2506 ---                 \u2502\n\u2502 i64       \u2506 i64         \u2506 str                        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11        \u2506 556         \u2506 Website not loading        \u2506 2022-04-16 10:53:20 \u2502\n\u2502 12        \u2506 557         \u2506 Incorrect order received   \u2506 2022-04-16 13:17:20 \u2502\n\u2502 13        \u2506 558         \u2506 Unable to track order      \u2506 2022-04-16 15:41:20 \u2502\n\u2502 14        \u2506 559         \u2506 Order delayed              \u2506 2022-04-16 18:05:20 \u2502\n\u2502 15        \u2506 560         \u2506 Product not as described   \u2506 2022-04-16 20:29:20 \u2502\n\u2502 \u2026         \u2506 \u2026           \u2506 \u2026                          \u2506 \u2026                   \u2502\n\u2502 17        \u2506 562         \u2506 Duplicate charge           \u2506 2022-04-17 01:17:20 \u2502\n\u2502 18        \u2506 563         \u2506 Unable to update profile   \u2506 2022-04-17 03:41:20 \u2502\n\u2502 19        \u2506 564         \u2506 App crashing               \u2506 2022-04-17 06:05:20 \u2502\n\u2502 20        \u2506 565         \u2506 Unable to download invoice \u2506 2022-04-17 08:29:20 \u2502\n\u2502 21        \u2506 566         \u2506 Incorrect billing amount   \u2506 2022-04-17 10:53:20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#python-cli","title":"Python CLI","text":"<p>Pyiceberg comes with a CLI that's available after installing the <code>pyiceberg</code> package.</p> <p>You can pass the path to the Catalog using the <code>--uri</code> and <code>--credential</code> argument, but it is recommended to setup a <code>~/.pyiceberg.yaml</code> config as described in the Catalog section.</p> <pre><code>\u279c  pyiceberg --help\nUsage: pyiceberg [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n--catalog TEXT\n--verbose BOOLEAN\n--output [text|json]\n--ugi TEXT\n--uri TEXT\n--credential TEXT\n--help                Show this message and exit.\n\nCommands:\ndescribe    Describes a namespace xor table\ndrop        Operations to drop a namespace or table\nlist        Lists tables or namespaces\nlocation    Returns the location of the table\nproperties  Properties on tables/namespaces\nrename      Renames a table\nschema      Gets the schema of the table\nspec        Returns the partition spec of the table\nuuid        Returns the UUID of the table\n</code></pre> <p>This example assumes that you have a default catalog set. If you want to load another catalog, for example, the rest example above. Then you need to set <code>--catalog rest</code>.</p> <pre><code>\u279c  pyiceberg list\ndefault\nnyc\n</code></pre> <pre><code>\u279c  pyiceberg list nyc\nnyc.taxis\n</code></pre> <pre><code>\u279c  pyiceberg describe nyc.taxis\nTable format version  1\nMetadata location     file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\nTable UUID            6cdfda33-bfa3-48a7-a09e-7abb462e3460\nLast Updated          1661783158061\nPartition spec        []\nSort order            []\nCurrent schema        Schema, id=0\n\u251c\u2500\u2500 1: VendorID: optional long\n\u251c\u2500\u2500 2: tpep_pickup_datetime: optional timestamptz\n\u251c\u2500\u2500 3: tpep_dropoff_datetime: optional timestamptz\n\u251c\u2500\u2500 4: passenger_count: optional double\n\u251c\u2500\u2500 5: trip_distance: optional double\n\u251c\u2500\u2500 6: RatecodeID: optional double\n\u251c\u2500\u2500 7: store_and_fwd_flag: optional string\n\u251c\u2500\u2500 8: PULocationID: optional long\n\u251c\u2500\u2500 9: DOLocationID: optional long\n\u251c\u2500\u2500 10: payment_type: optional long\n\u251c\u2500\u2500 11: fare_amount: optional double\n\u251c\u2500\u2500 12: extra: optional double\n\u251c\u2500\u2500 13: mta_tax: optional double\n\u251c\u2500\u2500 14: tip_amount: optional double\n\u251c\u2500\u2500 15: tolls_amount: optional double\n\u251c\u2500\u2500 16: improvement_surcharge: optional double\n\u251c\u2500\u2500 17: total_amount: optional double\n\u251c\u2500\u2500 18: congestion_surcharge: optional double\n\u2514\u2500\u2500 19: airport_fee: optional double\nCurrent snapshot      Operation.APPEND: id=5937117119577207079, schema_id=0\nSnapshots             Snapshots\n\u2514\u2500\u2500 Snapshot 5937117119577207079, schema 0: file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\nProperties            owner                 root\nwrite.format.default  parquet\n</code></pre> <p>Or output in JSON for automation:</p> <pre><code>\u279c  pyiceberg --output json describe nyc.taxis | jq\n{\n  \"identifier\": [\n    \"nyc\",\n    \"taxis\"\n  ],\n  \"metadata_location\": \"file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\",\n  \"metadata\": {\n    \"location\": \"file:/.../nyc.db/taxis\",\n    \"table-uuid\": \"6cdfda33-bfa3-48a7-a09e-7abb462e3460\",\n    \"last-updated-ms\": 1661783158061,\n    \"last-column-id\": 19,\n    \"schemas\": [\n      {\n        \"type\": \"struct\",\n        \"fields\": [\n          {\n            \"id\": 1,\n            \"name\": \"VendorID\",\n            \"type\": \"long\",\n            \"required\": false\n          },\n...\n          {\n            \"id\": 19,\n            \"name\": \"airport_fee\",\n            \"type\": \"double\",\n            \"required\": false\n          }\n        ],\n        \"schema-id\": 0,\n        \"identifier-field-ids\": []\n      }\n    ],\n    \"current-schema-id\": 0,\n    \"partition-specs\": [\n      {\n        \"spec-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-spec-id\": 0,\n    \"last-partition-id\": 999,\n    \"properties\": {\n      \"owner\": \"root\",\n      \"write.format.default\": \"parquet\"\n    },\n    \"current-snapshot-id\": 5937117119577207000,\n    \"snapshots\": [\n      {\n        \"snapshot-id\": 5937117119577207000,\n        \"timestamp-ms\": 1661783158061,\n        \"manifest-list\": \"file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\",\n        \"summary\": {\n          \"operation\": \"append\",\n          \"spark.app.id\": \"local-1661783139151\",\n          \"added-data-files\": \"1\",\n          \"added-records\": \"2979431\",\n          \"added-files-size\": \"46600777\",\n          \"changed-partition-count\": \"1\",\n          \"total-records\": \"2979431\",\n          \"total-files-size\": \"46600777\",\n          \"total-data-files\": \"1\",\n          \"total-delete-files\": \"0\",\n          \"total-position-deletes\": \"0\",\n          \"total-equality-deletes\": \"0\"\n        },\n        \"schema-id\": 0\n      }\n    ],\n    \"snapshot-log\": [\n      {\n        \"snapshot-id\": \"5937117119577207079\",\n        \"timestamp-ms\": 1661783158061\n      }\n    ],\n    \"metadata-log\": [],\n    \"sort-orders\": [\n      {\n        \"order-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-sort-order-id\": 0,\n    \"refs\": {\n      \"main\": {\n        \"snapshot-id\": 5937117119577207000,\n        \"type\": \"branch\"\n      }\n    },\n    \"format-version\": 1,\n    \"schema\": {\n      \"type\": \"struct\",\n      \"fields\": [\n        {\n          \"id\": 1,\n          \"name\": \"VendorID\",\n          \"type\": \"long\",\n          \"required\": false\n        },\n...\n        {\n          \"id\": 19,\n          \"name\": \"airport_fee\",\n          \"type\": \"double\",\n          \"required\": false\n        }\n      ],\n      \"schema-id\": 0,\n      \"identifier-field-ids\": []\n    },\n    \"partition-spec\": []\n  }\n}\n</code></pre>"},{"location":"community/","title":"Community","text":""},{"location":"community/#join-the-community","title":"Join the community","text":"<p>Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests.</p> <p>Community discussions happen primarily on the dev mailing list, on Apache Iceberg Slack workspace in the #python channel, and on specific GitHub issues.</p>"},{"location":"community/#iceberg-community-events","title":"Iceberg Community Events","text":"<p>The PyIceberg community sync is on the last Tuesday of every month. To join, make sure to subscribe to the iceberg-python-sync Google group.</p>"},{"location":"community/#community-guidelines","title":"Community Guidelines","text":""},{"location":"community/#apache-iceberg-community-guidelines","title":"Apache Iceberg Community Guidelines","text":"<p>The Apache Iceberg community is built on the principles described in the Apache Way and all who engage with the community are expected to be respectful, open, come with the best interests of the community in mind, and abide by the Apache Foundation Code of Conduct.</p>"},{"location":"community/#participants-with-corporate-interests","title":"Participants with Corporate Interests","text":"<p>A wide range of corporate entities have interests that overlap in both features and frameworks related to Iceberg and while we encourage engagement and contributions, the community is not a venue for marketing, solicitation, or recruitment.</p> <p>Any vendor who wants to participate in the Apache Iceberg community Slack workspace should create a dedicated vendor channel for their organization prefixed by <code>vendor-</code>.</p> <p>This space can be used to discuss features and integration with Iceberg related to the vendor offering.  This space should not be used to promote competing vendor products/services or disparage other vendor offerings.  Discussion should be focused on questions asked by the community and not to expand/introduce/redirect users to alternate offerings.</p>"},{"location":"community/#marketing-solicitation-recruiting","title":"Marketing / Solicitation / Recruiting","text":"<p>The Apache Iceberg community is a space for everyone to operate free of influence. The development lists, Slack workspace, and GitHub should not be used to market products or services.  Solicitation or overt promotion should not be performed in common channels or through direct messages.</p> <p>Recruitment of community members should not be conducted through direct messages or community channels, but opportunities related to contributing to or using Iceberg can be posted to the <code>#jobs</code> channel.</p> <p>For questions regarding any of the guidelines above, please contact a PMC member</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#setting-configuration-values","title":"Setting Configuration Values","text":"<p>There are three ways to pass in configuration:</p> <ul> <li>Using the <code>.pyiceberg.yaml</code> configuration file (Recommended)</li> <li>Through environment variables</li> <li>By passing in credentials through the CLI or the Python API</li> </ul> <p>The configuration file can be stored in either the directory specified by the <code>PYICEBERG_HOME</code> environment variable, the home directory, or current working directory (in this order).</p> <p>To change the path searched for the <code>.pyiceberg.yaml</code>, you can overwrite the <code>PYICEBERG_HOME</code> environment variable.</p> <p>Another option is through environment variables:</p> <pre><code>export PYICEBERG_CATALOG__DEFAULT__URI=thrift://localhost:9083\nexport PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID=username\nexport PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY=password\n</code></pre> <p>The environment variable picked up by Iceberg starts with <code>PYICEBERG_</code> and then follows the yaml structure below, where a double underscore <code>__</code> represents a nested field, and the underscore <code>_</code> is converted into a dash <code>-</code>.</p> <p>For example, <code>PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID</code>, sets <code>s3.access-key-id</code> on the <code>default</code> catalog.</p>"},{"location":"configuration/#tables","title":"Tables","text":"<p>Iceberg tables support table properties to configure table behavior.</p>"},{"location":"configuration/#write-options","title":"Write options","text":"Key Options Default Description <code>write.parquet.compression-codec</code> <code>{uncompressed,zstd,gzip,snappy}</code> zstd Sets the Parquet compression coddec. <code>write.parquet.compression-level</code> Integer null Parquet compression level for the codec. If not set, it is up to PyIceberg <code>write.parquet.row-group-limit</code> Number of rows 1048576 The upper bound of the number of entries within a single row group <code>write.parquet.page-size-bytes</code> Size in bytes 1MB Set a target threshold for the approximate encoded size of data pages within a column chunk <code>write.parquet.page-row-limit</code> Number of rows 20000 Set a target threshold for the maximum number of rows within a column chunk <code>write.parquet.dict-size-bytes</code> Size in bytes 2MB Set the dictionary page size limit per row group <code>write.metadata.previous-versions-max</code> Integer 100 The max number of previous version metadata files to keep before deleting after commit. <code>write.metadata.delete-after-commit.enabled</code> Boolean False Whether to automatically delete old tracked metadata files after each table commit. It will retain a number of the most recent metadata files, which can be set using property <code>write.metadata.previous-versions-max</code>. <code>write.object-storage.enabled</code> Boolean False Enables the <code>ObjectStoreLocationProvider</code> that adds a hash component to file paths. <code>write.object-storage.partitioned-paths</code> Boolean True Controls whether partition values are included in file paths when object storage is enabled <code>write.py-location-provider.impl</code> String of form <code>module.ClassName</code> null Optional, custom <code>LocationProvider</code> implementation <code>write.data.path</code> String pointing to location <code>{metadata.location}/data</code> Sets the location under which data is written. <code>write.metadata.path</code> String pointing to location <code>{metadata.location}/metadata</code> Sets the location under which metadata is written."},{"location":"configuration/#table-behavior-options","title":"Table behavior options","text":"Key Options Default Description <code>commit.manifest.target-size-bytes</code> Size in bytes 8388608 (8MB) Target size when merging manifest files <code>commit.manifest.min-count-to-merge</code> Number of manifests 100 Target size when merging manifest files <code>commit.manifest-merge.enabled</code> Boolean False Controls whether to automatically merge manifests on writes <p>Fast append</p> <p>Unlike Java implementation, PyIceberg default to the fast append and thus <code>commit.manifest-merge.enabled</code> is set to <code>False</code> by default.</p>"},{"location":"configuration/#fileio","title":"FileIO","text":"<p>Iceberg works with the concept of a FileIO which is a pluggable module for reading, writing, and deleting files. By default, PyIceberg will try to initialize the FileIO that's suitable for the scheme (<code>s3://</code>, <code>gs://</code>, etc.) and will use the first one that's installed.</p> <ul> <li>s3, s3a, s3n: <code>PyArrowFileIO</code>, <code>FsspecFileIO</code></li> <li>gs: <code>PyArrowFileIO</code></li> <li>file: <code>PyArrowFileIO</code></li> <li>hdfs: <code>PyArrowFileIO</code></li> <li>abfs, abfss: <code>FsspecFileIO</code></li> <li>oss: <code>PyArrowFileIO</code></li> </ul> <p>You can also set the FileIO explicitly:</p> Key Example Description py-io-impl pyiceberg.io.fsspec.FsspecFileIO Sets the FileIO explicitly to an implementation, and will fail explicitly if it can't be loaded <p>For the FileIO there are several configuration options available:</p>"},{"location":"configuration/#s3","title":"S3","text":"Key Example Description s3.endpoint https://10.0.19.25/ Configure an alternative endpoint of the S3 service for the FileIO to access. This could be used to use S3FileIO with any s3-compatible object storage service that has a different endpoint, or access a private S3 endpoint in a virtual private cloud. s3.access-key-id admin Configure the static access key id used to access the FileIO. s3.secret-access-key password Configure the static secret access key used to access the FileIO. s3.session-token AQoDYXdzEJr... Configure the static session token used to access the FileIO. s3.role-session-name session An optional identifier for the assumed role session. s3.role-arn arn:aws:... AWS Role ARN. If provided instead of access_key and secret_key, temporary credentials will be fetched by assuming this role. s3.signer bearer Configure the signature version of the FileIO. s3.signer.uri http://my.signer:8080/s3 Configure the remote signing uri if it differs from the catalog uri. Remote signing is only implemented for <code>FsspecFileIO</code>. The final request is sent to <code>&lt;s3.signer.uri&gt;/&lt;s3.signer.endpoint&gt;</code>. s3.signer.endpoint v1/main/s3-sign Configure the remote signing endpoint. Remote signing is only implemented for <code>FsspecFileIO</code>. The final request is sent to <code>&lt;s3.signer.uri&gt;/&lt;s3.signer.endpoint&gt;</code>. (default : v1/aws/s3/sign). s3.region us-west-2 Configure the default region used to initialize an <code>S3FileSystem</code>. <code>PyArrowFileIO</code> attempts to automatically tries to resolve the region if this isn't set (only supported for AWS S3 Buckets). s3.resolve-region False Only supported for <code>PyArrowFileIO</code>, when enabled, it will always try to resolve the location of the bucket (only supported for AWS S3 Buckets). s3.proxy-uri http://my.proxy.com:8080 Configure the proxy server to be used by the FileIO. s3.connect-timeout 60.0 Configure socket connection timeout, in seconds. s3.request-timeout 60.0 Configure socket read timeouts on Windows and macOS, in seconds. s3.force-virtual-addressing False Whether to use virtual addressing of buckets. If true, then virtual addressing is always enabled. If false, then virtual addressing is only enabled if endpoint_override is empty. This can be used for non-AWS backends that only support virtual hosted-style access."},{"location":"configuration/#hdfs","title":"HDFS","text":"Key Example Description hdfs.host https://10.0.19.25/ Configure the HDFS host to connect to hdfs.port 9000 Configure the HDFS port to connect to. hdfs.user user Configure the HDFS username used for connection. hdfs.kerberos_ticket kerberos_ticket Configure the path to the Kerberos ticket cache."},{"location":"configuration/#azure-data-lake","title":"Azure Data lake","text":"Key Example Description adls.connection-string AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqF...;BlobEndpoint=http://localhost/ A connection string. This could be used to use FileIO with any adls-compatible object storage service that has a different endpoint (like azurite). adls.account-name devstoreaccount1 The account that you want to connect to adls.account-key Eby8vdM02xNOcqF... The key to authentication against the account. adls.sas-token NuHOuuzdQN7VRM%2FOpOeqBlawRCA845IY05h9eu1Yte4%3D The shared access signature adls.tenant-id ad667be4-b811-11ed-afa1-0242ac120002 The tenant-id adls.client-id ad667be4-b811-11ed-afa1-0242ac120002 The client-id adls.client-secret oCA3R6P*ka#oa1Sms2J74z... The client-secret"},{"location":"configuration/#google-cloud-storage","title":"Google Cloud Storage","text":"Key Example Description gcs.project-id my-gcp-project Configure Google Cloud Project for GCS FileIO. gcs.oauth2.token ya29.dr.AfM... String representation of the access token used for temporary access. gcs.oauth2.token-expires-at 1690971805918 Configure expiration for credential generated with an access token. Milliseconds since epoch gcs.access read_only Configure client to have specific access. Must be one of 'read_only', 'read_write', or 'full_control' gcs.consistency md5 Configure the check method when writing files. Must be one of 'none', 'size', or 'md5' gcs.cache-timeout 60 Configure the cache expiration time in seconds for object metadata cache gcs.requester-pays False Configure whether to use requester-pays requests gcs.session-kwargs {} Configure a dict of parameters to pass on to aiohttp.ClientSession; can contain, for example, proxy settings. gcs.service.host http://0.0.0.0:4443 Configure an alternative endpoint for the GCS FileIO to access (format protocol://host:port) If not given, defaults to the value of environment variable \"STORAGE_EMULATOR_HOST\"; if that is not set either, will use the standard Google endpoint. gcs.default-location US Configure the default location where buckets are created, like 'US' or 'EUROPE-WEST3'. gcs.version-aware False Configure whether to support object versioning on the GCS bucket."},{"location":"configuration/#alibaba-cloud-object-storage-service-oss","title":"Alibaba Cloud Object Storage Service (OSS)","text":"<p>PyIceberg uses S3FileSystem class to connect to OSS bucket as the service is compatible with S3 SDK as long as the endpoint is addressed with virtual hosted style.</p> Key Example Description s3.endpoint https://s3.oss-your-bucket-region.aliyuncs.com/ Configure an endpoint of the OSS service for the FileIO to access. Be sure to use S3 compatible endpoint as given in the example. s3.access-key-id admin Configure the static access key id used to access the FileIO. s3.secret-access-key password Configure the static secret access key used to access the FileIO. s3.session-token AQoDYXdzEJr... Configure the static session token used to access the FileIO. s3.force-virtual-addressing True Whether to use virtual addressing of buckets. This must be set to True as OSS can only be accessed with virtual hosted style address."},{"location":"configuration/#pyarrow","title":"PyArrow","text":"Key Example Description pyarrow.use-large-types-on-read True Use large PyArrow types i.e. large_string, large_binary and large_list field types on table scans. The default value is True."},{"location":"configuration/#location-providers","title":"Location Providers","text":"<p>Apache Iceberg uses the concept of a <code>LocationProvider</code> to manage file paths for a table's data files. In PyIceberg, the <code>LocationProvider</code> module is designed to be pluggable, allowing customization for specific use cases, and to additionally determine metadata file locations. The <code>LocationProvider</code> for a table can be specified through table properties.</p> <p>Both data file and metadata file locations can be customized by configuring the table properties <code>write.data.path</code> and <code>write.metadata.path</code>, respectively.</p> <p>For more granular control, you can override the <code>LocationProvider</code>'s <code>new_data_location</code> and <code>new_metadata_location</code> methods to define custom logic for generating file paths. See <code>Loading a Custom Location Provider</code>.</p> <p>PyIceberg defaults to the <code>SimpleLocationProvider</code> for managing file paths.</p>"},{"location":"configuration/#simple-location-provider","title":"Simple Location Provider","text":"<p>The <code>SimpleLocationProvider</code> provides paths prefixed by <code>{location}/data/</code>, where <code>location</code> comes from the table metadata. This can be overridden by setting <code>write.data.path</code> table configuration.</p> <p>For example, a non-partitioned table might have a data file with location:</p> <pre><code>s3://bucket/ns/table/data/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre> <p>When the table is partitioned, files under a given partition are grouped into a subdirectory, with that partition key and value as the directory name - this is known as the Hive-style partition path format. For example, a table partitioned over a string column <code>category</code> might have a data file with location:</p> <pre><code>s3://bucket/ns/table/data/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre>"},{"location":"configuration/#object-store-location-provider","title":"Object Store Location Provider","text":"<p>PyIceberg offers the <code>ObjectStoreLocationProvider</code>, and an optional partition-exclusion optimization, designed for tables stored in object storage. For additional context and motivation concerning these configurations, see their documentation for Iceberg's Java implementation.</p> <p>When several files are stored under the same prefix, cloud object stores such as S3 often throttle requests on prefixes, resulting in slowdowns. The <code>ObjectStoreLocationProvider</code> counteracts this by injecting deterministic hashes, in the form of binary directories, into file paths, to distribute files across a larger number of object store prefixes.</p> <p>Paths are prefixed by <code>{location}/data/</code>, where <code>location</code> comes from the table metadata, in a similar manner to the <code>SimpleLocationProvider</code>. This can be overridden by setting <code>write.data.path</code> table configuration.</p> <p>For example, a table partitioned over a string column <code>category</code> might have a data file with location: (note the additional binary directories)</p> <pre><code>s3://bucket/ns/table/data/0101/0110/1001/10110010/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre> <p>The <code>ObjectStoreLocationProvider</code> is enabled for a table by explicitly setting its <code>write.object-storage.enabled</code> table property to <code>True</code>.</p>"},{"location":"configuration/#partition-exclusion","title":"Partition Exclusion","text":"<p>When the <code>ObjectStoreLocationProvider</code> is used, the table property <code>write.object-storage.partitioned-paths</code>, which defaults to <code>True</code>, can be set to <code>False</code> as an additional optimization for object stores. This omits partition keys and values from data file paths entirely to further reduce key size. With it disabled, the same data file above would instead be written to: (note the absence of <code>category=orders</code>)</p> <pre><code>s3://bucket/ns/table/data/1101/0100/1011/00111010-00000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre>"},{"location":"configuration/#loading-a-custom-location-provider","title":"Loading a Custom Location Provider","text":"<p>Similar to FileIO, a custom <code>LocationProvider</code> may be provided for a table by concretely subclassing the abstract base class <code>LocationProvider</code>.</p> <p>The table property <code>write.py-location-provider.impl</code> should be set to the fully-qualified name of the custom <code>LocationProvider</code> (i.e. <code>mymodule.MyLocationProvider</code>). Recall that a <code>LocationProvider</code> is configured per-table, permitting different location provision for different tables. Note also that Iceberg's Java implementation uses a different table property, <code>write.location-provider.impl</code>, for custom Java implementations.</p> <p>An example, custom <code>LocationProvider</code> implementation is shown below.</p> <pre><code>import uuid\n\nclass UUIDLocationProvider(LocationProvider):\n    def __init__(self, table_location: str, table_properties: Properties):\n        super().__init__(table_location, table_properties)\n\n    def new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -&gt; str:\n        # Can use any custom method to generate a file path given the partitioning information and file name\n        prefix = f\"{self.table_location}/{uuid.uuid4()}\"\n        return f\"{prefix}/{partition_key.to_path()}/{data_file_name}\" if partition_key else f\"{prefix}/{data_file_name}\"\n</code></pre>"},{"location":"configuration/#catalogs","title":"Catalogs","text":"<p>PyIceberg currently has native catalog type support for REST, SQL, Hive, Glue and DynamoDB. Alternatively, you can also directly set the catalog implementation:</p> Key Example Description type rest Type of catalog, one of <code>rest</code>, <code>sql</code>, <code>hive</code>, <code>glue</code>, <code>dymamodb</code>. Default to <code>rest</code> py-catalog-impl mypackage.mymodule.MyCatalog Sets the catalog explicitly to an implementation, and will fail explicitly if it can't be loaded"},{"location":"configuration/#rest-catalog","title":"REST Catalog","text":"<pre><code>catalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n\n  default-mtls-secured-catalog:\n    uri: https://rest-catalog/ws/\n    ssl:\n      client:\n        cert: /absolute/path/to/client.crt\n        key: /absolute/path/to/client.key\n      cabundle: /absolute/path/to/cabundle.pem\n</code></pre> Key Example Description uri https://rest-catalog/ws URI identifying the REST Server ugi t-1234:secret Hadoop UGI for Hive client. credential t-1234:secret Credential to use for OAuth2 credential flow when initializing the catalog token FEW23.DFSDF.FSDF Bearer token value to use for <code>Authorization</code> header scope openid offline corpds:ds:profile Desired scope of the requested security token (default : catalog) resource rest_catalog.iceberg.com URI for the target resource or service audience rest_catalog Logical name of target resource or service rest.sigv4-enabled true Sign requests to the REST Server using AWS SigV4 protocol rest.signing-region us-east-1 The region to use when SigV4 signing a request rest.signing-name execute-api The service signing name to use when SigV4 signing a request oauth2-server-uri https://auth-service/cc Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens')"},{"location":"configuration/#headers-in-restcatalog","title":"Headers in RESTCatalog","text":"<p>To configure custom headers in RESTCatalog, include them in the catalog properties with the prefix <code>header.</code>. This ensures that all HTTP requests to the REST service include the specified headers.</p> <pre><code>catalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n    header.content-type: application/vnd.api+json\n</code></pre> <p>Specific headers defined by the RESTCatalog spec include:</p> Key Options Default Description <code>header.X-Iceberg-Access-Delegation</code> <code>{vended-credentials,remote-signing}</code> <code>vended-credentials</code> Signal to the server that the client supports delegated access via a comma-separated list of access mechanisms. The server may choose to supply access via any or none of the requested mechanisms"},{"location":"configuration/#sql-catalog","title":"SQL Catalog","text":"<p>The SQL catalog requires a database for its backend. PyIceberg supports PostgreSQL and SQLite through psycopg2. The database connection has to be configured using the <code>uri</code> property. The init_catalog_tables is optional and defaults to True. If it is set to False, the catalog tables will not be created when the SQLCatalog is initialized. See SQLAlchemy's documentation for URL format:</p> <p>For PostgreSQL:</p> <pre><code>catalog:\n  default:\n    type: sql\n    uri: postgresql+psycopg2://username:password@localhost/mydatabase\n    init_catalog_tables: false\n</code></pre> <p>In the case of SQLite:</p> <p>Development only</p> <p>SQLite is not built for concurrency, you should use this catalog for exploratory or development purposes.</p> <pre><code>catalog:\n  default:\n    type: sql\n    uri: sqlite:////tmp/pyiceberg.db\n    init_catalog_tables: false\n</code></pre> Key Example Default Description uri postgresql+psycopg2://username:password@localhost/mydatabase SQLAlchemy backend URL for the catalog database (see documentation for URL format) echo true false SQLAlchemy engine echo param to log all statements to the default log handler pool_pre_ping true false SQLAlchemy engine pool_pre_ping param to test connections for liveness upon each checkout"},{"location":"configuration/#in-memory-catalog","title":"In Memory Catalog","text":"<p>The in-memory catalog is built on top of <code>SqlCatalog</code> and uses SQLite in-memory database for its backend.</p> <p>It is useful for test, demo, and playground but not in production as it does not support concurrent access.</p> <pre><code>catalog:\n  default:\n    type: in-memory\n    warehouse: /tmp/pyiceberg/warehouse\n</code></pre> Key Example Default Description warehouse /tmp/pyiceberg/warehouse file:///tmp/iceberg/warehouse The directory where the in-memory catalog will store its data files."},{"location":"configuration/#hive-catalog","title":"Hive Catalog","text":"<pre><code>catalog:\n  default:\n    uri: thrift://localhost:9083\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> Key Example Description hive.hive2-compatible true Using Hive 2.x compatibility mode hive.kerberos-authentication true Using authentication via Kerberos <p>When using Hive 2.x, make sure to set the compatibility flag:</p> <pre><code>catalog:\n  default:\n...\n    hive.hive2-compatible: true\n</code></pre>"},{"location":"configuration/#glue-catalog","title":"Glue Catalog","text":"<p>Your AWS credentials can be passed directly through the Python API. Otherwise, please refer to How to configure AWS credentials to set your AWS account credentials locally.</p> <pre><code>catalog:\n  default:\n    type: glue\n    glue.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    glue.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    glue.session-token: &lt;SESSION_TOKEN&gt;\n    glue.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <pre><code>catalog:\n  default:\n    type: glue\n    glue.profile-name: &lt;PROFILE_NAME&gt;\n    glue.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <p>Client-specific Properties</p> <p><code>glue.*</code> properties are for Glue Catalog only. If you want to use the same credentials for both Glue Catalog and S3 FileIO, you can set the <code>client.*</code> properties. See the Unified AWS Credentials section for more details.</p> Key Example Description glue.id 111111111111 Configure the 12-digit ID of the Glue Catalog glue.skip-archive true Configure whether to skip the archival of older table versions. Default to true glue.endpoint https://glue.us-east-1.amazonaws.com Configure an alternative endpoint of the Glue service for GlueCatalog to access glue.profile-name default Configure the static profile used to access the Glue Catalog glue.region us-east-1 Set the region of the Glue Catalog glue.access-key-id admin Configure the static access key id used to access the Glue Catalog glue.secret-access-key password Configure the static secret access key used to access the Glue Catalog glue.session-token AQoDYXdzEJr... Configure the static session token used to access the Glue Catalog glue.max-retries 10 Configure the maximum number of retries for the Glue service calls glue.retry-mode standard Configure the retry mode for the Glue service. Default to standard. <p>Removed Properties</p> <p>The properties <code>profile_name</code>, <code>region_name</code>, <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, and <code>aws_session_token</code> were deprecated and removed in 0.8.0</p>"},{"location":"configuration/#dynamodb-catalog","title":"DynamoDB Catalog","text":"<p>If you want to use AWS DynamoDB as the catalog, you can use the last two ways to configure the pyiceberg and refer How to configure AWS credentials to set your AWS account credentials locally. If you want to use the same credentials for both Dynamodb Catalog and S3 FileIO, you can set the <code>client.*</code> properties.</p> <pre><code>catalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n</code></pre> <p>If you prefer to pass the credentials explicitly to the client instead of relying on environment variables,</p> <pre><code>catalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n    dynamodb.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    dynamodb.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    dynamodb.session-token: &lt;SESSION_TOKEN&gt;\n    dynamodb.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <p>Client-specific Properties</p> <p><code>dynamodb.*</code> properties are for DynamoDB Catalog only. If you want to use the same credentials for both DynamoDB Catalog and S3 FileIO, you can set the <code>client.*</code> properties. See the Unified AWS Credentials section for more details.</p> Key Example Description dynamodb.profile-name default Configure the static profile used to access the DynamoDB Catalog dynamodb.region us-east-1 Set the region of the DynamoDB Catalog dynamodb.access-key-id admin Configure the static access key id used to access the DynamoDB Catalog dynamodb.secret-access-key password Configure the static secret access key used to access the DynamoDB Catalog dynamodb.session-token AQoDYXdzEJr... Configure the static session token used to access the DynamoDB Catalog <p>Removed Properties</p> <p>The properties <code>profile_name</code>, <code>region_name</code>, <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, and <code>aws_session_token</code> were deprecated and removed in 0.8.0</p>"},{"location":"configuration/#custom-catalog-implementations","title":"Custom Catalog Implementations","text":"<p>If you want to load any custom catalog implementation, you can set catalog configurations like the following:</p> <pre><code>catalog:\n  default:\n    py-catalog-impl: mypackage.mymodule.MyCatalog\n    custom-key1: value1\n    custom-key2: value2\n</code></pre>"},{"location":"configuration/#unified-aws-credentials","title":"Unified AWS Credentials","text":"<p>You can explicitly set the AWS credentials for both Glue/DynamoDB Catalog and S3 FileIO by configuring <code>client.*</code> properties. For example:</p> <pre><code>catalog:\n  default:\n    type: glue\n    client.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    client.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    client.region: &lt;REGION_NAME&gt;\n</code></pre> <p>configures the AWS credentials for both Glue Catalog and S3 FileIO.</p> Key Example Description client.region us-east-1 Set the region of both the Glue/DynamoDB Catalog and the S3 FileIO client.access-key-id admin Configure the static access key id used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.secret-access-key password Configure the static secret access key used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.session-token AQoDYXdzEJr... Configure the static session token used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.role-session-name session An optional identifier for the assumed role session. client.role-arn arn:aws:... AWS Role ARN. If provided instead of access_key and secret_key, temporary credentials will be fetched by assuming this role. <p>Properties Priority</p> <p><code>client.*</code> properties will be overridden by service-specific properties if they are set. For example, if <code>client.region</code> is set to <code>us-west-1</code> and <code>s3.region</code> is set to <code>us-east-1</code>, the S3 FileIO will use <code>us-east-1</code> as the region.</p>"},{"location":"configuration/#concurrency","title":"Concurrency","text":"<p>PyIceberg uses multiple threads to parallelize operations. The number of workers can be configured by supplying a <code>max-workers</code> entry in the configuration file, or by setting the <code>PYICEBERG_MAX_WORKERS</code> environment variable. The default value depends on the system hardware and Python version. See the Python documentation for more details.</p>"},{"location":"configuration/#backward-compatibility","title":"Backward Compatibility","text":"<p>Previous versions of Java (<code>&lt;1.4.0</code>) implementations incorrectly assume the optional attribute <code>current-snapshot-id</code> to be a required attribute in TableMetadata. This means that if <code>current-snapshot-id</code> is missing in the metadata file (e.g. on table creation), the application will throw an exception without being able to load the table. This assumption has been corrected in more recent Iceberg versions. However, it is possible to force PyIceberg to create a table with a metadata file that will be compatible with previous versions. This can be configured by setting the <code>legacy-current-snapshot-id</code> property as \"True\" in the configuration file, or by setting the <code>PYICEBERG_LEGACY_CURRENT_SNAPSHOT_ID</code> environment variable. Refer to the PR discussion for more details on the issue</p>"},{"location":"configuration/#nanoseconds-support","title":"Nanoseconds Support","text":"<p>PyIceberg currently only supports upto microsecond precision in its TimestampType. PyArrow timestamp types in 's' and 'ms' will be upcast automatically to 'us' precision timestamps on write. Timestamps in 'ns' precision can also be downcast automatically on write if desired. This can be configured by setting the <code>downcast-ns-timestamp-to-us-on-write</code> property as \"True\" in the configuration file, or by setting the <code>PYICEBERG_DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE</code> environment variable. Refer to the nanoseconds timestamp proposal document for more details on the long term roadmap for nanoseconds support</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>We welcome contributions to Apache Iceberg! To learn more about contributing to Apache Iceberg, please refer to the official Iceberg contribution guidelines. These guidelines are intended as helpful suggestions to make the contribution process as seamless as possible, and are not strict rules.</p> <p>If you would like to discuss your proposed change before contributing, we encourage you to visit our Community page. There, you will find various ways to connect with the community, including Slack and our mailing lists. Alternatively, you can open a new issue directly in the GitHub repository.</p> <p>For first-time contributors, feel free to check out our good first issues for an easy way to get started.</p>"},{"location":"contributing/#contributing-to-pyiceberg","title":"Contributing to PyIceberg","text":"<p>The PyIceberg Project is hosted on GitHub at https://github.com/apache/iceberg-python.</p> <p>For the development, Poetry is used for packing and dependency management. You can install this using:</p> <pre><code>pip install poetry\n</code></pre> <p>Make sure you're using an up-to-date environment from venv</p> <pre><code>pip install --upgrade virtualenv pip\npython -m venv ./venv\nsource ./venv/bin/activate\n</code></pre> <p>To get started, you can run <code>make install</code>, which installs Poetry and all the dependencies of the Iceberg library. This also installs the development dependencies. If you don't want to install the development dependencies, you need to install using <code>poetry install --no-dev</code>.</p> <p>If you want to install the library on the host, you can simply run <code>pip3 install -e .</code>. If you wish to use a virtual environment, you can run <code>poetry shell</code>. Poetry will open up a virtual environment with all the dependencies set.</p> <p>Note: If you want to use <code>poetry shell</code>, you need to install it using <code>pip install poetry-plugin-shell</code>. Alternatively, you can run commands directly with <code>poetry run</code>.</p> <p>To set up IDEA with Poetry:</p> <ul> <li>Open up the Python project in IntelliJ</li> <li>Make sure that you're on latest main (that includes Poetry)</li> <li>Go to File -&gt; Project Structure (\u2318;)</li> <li>Go to Platform Settings -&gt; SDKs</li> <li>Click the + sign -&gt; Add Python SDK</li> <li>Select Poetry Environment from the left hand side bar and hit OK</li> <li>It can take some time to download all the dependencies based on your internet</li> <li>Go to Project Settings -&gt; Project</li> <li>Select the Poetry SDK from the SDK dropdown, and click OK</li> </ul> <p>For IDEA \u22642021 you need to install the Poetry integration as a plugin.</p> <p>Now you're set using Poetry, and all the tests will run in Poetry, and you'll have syntax highlighting in the pyproject.toml to indicate stale dependencies.</p>"},{"location":"contributing/#installation-from-source","title":"Installation from source","text":"<p>Clone the repository for local development:</p> <pre><code>git clone https://github.com/apache/iceberg-python.git\ncd iceberg-python\npip3 install -e \".[s3fs,hive]\"\n</code></pre> <p>Install it directly for GitHub (not recommended), but sometimes handy:</p> <pre><code>pip install \"git+https://github.com/apache/iceberg-python.git#egg=pyiceberg[pyarrow]\"\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p><code>pre-commit</code> is used for autoformatting and linting:</p> <pre><code>make lint\n</code></pre> <p>Pre-commit will automatically fix the violations such as import orders, formatting etc. Pylint errors you need to fix yourself.</p> <p>In contrast to the name suggest, it doesn't run the checks on the commit. If this is something that you like, you can set this up by running <code>pre-commit install</code>.</p> <p>You can bump the integrations to the latest version using <code>pre-commit autoupdate</code>. This will check if there is a newer version of <code>{black,mypy,isort,...}</code> and update the yaml.</p>"},{"location":"contributing/#cleaning","title":"Cleaning","text":"<p>Removal of old cached files generated during the Cython build process:</p> <pre><code>make clean\n</code></pre> <p>Helps prevent build failures and unexpected behavior by removing outdated files, ensuring that only up-to-date sources are used &amp; the build environment is always clean.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>For Python, <code>pytest</code> is used a testing framework in combination with <code>coverage</code> to enforce 90%+ code coverage.</p> <pre><code>make test\n</code></pre> <p>By default, S3 and ADLS tests are ignored because that require minio and azurite to be running. To run the S3 suite:</p> <pre><code>make test-s3\n</code></pre> <p>To run the ADLS suite:</p> <pre><code>make test-adls\n</code></pre> <p>To pass additional arguments to pytest, you can use <code>PYTEST_ARGS</code>.</p>"},{"location":"contributing/#run-pytest-in-verbose-mode","title":"Run pytest in verbose mode","text":"<pre><code>make test PYTEST_ARGS=\"-v\"\n</code></pre>"},{"location":"contributing/#run-pytest-with-pdb-enabled","title":"Run pytest with pdb enabled","text":"<pre><code>make test PYTEST_ARGS=\"--pdb\"\n</code></pre> <p>To see all available pytest arguments, run <code>make test PYTEST_ARGS=\"--help\"</code>.</p>"},{"location":"contributing/#integration-tests","title":"Integration tests","text":"<p>PyIceberg has integration tests with Apache Spark. Spark will create a new database and provision some tables that PyIceberg can query against.</p> <pre><code>make test-integration\n</code></pre> <p>This will restart the containers, to get to a clean state, and then run the PyTest suite. In case something changed in the Dockerfile or the provision script, you can run:</p> <pre><code>make test-integration-rebuild\n</code></pre> <p>To rebuild the containers from scratch.</p>"},{"location":"contributing/#code-standards","title":"Code standards","text":"<p>Below are the formalized conventions that we adhere to in the PyIceberg project. The goal of this is to have a common agreement on how to evolve the codebase, but also using it as guidelines for newcomers to the project.</p>"},{"location":"contributing/#api-compatibility","title":"API Compatibility","text":"<p>It is important to keep the Python public API compatible across versions. The Python official PEP-8 defines public methods as: Public attributes should have no leading underscores. This means not removing any methods without any notice, or removing or renaming any existing parameters. Adding new optional parameters is okay.</p> <p>If you want to remove a method, please add a deprecation notice by annotating the function using <code>@deprecated</code>:</p> <pre><code>from pyiceberg.utils.deprecated import deprecated\n\n\n@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\ndef load_something():\n    pass\n</code></pre> <p>Which will warn:</p> <pre><code>Call to load_something, deprecated in 0.1.0, will be removed in 0.2.0. Please use load_something_else() instead.\n</code></pre> <p>If you want to remove a property or notify about a behavior change, please add a deprecation notice by calling the deprecation_message function:</p> <pre><code>from pyiceberg.utils.deprecated import deprecation_message\n\ndeprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n</code></pre> <p>Which will warn:</p> <pre><code>Deprecated in 0.1.0, will be removed in 0.2.0. The old_property is deprecated. Please use the something_else property instead.\n</code></pre>"},{"location":"contributing/#type-annotations","title":"Type annotations","text":"<p>For the type annotation the types from the <code>Typing</code> package are used.</p> <p>PyIceberg offers support from Python 3.9 onwards, we can't use the type hints from the standard collections.</p>"},{"location":"contributing/#third-party-libraries","title":"Third party libraries","text":"<p>PyIceberg naturally integrates into the rich Python ecosystem, however it is important to be hesitant adding third party packages. Adding a lot of packages makes the library heavyweight, and causes incompatibilities with other projects if they use a different version of the library. Also, big libraries such as <code>s3fs</code>, <code>adlfs</code>, <code>pyarrow</code>, <code>thrift</code> should be optional to avoid downloading everything, while not being sure if is actually being used.</p>"},{"location":"how-to-release/","title":"How to release","text":""},{"location":"how-to-release/#how-to-release","title":"How to Release","text":"<p>This guide outlines the process for releasing PyIceberg in accordance with the Apache Release Process. The steps include:</p> <ol> <li>Preparing for a release</li> <li>Publishing a Release Candidate (RC)</li> <li>Community Voting and Validation</li> <li>Publishing the Final Release (if the vote passes)</li> <li>Post-Release Step</li> </ol>"},{"location":"how-to-release/#requirements","title":"Requirements","text":"<ul> <li>A GPG key must be registered and published in the Apache Iceberg KEYS file. Follow the instructions for setting up a GPG key and uploading it to the KEYS file.</li> <li>SVN Access<ul> <li>Permission to upload artifacts to the Apache development distribution (requires Apache Committer access).</li> <li>Permission to upload artifacts to the Apache release distribution (requires Apache PMC access).</li> </ul> </li> <li>PyPI Access<ul> <li>The <code>twine</code> package must be installed for uploading releases to PyPi.</li> <li>A PyPI account with publishing permissions for the pyiceberg project.</li> </ul> </li> </ul>"},{"location":"how-to-release/#preparing-for-a-release","title":"Preparing for a Release","text":""},{"location":"how-to-release/#remove-deprecated-apis","title":"Remove Deprecated APIs","text":"<p>Before running the release candidate, we want to remove any APIs that were marked for removal under the <code>@deprecated</code> tag for this release. See #1269.</p> <p>For example, the API with the following deprecation tag should be removed when preparing for the 0.2.0 release.</p> <pre><code>@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\n</code></pre> <p>We also have the <code>deprecation_message</code> function. We need to change the behavior according to what is noted in the message of that deprecation.</p> <pre><code>deprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n</code></pre>"},{"location":"how-to-release/#update-library-version","title":"Update Library Version","text":"<p>Update the version in <code>pyproject.toml</code> and <code>pyiceberg/__init__.py</code> to match the release version. See #1276.</p>"},{"location":"how-to-release/#publishing-a-release-candidate-rc","title":"Publishing a Release Candidate (RC)","text":""},{"location":"how-to-release/#release-types","title":"Release Types","text":""},{"location":"how-to-release/#majorminor-release","title":"Major/Minor Release","text":"<ul> <li>Use the <code>main</code> branch for the release.</li> <li>Includes new features, enhancements, and any necessary backward-compatible changes.</li> <li>Examples: <code>0.8.0</code>, <code>0.9.0</code>, <code>1.0.0</code>.</li> </ul>"},{"location":"how-to-release/#patch-release","title":"Patch Release","text":"<ul> <li>Use the branch corresponding to the patch version, such as <code>pyiceberg-0.8.x</code>.</li> <li>Focuses on critical bug fixes or security patches that maintain backward compatibility.</li> <li>Examples: <code>0.8.1</code>, <code>0.8.2</code>.</li> </ul> <p>To create a patch branch from the latest release tag:</p> <pre><code># Fetch all tags\ngit fetch --tags\n\n# Assuming 0.8.0 is the latest release tag\ngit checkout -b pyiceberg-0.8.x pyiceberg-0.8.0\n\n# Cherry-pick commits for the upcoming patch release\ngit cherry-pick &lt;commit&gt;\n</code></pre>"},{"location":"how-to-release/#create-tag","title":"Create Tag","text":"<p>Ensure you are on the correct branch:</p> <ul> <li>For a major/minor release, use the <code>main</code> branch</li> <li>For a patch release, use the branch corresponding to the patch version, i.e. <code>pyiceberg-0.6.x</code>.</li> </ul> <p>Create a signed tag:</p> <p>Replace <code>VERSION</code> and <code>RC</code> with the appropriate values for the release.</p> <pre><code>export VERSION=0.7.0\nexport RC=1\n\nexport VERSION_WITH_RC=${VERSION}rc${RC}\nexport GIT_TAG=pyiceberg-${VERSION_WITH_RC}\n\ngit tag -s ${GIT_TAG} -m \"PyIceberg ${VERSION_WITH_RC}\"\ngit push git@github.com:apache/iceberg-python.git ${GIT_TAG}\n</code></pre>"},{"location":"how-to-release/#create-artifacts","title":"Create Artifacts","text":"<p>The <code>Python Build Release Candidate</code> Github Action will run automatically upon tag push.</p> <p>This action will generate artifacts that will include both source distribution (<code>sdist</code>) and binary distributions (<code>wheels</code> using <code>cibuildwheel</code>) for each architectures.</p> <p>This action will generate two final artifacts:</p> <ul> <li><code>svn-release-candidate-${VERSION}rc${RC}</code> for SVN</li> <li><code>pypi-release-candidate-${VERSION}rc${RC}</code> for PyPi</li> </ul> <p>If <code>gh</code> is available, watch the GitHub Action progress using:</p> <pre><code>RUN_ID=$(gh run list --repo apache/iceberg-python --workflow \"Python Build Release Candidate\" --branch \"${GIT_TAG}\" --event push --json databaseId -q '.[0].databaseId')\necho \"Waiting for workflow to complete, this will take several minutes...\"\ngh run watch $RUN_ID --repo apache/iceberg-python\n</code></pre> <p>and download the artifacts using:</p> <pre><code>gh run download $RUN_ID --repo apache/iceberg-python\n</code></pre>"},{"location":"how-to-release/#publish-release-candidate-rc","title":"Publish Release Candidate (RC)","text":""},{"location":"how-to-release/#upload-to-apache-dev-svn","title":"Upload to Apache Dev SVN","text":""},{"location":"how-to-release/#download-artifacts-sign-and-generate-checksums","title":"Download Artifacts, Sign, and Generate Checksums","text":"<p>Download the SVN artifact from the GitHub Action and unzip it.</p> <p>Navigate to the artifact directory. Generate signature and checksum files:</p> <ul> <li><code>.asc</code> files: GPG-signed versions of each artifact to ensure authenticity.</li> <li><code>.sha512</code> files: SHA-512 checksums for verifying file integrity.</li> </ul> <pre><code>(\n    cd svn-release-candidate-${VERSION}rc${RC}\n\n    for name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\n    do\n        gpg --yes --armor --output \"${name}.asc\" --detach-sig \"${name}\"\n        shasum -a 512 \"${name}\" &gt; \"${name}.sha512\"\n    done\n)\n</code></pre> <p>The parentheses <code>()</code> create a subshell. Any changes to the directory (<code>cd</code>) are limited to this subshell, so the current directory in the parent shell remains unchanged.</p>"},{"location":"how-to-release/#upload-artifacts-to-apache-dev-svn","title":"Upload Artifacts to Apache Dev SVN","text":"<p>Now, upload the files from the same directory:</p> <pre><code>export SVN_TMP_DIR=/tmp/iceberg-${VERSION}/\nsvn checkout https://dist.apache.org/repos/dist/dev/iceberg $SVN_TMP_DIR\n\nexport SVN_TMP_DIR_VERSIONED=${SVN_TMP_DIR}pyiceberg-$VERSION_WITH_RC/\nmkdir -p $SVN_TMP_DIR_VERSIONED\ncp svn-release-candidate-${VERSION}rc${RC}/* $SVN_TMP_DIR_VERSIONED\nsvn add $SVN_TMP_DIR_VERSIONED\nsvn ci -m \"PyIceberg ${VERSION_WITH_RC}\" ${SVN_TMP_DIR_VERSIONED}\n</code></pre> <p>Verify the artifact is uploaded to https://dist.apache.org/repos/dist/dev/iceberg.</p>"},{"location":"how-to-release/#remove-old-artifacts-from-apache-dev-svn","title":"Remove Old Artifacts From Apache Dev SVN","text":"<p>Clean up old RC artifacts:</p> <pre><code>svn delete https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-&lt;OLD_RC_VERSION&gt; -m \"Remove old RC artifacts\"\n</code></pre>"},{"location":"how-to-release/#upload-to-pypi","title":"Upload to PyPi","text":""},{"location":"how-to-release/#download-artifacts","title":"Download Artifacts","text":"<p>Download the PyPi artifact from the GitHub Action and unzip it.</p>"},{"location":"how-to-release/#upload-artifacts-to-pypi","title":"Upload Artifacts to PyPi","text":"<p>Update the artifact directory to PyPi using <code>twine</code>. This won't bump the version for everyone that hasn't pinned their version, since it is set to an RC pre-release and those are ignored.</p> <p>Note</p> <p><code>twine</code> might require an PyPi API token.</p> <pre><code>twine upload pypi-release-candidate-${VERSION}rc${RC}/*\n</code></pre> <p>Verify the artifact is uploaded to PyPi.</p>"},{"location":"how-to-release/#vote","title":"Vote","text":""},{"location":"how-to-release/#generate-vote-email","title":"Generate Vote Email","text":"<p>Final step is to generate the email to the dev mail list:</p> <pre><code>export GIT_TAG_REF=$(git show-ref ${GIT_TAG})\nexport GIT_TAG_HASH=${GIT_TAG_REF:0:40}\nexport LAST_COMMIT_ID=$(git rev-list ${GIT_TAG} 2&gt; /dev/null | head -n 1)\n\ncat &lt;&lt; EOF &gt; release-announcement-email.txt\nTo: dev@iceberg.apache.org\nSubject: [VOTE] Release Apache PyIceberg $VERSION_WITH_RC\nHi Everyone,\n\nI propose that we release the following RC as the official PyIceberg $VERSION release.\n\nA summary of the high level features:\n\n* &lt;Add summary by hand&gt;\n\nThe commit ID is $LAST_COMMIT_ID\n\n* This corresponds to the tag: $GIT_TAG ($GIT_TAG_HASH)\n* https://github.com/apache/iceberg-python/releases/tag/$GIT_TAG\n* https://github.com/apache/iceberg-python/tree/$LAST_COMMIT_ID\n\nThe release tarball, signature, and checksums are here:\n\n* https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-$VERSION_WITH_RC/\n\nYou can find the KEYS file here:\n\n* https://downloads.apache.org/iceberg/KEYS\n\nConvenience binary artifacts are staged on pypi:\n\nhttps://pypi.org/project/pyiceberg/$VERSION_WITH_RC/\n\nAnd can be installed using: pip3 install pyiceberg==$VERSION_WITH_RC\n\nInstructions for verifying a release can be found here:\n\n* https://py.iceberg.apache.org/verify-release/\n\nPlease download, verify, and test.\n\nPlease vote in the next 72 hours.\n[ ] +1 Release this as PyIceberg $VERSION\n[ ] +0\n[ ] -1 Do not release this because...\nEOF\n</code></pre>"},{"location":"how-to-release/#send-vote-email","title":"Send Vote Email","text":"<p>Verify the content of <code>release-announcement-email.txt</code> and send it to <code>dev@iceberg.apache.org</code> with the corresponding subject line.</p>"},{"location":"how-to-release/#vote-has-failed","title":"Vote has failed","text":"<p>If there are concerns with the RC, address the issues and generate another RC.</p>"},{"location":"how-to-release/#publish-the-final-release-vote-has-passed","title":"Publish the Final Release (Vote has passed)","text":"<p>A minimum of 3 binding +1 votes is required to pass an RC. Once the vote has been passed, you can close the vote thread by concluding it:</p> <pre><code>Thanks everyone for voting! The 72 hours have passed, and a minimum of 3 binding votes have been cast:\n\n+1 Foo Bar (non-binding)\n...\n+1 Fokko Driesprong (binding)\n\nThe release candidate has been accepted as PyIceberg &lt;VERSION&gt;. Thanks everyone, when all artifacts are published the announcement will be sent out.\n\nKind regards,\n</code></pre>"},{"location":"how-to-release/#upload-the-accepted-rc-to-apache-release-svn","title":"Upload the accepted RC to Apache Release SVN","text":"<p>Note</p> <p>Only a PMC member has the permission to upload an artifact to the SVN release dist.</p> <pre><code>export SVN_DEV_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-${VERSION_WITH_RC}\"\nexport SVN_RELEASE_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-${VERSION}\"\n\nsvn mv ${SVN_DEV_DIR_VERSIONED} ${SVN_RELEASE_DIR_VERSIONED} -m \"PyIceberg: Add release ${VERSION}\"\n</code></pre> <p>Verify the artifact is uploaded to https://dist.apache.org/repos/dist/release/iceberg.</p>"},{"location":"how-to-release/#remove-old-artifacts-from-apache-release-svn","title":"Remove Old Artifacts From Apache Release SVN","text":"<p>We only want to host the latest release. Clean up old release artifacts:</p> <pre><code>svn delete https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-&lt;OLD_RELEASE_VERSION&gt; -m \"Remove old release artifacts\"\n</code></pre>"},{"location":"how-to-release/#upload-the-accepted-release-to-pypi","title":"Upload the accepted release to PyPi","text":"<p>The latest version can be pushed to PyPi. Check out the Apache SVN and make sure to publish the right version with <code>twine</code>:</p> <p>Note</p> <p><code>twine</code> might require an PyPi API token.</p> <pre><code>svn checkout https://dist.apache.org/repos/dist/release/iceberg /tmp/iceberg-dist-release/\ncd /tmp/iceberg-dist-release/pyiceberg-${VERSION}\ntwine upload pyiceberg-*.whl pyiceberg-*.tar.gz\n</code></pre> <p>Verify the artifact is uploaded to PyPi.</p>"},{"location":"how-to-release/#post-release","title":"Post Release","text":""},{"location":"how-to-release/#send-out-release-announcement-email","title":"Send out Release Announcement Email","text":"<p>Send out an announcement on the dev mail list:</p> <pre><code>To: dev@iceberg.apache.org\nSubject: [ANNOUNCE] Apache PyIceberg release &lt;VERSION&gt;\n\nI'm pleased to announce the release of Apache PyIceberg &lt;VERSION&gt;!\n\nApache Iceberg is an open table format for huge analytic datasets. Iceberg\ndelivers high query performance for tables with tens of petabytes of data,\nalong with atomic commits, concurrent writes, and SQL-compatible table\nevolution.\n\nThis Python release can be downloaded from: https://pypi.org/project/pyiceberg/&lt;VERSION&gt;/\n\nThanks to everyone for contributing!\n</code></pre>"},{"location":"how-to-release/#create-a-github-release-note","title":"Create a Github Release Note","text":"<p>Create a new Release Note on the iceberg-python Github repository.</p> <p>Input the tag in Choose a tag with the newly approved released version (e.g. <code>0.7.0</code>) and set it to Create new tag on publish. Pick the target commit version as the commit ID the release was approved on. For example: </p> <p>Then, select the previous release version as the Previous tag to use the diff between the two versions in generating the release notes.</p> <p>Generate release notes.</p> <p>Set as the latest release and Publish.</p>"},{"location":"how-to-release/#release-the-docs","title":"Release the docs","text":"<p>Run the <code>Release Docs</code> Github Action.</p>"},{"location":"how-to-release/#update-the-github-template","title":"Update the Github template","text":"<p>Make sure to create a PR to update the GitHub issues template with the latest version.</p>"},{"location":"how-to-release/#update-the-integration-tests","title":"Update the integration tests","text":"<p>Ensure to update the <code>PYICEBERG_VERSION</code> in the Dockerfile.</p>"},{"location":"how-to-release/#misc","title":"Misc","text":""},{"location":"how-to-release/#set-up-gpg-key-and-upload-to-apache-iceberg-keys-file","title":"Set up GPG key and Upload to Apache Iceberg KEYS file","text":"<p>To set up GPG key locally, see the instructions here.</p> <p>To install gpg on a M1 based Mac, a couple of additional steps are required: https://gist.github.com/phortuin/cf24b1cca3258720c71ad42977e1ba57.</p> <p>Then, published GPG key to the Apache Iceberg KEYS file:</p> <pre><code>svn co https://dist.apache.org/repos/dist/release/iceberg icebergsvn\ncd icebergsvn\necho \"\" &gt;&gt; KEYS # append a newline\ngpg --list-sigs &lt;YOUR KEY ID HERE&gt; &gt;&gt; KEYS # append signatures\ngpg --armor --export &lt;YOUR KEY ID HERE&gt; &gt;&gt; KEYS # append public key block\nsvn commit -m \"add key for &lt;YOUR NAME HERE&gt;\"\n</code></pre>"},{"location":"nightly-build/","title":"Nightly Build","text":""},{"location":"nightly-build/#nightly-build","title":"Nightly Build","text":"<p>A nightly build of PyIceberg is available on testpypi, https://test.pypi.org/project/pyiceberg/.</p> <p>To install the nightly build,</p> <pre><code>pip install -i https://test.pypi.org/simple/ --pre pyiceberg\n</code></pre> <p>For Testing Purposes Only</p> <p>Nightly builds are for testing purposes only and have not been validated. Please use at your own risk, as they may contain untested changes, potential bugs, or incomplete features. Additionally, ensure compliance with any applicable licenses, as these builds may include changes that have not been reviewed for legal or licensing implications.</p>"},{"location":"verify-release/","title":"Verify a release","text":""},{"location":"verify-release/#verifying-a-release","title":"Verifying a release","text":"<p>Each Apache PyIceberg release is validated by the community by holding a vote. A community release manager will prepare a release candidate and call a vote on the Iceberg dev list. To validate the release candidate, community members will test it out in their downstream projects and environments.</p> <p>In addition to testing in downstream projects, community members also check the release\u2019s signatures, checksums, and license documentation.</p>"},{"location":"verify-release/#validating-a-release-candidate","title":"Validating a release candidate","text":"<p>Release announcements include links to the following:</p> <ul> <li>A source tarball</li> <li>A signature (.asc)</li> <li>A checksum (.sha512)</li> <li>KEYS file</li> <li>GitHub change comparison</li> </ul> <p>After downloading the source tarball, signature, checksum, and KEYS file, here are instructions on how to verify signatures, checksums, and documentation.</p>"},{"location":"verify-release/#verifying-signatures","title":"Verifying signatures","text":"<p>First, import the keys.</p> <pre><code>curl https://downloads.apache.org/iceberg/KEYS -o KEYS\ngpg --import KEYS\n</code></pre> <p>Set an environment variable to the version to verify and path to use</p> <pre><code>export PYICEBERG_VERSION=&lt;version&gt; # e.g. 0.6.1rc3\nexport PYICEBERG_VERIFICATION_DIR=/tmp/pyiceberg/${PYICEBERG_VERSION}\n</code></pre> <p>Next, verify the <code>.asc</code> file.</p> <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-${PYICEBERG_VERSION}/ ${PYICEBERG_VERIFICATION_DIR}\n\ncd ${PYICEBERG_VERIFICATION_DIR}\n\nfor name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\ndo\n    gpg --verify ${name}.asc ${name}\ndone\n</code></pre>"},{"location":"verify-release/#verifying-checksums","title":"Verifying checksums","text":"<pre><code>cd ${PYICEBERG_VERIFICATION_DIR}\nfor name in $(ls pyiceberg-*.whl.sha512 pyiceberg-*.tar.gz.sha512)\ndo\n    shasum -a 512 --check ${name}\ndone\n</code></pre>"},{"location":"verify-release/#verifying-license-documentation","title":"Verifying License Documentation","text":"<pre><code>export PYICEBERG_RELEASE_VERSION=${PYICEBERG_VERSION/rc?/}  # remove rcX qualifier\ntar xzf pyiceberg-${PYICEBERG_RELEASE_VERSION}.tar.gz\ncd pyiceberg-${PYICEBERG_RELEASE_VERSION}\n</code></pre> <p>Run RAT checks to validate license header:</p> <pre><code>./dev/check-license\n</code></pre>"},{"location":"verify-release/#testing","title":"Testing","text":"<p>This section explains how to run the tests of the source distribution.</p> <p>Python Version</p> <p>Make sure you're using a supported Python version</p> <p>First step is to install the package:</p> <pre><code>make install\n</code></pre> <p>To run the full test coverage, with both unit tests and integration tests:</p> <pre><code>make test-coverage\n</code></pre> <p>This will spin up Docker containers to facilitate running test coverage.</p>"},{"location":"verify-release/#cast-the-vote","title":"Cast the vote","text":"<p>Votes are cast by replying to the release candidate announcement email on the dev mailing list with either <code>+1</code>, <code>0</code>, or <code>-1</code>. For example :</p> <p>[ ] +1 Release this as PyIceberg 0.3.0</p> <p>[ ] +0</p> <p>[ ] -1 Do not release this because\u2026</p> <p>In addition to your vote, it\u2019s customary to specify if your vote is binding or non-binding. Only members of the Project Management Committee have formally binding votes. If you\u2019re unsure, you can specify that your vote is non-binding. To read more about voting in the Apache framework, checkout the Voting information page on the Apache foundation\u2019s website.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>pyiceberg<ul> <li>avro<ul> <li>codecs<ul> <li>bzip2</li> <li>codec</li> <li>deflate</li> <li>snappy_codec</li> <li>zstandard_codec</li> </ul> </li> <li>decoder</li> <li>encoder</li> <li>file</li> <li>reader</li> <li>resolver</li> <li>writer</li> </ul> </li> <li>catalog<ul> <li>dynamodb</li> <li>glue</li> <li>hive</li> <li>memory</li> <li>noop</li> <li>rest</li> <li>sql</li> </ul> </li> <li>cli<ul> <li>console</li> <li>output</li> </ul> </li> <li>conversions</li> <li>exceptions</li> <li>expressions<ul> <li>literals</li> <li>parser</li> <li>visitors</li> </ul> </li> <li>io<ul> <li>fsspec</li> <li>pyarrow</li> </ul> </li> <li>manifest</li> <li>partitioning</li> <li>schema</li> <li>serializers</li> <li>table<ul> <li>inspect</li> <li>locations</li> <li>metadata</li> <li>name_mapping</li> <li>refs</li> <li>snapshots</li> <li>sorting</li> <li>statistics</li> <li>update<ul> <li>schema</li> <li>snapshot</li> <li>spec</li> <li>statistics</li> </ul> </li> <li>upsert_util</li> </ul> </li> <li>transforms</li> <li>typedef</li> <li>types</li> <li>utils<ul> <li>bin_packing</li> <li>concurrent</li> <li>config</li> <li>datetime</li> <li>decimal</li> <li>deprecated</li> <li>lazydict</li> <li>parsing</li> <li>properties</li> <li>schema_conversion</li> <li>singleton</li> <li>truncate</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/pyiceberg/","title":"pyiceberg","text":""},{"location":"reference/pyiceberg/conversions/","title":"conversions","text":"<p>Utility module for various conversions around PrimitiveType implementations.</p> This module enables <ul> <li>Converting partition strings to built-in python objects.</li> <li>Converting a value to a byte buffer.</li> <li>Converting a byte buffer to a value.</li> </ul> Note <p>Conversion logic varies based on the PrimitiveType implementation. Therefore conversion functions are defined here as generic functions using the @singledispatch decorator. For each PrimitiveType implementation, a concrete function is registered for each generic conversion function. For PrimitiveType implementations that share the same conversion logic, registrations can be stacked.</p>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.from_bytes","title":"<code>from_bytes(primitive_type, b)</code>","text":"<p>Convert bytes to a built-in python value.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>b</code> <code>bytes</code> <p>The bytes to convert.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch  # type: ignore\ndef from_bytes(primitive_type: PrimitiveType, b: bytes) -&gt; L:  # type: ignore\n    \"\"\"Convert bytes to a built-in python value.\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        b (bytes): The bytes to convert.\n    \"\"\"\n    raise TypeError(f\"Cannot deserialize bytes, type {primitive_type} not supported: {str(b)}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.handle_none","title":"<code>handle_none(func)</code>","text":"<p>Handle cases where partition values are <code>None</code> or \"HIVE_DEFAULT_PARTITION\".</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function registered to the singledispatch function <code>partition_to_py</code>.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>def handle_none(func: Callable) -&gt; Callable:  # type: ignore\n    \"\"\"Handle cases where partition values are `None` or \"__HIVE_DEFAULT_PARTITION__\".\n\n    Args:\n        func (Callable): A function registered to the singledispatch function `partition_to_py`.\n    \"\"\"\n\n    def wrapper(primitive_type: PrimitiveType, value_str: Optional[str]) -&gt; Any:\n        if value_str is None:\n            return None\n        elif value_str == \"__HIVE_DEFAULT_PARTITION__\":\n            return None\n        return func(primitive_type, value_str)\n\n    return wrapper\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.partition_to_py","title":"<code>partition_to_py(primitive_type, value_str)</code>","text":"<p>Convert a partition string to a python built-in.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>value_str</code> <code>str</code> <p>A string representation of a partition value.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch\ndef partition_to_py(primitive_type: PrimitiveType, value_str: str) -&gt; Union[int, float, str, uuid.UUID, bytes, Decimal]:\n    \"\"\"Convert a partition string to a python built-in.\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        value_str (str): A string representation of a partition value.\n    \"\"\"\n    raise TypeError(f\"Cannot convert '{value_str}' to unsupported type: {primitive_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.to_bytes","title":"<code>to_bytes(primitive_type, _)</code>","text":"<p>Convert a built-in python value to bytes.</p> <p>This conversion follows the serialization scheme for storing single values as individual binary values defined in the Iceberg specification that can be found at https://iceberg.apache.org/spec/#appendix-d-single-value-serialization</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>_</code> <code>Union[bool, bytes, Decimal, date, datetime, float, int, str, time, UUID]</code> <p>The value to convert to bytes (The type of this value depends on which dispatched function is used--check dispatchable functions for type hints).</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch\ndef to_bytes(\n    primitive_type: PrimitiveType, _: Union[bool, bytes, Decimal, date, datetime, float, int, str, time, uuid.UUID]\n) -&gt; bytes:\n    \"\"\"Convert a built-in python value to bytes.\n\n    This conversion follows the serialization scheme for storing single values as individual binary values defined in the Iceberg specification that\n    can be found at https://iceberg.apache.org/spec/#appendix-d-single-value-serialization\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        _: The value to convert to bytes (The type of this value depends on which dispatched function is\n            used--check dispatchable functions for type hints).\n    \"\"\"\n    raise TypeError(f\"scale does not match {primitive_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/","title":"exceptions","text":""},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.AuthorizationExpiredError","title":"<code>AuthorizationExpiredError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>When the credentials are expired when performing an action on the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class AuthorizationExpiredError(RESTError):\n    \"\"\"When the credentials are expired when performing an action on the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.BadRequestError","title":"<code>BadRequestError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when an invalid request is being made.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class BadRequestError(RESTError):\n    \"\"\"Raises when an invalid request is being made.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.CommitFailedException","title":"<code>CommitFailedException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Commit failed, refresh and try again.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class CommitFailedException(Exception):\n    \"\"\"Commit failed, refresh and try again.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.CommitStateUnknownException","title":"<code>CommitStateUnknownException</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Commit failed due to unknown reason.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class CommitStateUnknownException(RESTError):\n    \"\"\"Commit failed due to unknown reason.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ForbiddenError","title":"<code>ForbiddenError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when you don't have the credentials to perform the action on the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ForbiddenError(RESTError):\n    \"\"\"Raises when you don't have the credentials to perform the action on the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NamespaceAlreadyExistsError","title":"<code>NamespaceAlreadyExistsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a name-space being created already exists in the catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NamespaceAlreadyExistsError(Exception):\n    \"\"\"Raised when a name-space being created already exists in the catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NamespaceNotEmptyError","title":"<code>NamespaceNotEmptyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a name-space being dropped is not empty.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NamespaceNotEmptyError(Exception):\n    \"\"\"Raised when a name-space being dropped is not empty.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchIcebergTableError","title":"<code>NoSuchIcebergTableError</code>","text":"<p>               Bases: <code>NoSuchTableError</code></p> <p>Raises when the table found in the REST catalog is not an iceberg table.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchIcebergTableError(NoSuchTableError):\n    \"\"\"Raises when the table found in the REST catalog is not an iceberg table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchIdentifierError","title":"<code>NoSuchIdentifierError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the identifier can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchIdentifierError(Exception):\n    \"\"\"Raises when the identifier can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchNamespaceError","title":"<code>NoSuchNamespaceError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a referenced name-space is not found.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchNamespaceError(Exception):\n    \"\"\"Raised when a referenced name-space is not found.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchPropertyException","title":"<code>NoSuchPropertyException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>When a property is missing.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchPropertyException(Exception):\n    \"\"\"When a property is missing.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchTableError","title":"<code>NoSuchTableError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the table can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchTableError(Exception):\n    \"\"\"Raises when the table can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchViewError","title":"<code>NoSuchViewError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the view can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchViewError(Exception):\n    \"\"\"Raises when the view can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NotInstalledError","title":"<code>NotInstalledError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>When an optional dependency is not installed.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NotInstalledError(Exception):\n    \"\"\"When an optional dependency is not installed.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.OAuthError","title":"<code>OAuthError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when there is an error with the OAuth call.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class OAuthError(RESTError):\n    \"\"\"Raises when there is an error with the OAuth call.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.RESTError","title":"<code>RESTError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when there is an unknown response from the REST Catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class RESTError(Exception):\n    \"\"\"Raises when there is an unknown response from the REST Catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ServerError","title":"<code>ServerError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when there is an unhandled exception on the server side.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ServerError(RESTError):\n    \"\"\"Raises when there is an unhandled exception on the server side.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ServiceUnavailableError","title":"<code>ServiceUnavailableError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when the service doesn't respond.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ServiceUnavailableError(RESTError):\n    \"\"\"Raises when the service doesn't respond.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.SignError","title":"<code>SignError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when unable to sign a S3 request.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class SignError(Exception):\n    \"\"\"Raises when unable to sign a S3 request.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.TableAlreadyExistsError","title":"<code>TableAlreadyExistsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when creating a table with a name that already exists.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class TableAlreadyExistsError(Exception):\n    \"\"\"Raised when creating a table with a name that already exists.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.UnauthorizedError","title":"<code>UnauthorizedError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when you don't have the proper authorization.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class UnauthorizedError(RESTError):\n    \"\"\"Raises when you don't have the proper authorization.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when there is an issue with the schema.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ValidationError(Exception):\n    \"\"\"Raises when there is an issue with the schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.WaitingForLockException","title":"<code>WaitingForLockException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Need to wait for a lock, try again.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class WaitingForLockException(Exception):\n    \"\"\"Need to wait for a lock, try again.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/","title":"manifest","text":""},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile","title":"<code>DataFile</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class DataFile(Record):\n    __slots__ = (\n        \"content\",\n        \"file_path\",\n        \"file_format\",\n        \"partition\",\n        \"record_count\",\n        \"file_size_in_bytes\",\n        \"column_sizes\",\n        \"value_counts\",\n        \"null_value_counts\",\n        \"nan_value_counts\",\n        \"lower_bounds\",\n        \"upper_bounds\",\n        \"key_metadata\",\n        \"split_offsets\",\n        \"equality_ids\",\n        \"sort_order_id\",\n        \"spec_id\",\n    )\n    content: DataFileContent\n    file_path: str\n    file_format: FileFormat\n    partition: Record\n    record_count: int\n    file_size_in_bytes: int\n    column_sizes: Dict[int, int]\n    value_counts: Dict[int, int]\n    null_value_counts: Dict[int, int]\n    nan_value_counts: Dict[int, int]\n    lower_bounds: Dict[int, bytes]\n    upper_bounds: Dict[int, bytes]\n    key_metadata: Optional[bytes]\n    split_offsets: Optional[List[int]]\n    equality_ids: Optional[List[int]]\n    sort_order_id: Optional[int]\n    spec_id: int\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"Assign a key/value to a DataFile.\"\"\"\n        # The file_format is written as a string, so we need to cast it to the Enum\n        if name == \"file_format\":\n            value = FileFormat[value]\n        super().__setattr__(name, value)\n\n    def __init__(self, format_version: TableVersion = DEFAULT_READ_VERSION, *data: Any, **named_data: Any) -&gt; None:\n        super().__init__(\n            *data,\n            **{\"struct\": DATA_FILE_TYPE[format_version], **named_data},\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the file path.\"\"\"\n        return hash(self.file_path)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the datafile with another object.\n\n        If it is a datafile, it will compare based on the file_path.\n        \"\"\"\n        return self.file_path == other.file_path if isinstance(other, DataFile) else False\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the datafile with another object.</p> <p>If it is a datafile, it will compare based on the file_path.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the datafile with another object.\n\n    If it is a datafile, it will compare based on the file_path.\n    \"\"\"\n    return self.file_path == other.file_path if isinstance(other, DataFile) else False\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the file path.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the file path.\"\"\"\n    return hash(self.file_path)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Assign a key/value to a DataFile.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"Assign a key/value to a DataFile.\"\"\"\n    # The file_format is written as a string, so we need to cast it to the Enum\n    if name == \"file_format\":\n        value = FileFormat[value]\n    super().__setattr__(name, value)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFileContent","title":"<code>DataFileContent</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class DataFileContent(int, Enum):\n    DATA = 0\n    POSITION_DELETES = 1\n    EQUALITY_DELETES = 2\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DataFileContent class.\"\"\"\n        return f\"DataFileContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFileContent.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DataFileContent class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DataFileContent class.\"\"\"\n    return f\"DataFileContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.FileFormat","title":"<code>FileFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class FileFormat(str, Enum):\n    AVRO = \"AVRO\"\n    PARQUET = \"PARQUET\"\n    ORC = \"ORC\"\n\n    @classmethod\n    def _missing_(cls, value: object) -&gt; Union[None, str]:\n        for member in cls:\n            if member.value == str(value).upper():\n                return member\n        return None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FileFormat class.\"\"\"\n        return f\"FileFormat.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.FileFormat.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FileFormat class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FileFormat class.\"\"\"\n    return f\"FileFormat.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestContent","title":"<code>ManifestContent</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestContent(int, Enum):\n    DATA = 0\n    DELETES = 1\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the ManifestContent class.\"\"\"\n        return f\"ManifestContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestContent.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the ManifestContent class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the ManifestContent class.\"\"\"\n    return f\"ManifestContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestEntryStatus","title":"<code>ManifestEntryStatus</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestEntryStatus(int, Enum):\n    EXISTING = 0\n    ADDED = 1\n    DELETED = 2\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the ManifestEntryStatus class.\"\"\"\n        return f\"ManifestEntryStatus.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestEntryStatus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the ManifestEntryStatus class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the ManifestEntryStatus class.\"\"\"\n    return f\"ManifestEntryStatus.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile","title":"<code>ManifestFile</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestFile(Record):\n    __slots__ = (\n        \"manifest_path\",\n        \"manifest_length\",\n        \"partition_spec_id\",\n        \"content\",\n        \"sequence_number\",\n        \"min_sequence_number\",\n        \"added_snapshot_id\",\n        \"added_files_count\",\n        \"existing_files_count\",\n        \"deleted_files_count\",\n        \"added_rows_count\",\n        \"existing_rows_count\",\n        \"deleted_rows_count\",\n        \"partitions\",\n        \"key_metadata\",\n    )\n    manifest_path: str\n    manifest_length: int\n    partition_spec_id: int\n    content: ManifestContent\n    sequence_number: int\n    min_sequence_number: int\n    added_snapshot_id: int\n    added_files_count: Optional[int]\n    existing_files_count: Optional[int]\n    deleted_files_count: Optional[int]\n    added_rows_count: Optional[int]\n    existing_rows_count: Optional[int]\n    deleted_rows_count: Optional[int]\n    partitions: Optional[List[PartitionFieldSummary]]\n    key_metadata: Optional[bytes]\n\n    def __init__(self, *data: Any, **named_data: Any) -&gt; None:\n        super().__init__(*data, **{\"struct\": MANIFEST_LIST_FILE_STRUCTS[DEFAULT_READ_VERSION], **named_data})\n\n    def has_added_files(self) -&gt; bool:\n        return self.added_files_count is None or self.added_files_count &gt; 0\n\n    def has_existing_files(self) -&gt; bool:\n        return self.existing_files_count is None or self.existing_files_count &gt; 0\n\n    def fetch_manifest_entry(self, io: FileIO, discard_deleted: bool = True) -&gt; List[ManifestEntry]:\n        \"\"\"\n        Read the manifest entries from the manifest file.\n\n        Args:\n            io: The FileIO to fetch the file.\n            discard_deleted: Filter on live entries.\n\n        Returns:\n            An Iterator of manifest entries.\n        \"\"\"\n        input_file = io.new_input(self.manifest_path)\n        with AvroFile[ManifestEntry](\n            input_file,\n            MANIFEST_ENTRY_SCHEMAS[DEFAULT_READ_VERSION],\n            read_types={-1: ManifestEntry, 2: DataFile},\n            read_enums={0: ManifestEntryStatus, 101: FileFormat, 134: DataFileContent},\n        ) as reader:\n            return [\n                _inherit_from_manifest(entry, self)\n                for entry in reader\n                if not discard_deleted or entry.status != ManifestEntryStatus.DELETED\n            ]\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile.fetch_manifest_entry","title":"<code>fetch_manifest_entry(io, discard_deleted=True)</code>","text":"<p>Read the manifest entries from the manifest file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO to fetch the file.</p> required <code>discard_deleted</code> <code>bool</code> <p>Filter on live entries.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ManifestEntry]</code> <p>An Iterator of manifest entries.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def fetch_manifest_entry(self, io: FileIO, discard_deleted: bool = True) -&gt; List[ManifestEntry]:\n    \"\"\"\n    Read the manifest entries from the manifest file.\n\n    Args:\n        io: The FileIO to fetch the file.\n        discard_deleted: Filter on live entries.\n\n    Returns:\n        An Iterator of manifest entries.\n    \"\"\"\n    input_file = io.new_input(self.manifest_path)\n    with AvroFile[ManifestEntry](\n        input_file,\n        MANIFEST_ENTRY_SCHEMAS[DEFAULT_READ_VERSION],\n        read_types={-1: ManifestEntry, 2: DataFile},\n        read_enums={0: ManifestEntryStatus, 101: FileFormat, 134: DataFileContent},\n    ) as reader:\n        return [\n            _inherit_from_manifest(entry, self)\n            for entry in reader\n            if not discard_deleted or entry.status != ManifestEntryStatus.DELETED\n        ]\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter","title":"<code>ManifestListWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestListWriter(ABC):\n    _format_version: TableVersion\n    _output_file: OutputFile\n    _meta: Dict[str, str]\n    _manifest_files: List[ManifestFile]\n    _commit_snapshot_id: int\n    _writer: AvroOutputFile[ManifestFile]\n\n    def __init__(self, format_version: TableVersion, output_file: OutputFile, meta: Dict[str, Any]):\n        self._format_version = format_version\n        self._output_file = output_file\n        self._meta = meta\n        self._manifest_files = []\n\n    def __enter__(self) -&gt; ManifestListWriter:\n        \"\"\"Open the writer for writing.\"\"\"\n        self._writer = AvroOutputFile[ManifestFile](\n            output_file=self._output_file,\n            record_schema=MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n            file_schema=MANIFEST_LIST_FILE_SCHEMAS[self._format_version],\n            schema_name=\"manifest_file\",\n            metadata=self._meta,\n        )\n        self._writer.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_value: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -&gt; None:\n        \"\"\"Close the writer.\"\"\"\n        self._writer.__exit__(exc_type, exc_value, traceback)\n        return\n\n    @abstractmethod\n    def prepare_manifest(self, manifest_file: ManifestFile) -&gt; ManifestFile: ...\n\n    def add_manifests(self, manifest_files: List[ManifestFile]) -&gt; ManifestListWriter:\n        self._writer.write_block([self.prepare_manifest(manifest_file) for manifest_file in manifest_files])\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the writer for writing.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __enter__(self) -&gt; ManifestListWriter:\n    \"\"\"Open the writer for writing.\"\"\"\n    self._writer = AvroOutputFile[ManifestFile](\n        output_file=self._output_file,\n        record_schema=MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n        file_schema=MANIFEST_LIST_FILE_SCHEMAS[self._format_version],\n        schema_name=\"manifest_file\",\n        metadata=self._meta,\n    )\n    self._writer.__enter__()\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: Optional[Type[BaseException]],\n    exc_value: Optional[BaseException],\n    traceback: Optional[TracebackType],\n) -&gt; None:\n    \"\"\"Close the writer.\"\"\"\n    self._writer.__exit__(exc_type, exc_value, traceback)\n    return\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter","title":"<code>ManifestWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestWriter(ABC):\n    closed: bool\n    _spec: PartitionSpec\n    _schema: Schema\n    _output_file: OutputFile\n    _writer: AvroOutputFile[ManifestEntry]\n    _snapshot_id: int\n    _added_files: int\n    _added_rows: int\n    _existing_files: int\n    _existing_rows: int\n    _deleted_files: int\n    _deleted_rows: int\n    _min_sequence_number: Optional[int]\n    _partitions: List[Record]\n    _reused_entry_wrapper: ManifestEntry\n\n    def __init__(self, spec: PartitionSpec, schema: Schema, output_file: OutputFile, snapshot_id: int) -&gt; None:\n        self.closed = False\n        self._spec = spec\n        self._schema = schema\n        self._output_file = output_file\n        self._snapshot_id = snapshot_id\n\n        self._added_files = 0\n        self._added_rows = 0\n        self._existing_files = 0\n        self._existing_rows = 0\n        self._deleted_files = 0\n        self._deleted_rows = 0\n        self._min_sequence_number = None\n        self._partitions = []\n        self._reused_entry_wrapper = ManifestEntry()\n\n    def __enter__(self) -&gt; ManifestWriter:\n        \"\"\"Open the writer.\"\"\"\n        self._writer = self.new_writer()\n        self._writer.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_value: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -&gt; None:\n        \"\"\"Close the writer.\"\"\"\n        if (self._added_files + self._existing_files + self._deleted_files) == 0:\n            # This is just a guard to ensure that we don't write empty manifest files\n            raise ValueError(\"An empty manifest file has been written\")\n\n        self.closed = True\n        self._writer.__exit__(exc_type, exc_value, traceback)\n\n    @abstractmethod\n    def content(self) -&gt; ManifestContent: ...\n\n    @property\n    @abstractmethod\n    def version(self) -&gt; TableVersion: ...\n\n    @property\n    def _meta(self) -&gt; Dict[str, str]:\n        return {\n            \"schema\": self._schema.model_dump_json(),\n            \"partition-spec\": to_json(self._spec.fields).decode(\"utf-8\"),\n            \"partition-spec-id\": str(self._spec.spec_id),\n            \"format-version\": str(self.version),\n        }\n\n    def _with_partition(self, format_version: TableVersion) -&gt; Schema:\n        data_file_type = data_file_with_partition(\n            format_version=format_version, partition_type=self._spec.partition_type(self._schema)\n        )\n        return manifest_entry_schema_with_data_file(format_version=format_version, data_file=data_file_type)\n\n    def new_writer(self) -&gt; AvroOutputFile[ManifestEntry]:\n        return AvroOutputFile[ManifestEntry](\n            output_file=self._output_file,\n            file_schema=self._with_partition(self.version),\n            record_schema=self._with_partition(DEFAULT_READ_VERSION),\n            schema_name=\"manifest_entry\",\n            metadata=self._meta,\n        )\n\n    @abstractmethod\n    def prepare_entry(self, entry: ManifestEntry) -&gt; ManifestEntry: ...\n\n    def to_manifest_file(self) -&gt; ManifestFile:\n        \"\"\"Return the manifest file.\"\"\"\n        # once the manifest file is generated, no more entries can be added\n        self.closed = True\n        min_sequence_number = self._min_sequence_number or UNASSIGNED_SEQ\n        return ManifestFile(\n            manifest_path=self._output_file.location,\n            manifest_length=len(self._writer.output_file),\n            partition_spec_id=self._spec.spec_id,\n            content=self.content(),\n            sequence_number=UNASSIGNED_SEQ,\n            min_sequence_number=min_sequence_number,\n            added_snapshot_id=self._snapshot_id,\n            added_files_count=self._added_files,\n            existing_files_count=self._existing_files,\n            deleted_files_count=self._deleted_files,\n            added_rows_count=self._added_rows,\n            existing_rows_count=self._existing_rows,\n            deleted_rows_count=self._deleted_rows,\n            partitions=construct_partition_summaries(self._spec, self._schema, self._partitions),\n            key_metadata=None,\n        )\n\n    def add_entry(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        if self.closed:\n            raise RuntimeError(\"Cannot add entry to closed manifest writer\")\n        if entry.status == ManifestEntryStatus.ADDED:\n            self._added_files += 1\n            self._added_rows += entry.data_file.record_count\n        elif entry.status == ManifestEntryStatus.EXISTING:\n            self._existing_files += 1\n            self._existing_rows += entry.data_file.record_count\n        elif entry.status == ManifestEntryStatus.DELETED:\n            self._deleted_files += 1\n            self._deleted_rows += entry.data_file.record_count\n        else:\n            raise ValueError(f\"Unknown entry: {entry.status}\")\n\n        self._partitions.append(entry.data_file.partition)\n\n        if (\n            (entry.status == ManifestEntryStatus.ADDED or entry.status == ManifestEntryStatus.EXISTING)\n            and entry.sequence_number is not None\n            and (self._min_sequence_number is None or entry.sequence_number &lt; self._min_sequence_number)\n        ):\n            self._min_sequence_number = entry.sequence_number\n\n        self._writer.write_block([self.prepare_entry(entry)])\n        return self\n\n    def add(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        if entry.sequence_number is not None and entry.sequence_number &gt;= 0:\n            self.add_entry(self._reused_entry_wrapper._wrap_append(self._snapshot_id, entry.sequence_number, entry.data_file))\n        else:\n            self.add_entry(self._reused_entry_wrapper._wrap_append(self._snapshot_id, None, entry.data_file))\n        return self\n\n    def delete(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        self.add_entry(\n            self._reused_entry_wrapper._wrap_delete(\n                self._snapshot_id, entry.sequence_number, entry.file_sequence_number, entry.data_file\n            )\n        )\n        return self\n\n    def existing(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        self.add_entry(\n            self._reused_entry_wrapper._wrap_existing(\n                entry.snapshot_id, entry.sequence_number, entry.file_sequence_number, entry.data_file\n            )\n        )\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __enter__(self) -&gt; ManifestWriter:\n    \"\"\"Open the writer.\"\"\"\n    self._writer = self.new_writer()\n    self._writer.__enter__()\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: Optional[Type[BaseException]],\n    exc_value: Optional[BaseException],\n    traceback: Optional[TracebackType],\n) -&gt; None:\n    \"\"\"Close the writer.\"\"\"\n    if (self._added_files + self._existing_files + self._deleted_files) == 0:\n        # This is just a guard to ensure that we don't write empty manifest files\n        raise ValueError(\"An empty manifest file has been written\")\n\n    self.closed = True\n    self._writer.__exit__(exc_type, exc_value, traceback)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.to_manifest_file","title":"<code>to_manifest_file()</code>","text":"<p>Return the manifest file.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def to_manifest_file(self) -&gt; ManifestFile:\n    \"\"\"Return the manifest file.\"\"\"\n    # once the manifest file is generated, no more entries can be added\n    self.closed = True\n    min_sequence_number = self._min_sequence_number or UNASSIGNED_SEQ\n    return ManifestFile(\n        manifest_path=self._output_file.location,\n        manifest_length=len(self._writer.output_file),\n        partition_spec_id=self._spec.spec_id,\n        content=self.content(),\n        sequence_number=UNASSIGNED_SEQ,\n        min_sequence_number=min_sequence_number,\n        added_snapshot_id=self._snapshot_id,\n        added_files_count=self._added_files,\n        existing_files_count=self._existing_files,\n        deleted_files_count=self._deleted_files,\n        added_rows_count=self._added_rows,\n        existing_rows_count=self._existing_rows,\n        deleted_rows_count=self._deleted_rows,\n        partitions=construct_partition_summaries(self._spec, self._schema, self._partitions),\n        key_metadata=None,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest._inherit_from_manifest","title":"<code>_inherit_from_manifest(entry, manifest)</code>","text":"<p>Inherits properties from manifest file.</p> <p>The properties that will be inherited are: - sequence numbers - partition spec id.</p> <p>More information about inheriting sequence numbers: https://iceberg.apache.org/spec/#sequence-number-inheritance</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>ManifestEntry</code> <p>The manifest entry.</p> required <code>manifest</code> <code>ManifestFile</code> <p>The manifest file.</p> required <p>Returns:</p> Type Description <code>ManifestEntry</code> <p>The manifest entry with properties inherited.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def _inherit_from_manifest(entry: ManifestEntry, manifest: ManifestFile) -&gt; ManifestEntry:\n    \"\"\"\n    Inherits properties from manifest file.\n\n    The properties that will be inherited are:\n    - sequence numbers\n    - partition spec id.\n\n    More information about inheriting sequence numbers: https://iceberg.apache.org/spec/#sequence-number-inheritance\n\n    Args:\n        entry: The manifest entry.\n        manifest: The manifest file.\n\n    Returns:\n        The manifest entry with properties inherited.\n    \"\"\"\n    # Inherit sequence numbers.\n    # The snapshot_id is required in V1, inherit with V2 when null\n    if entry.snapshot_id is None:\n        entry.snapshot_id = manifest.added_snapshot_id\n\n    # in v1 tables, the sequence number is not persisted and can be safely defaulted to 0\n    # in v2 tables, the sequence number should be inherited iff the entry status is ADDED\n    if entry.sequence_number is None and (manifest.sequence_number == 0 or entry.status == ManifestEntryStatus.ADDED):\n        entry.sequence_number = manifest.sequence_number\n\n    # in v1 tables, the file sequence number is not persisted and can be safely defaulted to 0\n    # in v2 tables, the file sequence number should be inherited iff the entry status is ADDED\n    if entry.file_sequence_number is None and (manifest.sequence_number == 0 or entry.status == ManifestEntryStatus.ADDED):\n        # Only available in V2, always 0 in V1\n        entry.file_sequence_number = manifest.sequence_number\n\n    # Inherit partition spec id.\n    entry.data_file.spec_id = manifest.partition_spec_id\n\n    return entry\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest._manifests","title":"<code>_manifests(io, manifest_list)</code>","text":"<p>Read and cache manifests from the given manifest list, returning a tuple to prevent modification.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>@cached(cache=LRUCache(maxsize=128), key=lambda io, manifest_list: hashkey(manifest_list))\ndef _manifests(io: FileIO, manifest_list: str) -&gt; Tuple[ManifestFile, ...]:\n    \"\"\"Read and cache manifests from the given manifest list, returning a tuple to prevent modification.\"\"\"\n    file = io.new_input(manifest_list)\n    return tuple(read_manifest_list(file))\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.read_manifest_list","title":"<code>read_manifest_list(input_file)</code>","text":"<p>Read the manifests from the manifest list.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>InputFile</code> <p>The input file where the stream can be read from.</p> required <p>Returns:</p> Type Description <code>Iterator[ManifestFile]</code> <p>An iterator of ManifestFiles that are part of the list.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def read_manifest_list(input_file: InputFile) -&gt; Iterator[ManifestFile]:\n    \"\"\"\n    Read the manifests from the manifest list.\n\n    Args:\n        input_file: The input file where the stream can be read from.\n\n    Returns:\n        An iterator of ManifestFiles that are part of the list.\n    \"\"\"\n    with AvroFile[ManifestFile](\n        input_file,\n        MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n        read_types={-1: ManifestFile, 508: PartitionFieldSummary},\n        read_enums={517: ManifestContent},\n    ) as reader:\n        yield from reader\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/","title":"partitioning","text":""},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionField","title":"<code>PartitionField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>PartitionField represents how one partition value is derived from the source column via transformation.</p> <p>Attributes:</p> Name Type Description <code>source_id(int)</code> <p>The source column id of table's schema.</p> <code>field_id(int)</code> <p>The partition field id across all the table partition specs.</p> <code>transform(Transform)</code> <p>The transform used to produce partition values from source column.</p> <code>name(str)</code> <p>The name of this partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionField(IcebergBaseModel):\n    \"\"\"PartitionField represents how one partition value is derived from the source column via transformation.\n\n    Attributes:\n        source_id(int): The source column id of table's schema.\n        field_id(int): The partition field id across all the table partition specs.\n        transform(Transform): The transform used to produce partition values from source column.\n        name(str): The name of this partition field.\n    \"\"\"\n\n    source_id: int = Field(alias=\"source-id\")\n    field_id: int = Field(alias=\"field-id\")\n    transform: Annotated[  # type: ignore\n        Transform,\n        BeforeValidator(parse_transform),\n        PlainSerializer(lambda c: str(c), return_type=str),  # pylint: disable=W0108\n        WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    ] = Field()\n    name: str = Field()\n\n    def __init__(\n        self,\n        source_id: Optional[int] = None,\n        field_id: Optional[int] = None,\n        transform: Optional[Transform[Any, Any]] = None,\n        name: Optional[str] = None,\n        **data: Any,\n    ):\n        if source_id is not None:\n            data[\"source-id\"] = source_id\n        if field_id is not None:\n            data[\"field-id\"] = field_id\n        if transform is not None:\n            data[\"transform\"] = transform\n        if name is not None:\n            data[\"name\"] = name\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def map_source_ids_onto_source_id(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"source-id\" not in data and (source_ids := data[\"source-ids\"]):\n                if isinstance(source_ids, list):\n                    if len(source_ids) == 0:\n                        raise ValueError(\"Empty source-ids is not allowed\")\n                    if len(source_ids) &gt; 1:\n                        raise ValueError(\"Multi argument transforms are not yet supported\")\n                    data[\"source-id\"] = source_ids[0]\n        return data\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the PartitionField class.\"\"\"\n        return f\"{self.field_id}: {self.name}: {self.transform}({self.source_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the PartitionField class.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the PartitionField class.\"\"\"\n    return f\"{self.field_id}: {self.name}: {self.transform}({self.source_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec","title":"<code>PartitionSpec</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>PartitionSpec captures the transformation from table data to partition values.</p> <p>Attributes:</p> Name Type Description <code>spec_id(int)</code> <p>any change to PartitionSpec will produce a new specId.</p> <code>fields(Tuple[PartitionField)</code> <p>list of partition fields to produce partition values.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionSpec(IcebergBaseModel):\n    \"\"\"\n    PartitionSpec captures the transformation from table data to partition values.\n\n    Attributes:\n        spec_id(int): any change to PartitionSpec will produce a new specId.\n        fields(Tuple[PartitionField): list of partition fields to produce partition values.\n    \"\"\"\n\n    spec_id: int = Field(alias=\"spec-id\", default=INITIAL_PARTITION_SPEC_ID)\n    fields: Tuple[PartitionField, ...] = Field(default_factory=tuple)\n\n    def __init__(\n        self,\n        *fields: PartitionField,\n        **data: Any,\n    ):\n        if fields:\n            data[\"fields\"] = tuple(fields)\n        super().__init__(**data)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"\n        Produce a boolean to return True if two objects are considered equal.\n\n        Note:\n            Equality of PartitionSpec is determined by spec_id and partition fields only.\n        \"\"\"\n        if not isinstance(other, PartitionSpec):\n            return False\n        return self.spec_id == other.spec_id and self.fields == other.fields\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Produce a human-readable string representation of PartitionSpec.\n\n        Note:\n            Only include list of partition fields in the PartitionSpec's string representation.\n        \"\"\"\n        result_str = \"[\"\n        if self.fields:\n            result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n        result_str += \"]\"\n        return result_str\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the PartitionSpec class.\"\"\"\n        fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n        return f\"PartitionSpec({fields}spec_id={self.spec_id})\"\n\n    def is_unpartitioned(self) -&gt; bool:\n        return not self.fields\n\n    @property\n    def last_assigned_field_id(self) -&gt; int:\n        if self.fields:\n            return max(pf.field_id for pf in self.fields)\n        return PARTITION_FIELD_ID_START - 1\n\n    @cached_property\n    def source_id_to_fields_map(self) -&gt; Dict[int, List[PartitionField]]:\n        source_id_to_fields_map: Dict[int, List[PartitionField]] = {}\n        for partition_field in self.fields:\n            existing = source_id_to_fields_map.get(partition_field.source_id, [])\n            existing.append(partition_field)\n            source_id_to_fields_map[partition_field.source_id] = existing\n        return source_id_to_fields_map\n\n    def fields_by_source_id(self, field_id: int) -&gt; List[PartitionField]:\n        return self.source_id_to_fields_map.get(field_id, [])\n\n    def compatible_with(self, other: PartitionSpec) -&gt; bool:\n        \"\"\"Produce a boolean to return True if two PartitionSpec are considered compatible.\"\"\"\n        if self == other:\n            return True\n        if len(self.fields) != len(other.fields):\n            return False\n        return all(\n            this_field.source_id == that_field.source_id\n            and this_field.transform == that_field.transform\n            and this_field.name == that_field.name\n            for this_field, that_field in zip(self.fields, other.fields)\n        )\n\n    def partition_type(self, schema: Schema) -&gt; StructType:\n        \"\"\"Produce a struct of the PartitionSpec.\n\n        The partition fields should be optional:\n\n        - All partition transforms are required to produce null if the input value is null, so it can\n          happen when the source column is optional.\n        - Partition fields may be added later, in which case not all files would have the result field,\n          and it may be null.\n\n        There is a case where we can guarantee that a partition field in the first and only partition spec\n        that uses a required source column will never be null, but it doesn't seem worth tracking this case.\n\n        :param schema: The schema to bind to.\n        :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.\n        \"\"\"\n        nested_fields = []\n        for field in self.fields:\n            source_type = schema.find_type(field.source_id)\n            result_type = field.transform.result_type(source_type)\n            required = schema.find_field(field.source_id).required\n            nested_fields.append(NestedField(field.field_id, field.name, result_type, required=required))\n        return StructType(*nested_fields)\n\n    def partition_to_path(self, data: Record, schema: Schema) -&gt; str:\n        partition_type = self.partition_type(schema)\n        field_types = partition_type.fields\n\n        field_strs = []\n        value_strs = []\n        for pos in range(len(self.fields)):\n            partition_field = self.fields[pos]\n            value_str = partition_field.transform.to_human_string(field_types[pos].field_type, value=data[pos])\n\n            value_strs.append(quote_plus(value_str, safe=\"\"))\n            field_strs.append(quote_plus(partition_field.name, safe=\"\"))\n\n        path = \"/\".join([field_str + \"=\" + value_str for field_str, value_str in zip(field_strs, value_strs)])\n        return path\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Produce a boolean to return True if two objects are considered equal.</p> Note <p>Equality of PartitionSpec is determined by spec_id and partition fields only.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"\n    Produce a boolean to return True if two objects are considered equal.\n\n    Note:\n        Equality of PartitionSpec is determined by spec_id and partition fields only.\n    \"\"\"\n    if not isinstance(other, PartitionSpec):\n        return False\n    return self.spec_id == other.spec_id and self.fields == other.fields\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the PartitionSpec class.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the PartitionSpec class.\"\"\"\n    fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n    return f\"PartitionSpec({fields}spec_id={self.spec_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__str__","title":"<code>__str__()</code>","text":"<p>Produce a human-readable string representation of PartitionSpec.</p> Note <p>Only include list of partition fields in the PartitionSpec's string representation.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Produce a human-readable string representation of PartitionSpec.\n\n    Note:\n        Only include list of partition fields in the PartitionSpec's string representation.\n    \"\"\"\n    result_str = \"[\"\n    if self.fields:\n        result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n    result_str += \"]\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.compatible_with","title":"<code>compatible_with(other)</code>","text":"<p>Produce a boolean to return True if two PartitionSpec are considered compatible.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def compatible_with(self, other: PartitionSpec) -&gt; bool:\n    \"\"\"Produce a boolean to return True if two PartitionSpec are considered compatible.\"\"\"\n    if self == other:\n        return True\n    if len(self.fields) != len(other.fields):\n        return False\n    return all(\n        this_field.source_id == that_field.source_id\n        and this_field.transform == that_field.transform\n        and this_field.name == that_field.name\n        for this_field, that_field in zip(self.fields, other.fields)\n    )\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.partition_type","title":"<code>partition_type(schema)</code>","text":"<p>Produce a struct of the PartitionSpec.</p> <p>The partition fields should be optional:</p> <ul> <li>All partition transforms are required to produce null if the input value is null, so it can   happen when the source column is optional.</li> <li>Partition fields may be added later, in which case not all files would have the result field,   and it may be null.</li> </ul> <p>There is a case where we can guarantee that a partition field in the first and only partition spec that uses a required source column will never be null, but it doesn't seem worth tracking this case.</p> <p>:param schema: The schema to bind to. :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def partition_type(self, schema: Schema) -&gt; StructType:\n    \"\"\"Produce a struct of the PartitionSpec.\n\n    The partition fields should be optional:\n\n    - All partition transforms are required to produce null if the input value is null, so it can\n      happen when the source column is optional.\n    - Partition fields may be added later, in which case not all files would have the result field,\n      and it may be null.\n\n    There is a case where we can guarantee that a partition field in the first and only partition spec\n    that uses a required source column will never be null, but it doesn't seem worth tracking this case.\n\n    :param schema: The schema to bind to.\n    :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.\n    \"\"\"\n    nested_fields = []\n    for field in self.fields:\n        source_type = schema.find_type(field.source_id)\n        result_type = field.transform.result_type(source_type)\n        required = schema.find_field(field.source_id).required\n        nested_fields.append(NestedField(field.field_id, field.name, result_type, required=required))\n    return StructType(*nested_fields)\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor","title":"<code>PartitionSpecVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionSpecVisitor(Generic[T], ABC):\n    @abstractmethod\n    def identity(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit identity partition field.\"\"\"\n\n    @abstractmethod\n    def bucket(self, field_id: int, source_name: str, source_id: int, num_buckets: int) -&gt; T:\n        \"\"\"Visit bucket partition field.\"\"\"\n\n    @abstractmethod\n    def truncate(self, field_id: int, source_name: str, source_id: int, width: int) -&gt; T:\n        \"\"\"Visit truncate partition field.\"\"\"\n\n    @abstractmethod\n    def year(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit year partition field.\"\"\"\n\n    @abstractmethod\n    def month(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit month partition field.\"\"\"\n\n    @abstractmethod\n    def day(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit day partition field.\"\"\"\n\n    @abstractmethod\n    def hour(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit hour partition field.\"\"\"\n\n    @abstractmethod\n    def always_null(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit void partition field.\"\"\"\n\n    @abstractmethod\n    def unknown(self, field_id: int, source_name: str, source_id: int, transform: str) -&gt; T:\n        \"\"\"Visit unknown partition field.\"\"\"\n        raise ValueError(f\"Unknown transform is not supported: {transform}\")\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.always_null","title":"<code>always_null(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit void partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef always_null(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit void partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.bucket","title":"<code>bucket(field_id, source_name, source_id, num_buckets)</code>  <code>abstractmethod</code>","text":"<p>Visit bucket partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef bucket(self, field_id: int, source_name: str, source_id: int, num_buckets: int) -&gt; T:\n    \"\"\"Visit bucket partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.day","title":"<code>day(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit day partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef day(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit day partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.hour","title":"<code>hour(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit hour partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef hour(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit hour partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.identity","title":"<code>identity(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit identity partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef identity(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit identity partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.month","title":"<code>month(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit month partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef month(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit month partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.truncate","title":"<code>truncate(field_id, source_name, source_id, width)</code>  <code>abstractmethod</code>","text":"<p>Visit truncate partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef truncate(self, field_id: int, source_name: str, source_id: int, width: int) -&gt; T:\n    \"\"\"Visit truncate partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.unknown","title":"<code>unknown(field_id, source_name, source_id, transform)</code>  <code>abstractmethod</code>","text":"<p>Visit unknown partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef unknown(self, field_id: int, source_name: str, source_id: int, transform: str) -&gt; T:\n    \"\"\"Visit unknown partition field.\"\"\"\n    raise ValueError(f\"Unknown transform is not supported: {transform}\")\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.year","title":"<code>year(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit year partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef year(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit year partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning._to_partition_representation","title":"<code>_to_partition_representation(type, value)</code>","text":"<p>Strip the logical type into the physical type.</p> <p>It can be that the value is already transformed into its physical type, in this case it will return the original value. Keep in mind that the bucket transform always will return an int, but an identity transform can return date that still needs to be transformed into an int (days since epoch).</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@singledispatch\ndef _to_partition_representation(type: IcebergType, value: Any) -&gt; Any:\n    \"\"\"Strip the logical type into the physical type.\n\n    It can be that the value is already transformed into its physical type,\n    in this case it will return the original value. Keep in mind that the\n    bucket transform always will return an int, but an identity transform\n    can return date that still needs to be transformed into an int (days\n    since epoch).\n    \"\"\"\n    return TypeError(f\"Unsupported partition field type: {type}\")\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.partition_record_value","title":"<code>partition_record_value(partition_field, value, schema)</code>","text":"<p>Return the Partition Record representation of the value.</p> <p>The value is first converted to internal partition representation. For example, UUID is converted to bytes[16], DateType to days since epoch, etc.</p> <p>Then the corresponding PartitionField's transform is applied to return the final partition record value.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def partition_record_value(partition_field: PartitionField, value: Any, schema: Schema) -&gt; Any:\n    \"\"\"\n    Return the Partition Record representation of the value.\n\n    The value is first converted to internal partition representation.\n    For example, UUID is converted to bytes[16], DateType to days since epoch, etc.\n\n    Then the corresponding PartitionField's transform is applied to return\n    the final partition record value.\n    \"\"\"\n    iceberg_type = schema.find_field(name_or_id=partition_field.source_id).field_type\n    return _to_partition_representation(iceberg_type, value)\n</code></pre>"},{"location":"reference/pyiceberg/schema/","title":"schema","text":""},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor","title":"<code>Accessor</code>  <code>dataclass</code>","text":"<p>An accessor for a specific position in a container that implements the StructProtocol.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@dataclass(init=True, eq=True, frozen=True)\nclass Accessor:\n    \"\"\"An accessor for a specific position in a container that implements the StructProtocol.\"\"\"\n\n    position: int\n    inner: Optional[Accessor] = None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Accessor class.\"\"\"\n        return f\"Accessor(position={self.position},inner={self.inner})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Accessor class.\"\"\"\n        return self.__str__()\n\n    def get(self, container: StructProtocol) -&gt; Any:\n        \"\"\"Return the value at self.position in `container`.\n\n        Args:\n            container (StructProtocol): A container to access at position `self.position`.\n\n        Returns:\n            Any: The value at position `self.position` in the container.\n        \"\"\"\n        pos = self.position\n        val = container[pos]\n        inner = self\n        while inner.inner:\n            inner = inner.inner\n            val = val[inner.position]\n\n        return val\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Accessor class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Accessor class.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Accessor class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Accessor class.\"\"\"\n    return f\"Accessor(position={self.position},inner={self.inner})\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.get","title":"<code>get(container)</code>","text":"<p>Return the value at self.position in <code>container</code>.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>StructProtocol</code> <p>A container to access at position <code>self.position</code>.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value at position <code>self.position</code> in the container.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def get(self, container: StructProtocol) -&gt; Any:\n    \"\"\"Return the value at self.position in `container`.\n\n    Args:\n        container (StructProtocol): A container to access at position `self.position`.\n\n    Returns:\n        Any: The value at position `self.position` in the container.\n    \"\"\"\n    pos = self.position\n    val = container[pos]\n    inner = self\n    while inner.inner:\n        inner = inner.inner\n        val = val[inner.position]\n\n    return val\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor","title":"<code>PartnerAccessor</code>","text":"<p>               Bases: <code>Generic[P]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PartnerAccessor(Generic[P], ABC):\n    @abstractmethod\n    def schema_partner(self, partner: Optional[P]) -&gt; Optional[P]:\n        \"\"\"Return the equivalent of the schema as a struct.\"\"\"\n\n    @abstractmethod\n    def field_partner(self, partner_struct: Optional[P], field_id: int, field_name: str) -&gt; Optional[P]:\n        \"\"\"Return the equivalent struct field by name or id in the partner struct.\"\"\"\n\n    @abstractmethod\n    def list_element_partner(self, partner_list: Optional[P]) -&gt; Optional[P]:\n        \"\"\"Return the equivalent list element in the partner list.\"\"\"\n\n    @abstractmethod\n    def map_key_partner(self, partner_map: Optional[P]) -&gt; Optional[P]:\n        \"\"\"Return the equivalent map key in the partner map.\"\"\"\n\n    @abstractmethod\n    def map_value_partner(self, partner_map: Optional[P]) -&gt; Optional[P]:\n        \"\"\"Return the equivalent map value in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.field_partner","title":"<code>field_partner(partner_struct, field_id, field_name)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent struct field by name or id in the partner struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field_partner(self, partner_struct: Optional[P], field_id: int, field_name: str) -&gt; Optional[P]:\n    \"\"\"Return the equivalent struct field by name or id in the partner struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.list_element_partner","title":"<code>list_element_partner(partner_list)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent list element in the partner list.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list_element_partner(self, partner_list: Optional[P]) -&gt; Optional[P]:\n    \"\"\"Return the equivalent list element in the partner list.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.map_key_partner","title":"<code>map_key_partner(partner_map)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent map key in the partner map.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map_key_partner(self, partner_map: Optional[P]) -&gt; Optional[P]:\n    \"\"\"Return the equivalent map key in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.map_value_partner","title":"<code>map_value_partner(partner_map)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent map value in the partner map.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map_value_partner(self, partner_map: Optional[P]) -&gt; Optional[P]:\n    \"\"\"Return the equivalent map value in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.schema_partner","title":"<code>schema_partner(partner)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent of the schema as a struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema_partner(self, partner: Optional[P]) -&gt; Optional[P]:\n    \"\"\"Return the equivalent of the schema as a struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor","title":"<code>PreOrderSchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PreOrderSchemaVisitor(Generic[T], ABC):\n    @abstractmethod\n    def schema(self, schema: Schema, struct_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a Schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, field_results: List[Callable[[], T]]) -&gt; T:\n        \"\"\"Visit a StructType.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a NestedField.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, element_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a ListType.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, key_result: Callable[[], T], value_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a MapType.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a NestedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, element_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, key_result: Callable[[], T], value_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a Schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, struct_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a Schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a StructType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, field_results: List[Callable[[], T]]) -&gt; T:\n    \"\"\"Visit a StructType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor","title":"<code>PrimitiveWithPartnerVisitor</code>","text":"<p>               Bases: <code>SchemaWithPartnerVisitor[P, T]</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PrimitiveWithPartnerVisitor(SchemaWithPartnerVisitor[P, T]):\n    def primitive(self, primitive: PrimitiveType, primitive_partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n        if isinstance(primitive, BooleanType):\n            return self.visit_boolean(primitive, primitive_partner)\n        elif isinstance(primitive, IntegerType):\n            return self.visit_integer(primitive, primitive_partner)\n        elif isinstance(primitive, LongType):\n            return self.visit_long(primitive, primitive_partner)\n        elif isinstance(primitive, FloatType):\n            return self.visit_float(primitive, primitive_partner)\n        elif isinstance(primitive, DoubleType):\n            return self.visit_double(primitive, primitive_partner)\n        elif isinstance(primitive, DecimalType):\n            return self.visit_decimal(primitive, primitive_partner)\n        elif isinstance(primitive, DateType):\n            return self.visit_date(primitive, primitive_partner)\n        elif isinstance(primitive, TimeType):\n            return self.visit_time(primitive, primitive_partner)\n        elif isinstance(primitive, TimestampType):\n            return self.visit_timestamp(primitive, primitive_partner)\n        elif isinstance(primitive, TimestamptzType):\n            return self.visit_timestamptz(primitive, primitive_partner)\n        elif isinstance(primitive, StringType):\n            return self.visit_string(primitive, primitive_partner)\n        elif isinstance(primitive, UUIDType):\n            return self.visit_uuid(primitive, primitive_partner)\n        elif isinstance(primitive, FixedType):\n            return self.visit_fixed(primitive, primitive_partner)\n        elif isinstance(primitive, BinaryType):\n            return self.visit_binary(primitive, primitive_partner)\n        else:\n            raise ValueError(f\"Unknown type: {primitive}\")\n\n    @abstractmethod\n    def visit_boolean(self, boolean_type: BooleanType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a BooleanType.\"\"\"\n\n    @abstractmethod\n    def visit_integer(self, integer_type: IntegerType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a IntegerType.\"\"\"\n\n    @abstractmethod\n    def visit_long(self, long_type: LongType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a LongType.\"\"\"\n\n    @abstractmethod\n    def visit_float(self, float_type: FloatType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a FloatType.\"\"\"\n\n    @abstractmethod\n    def visit_double(self, double_type: DoubleType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a DoubleType.\"\"\"\n\n    @abstractmethod\n    def visit_decimal(self, decimal_type: DecimalType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_date(self, date_type: DateType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_time(self, time_type: TimeType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp(self, timestamp_type: TimestampType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a TimestampType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a TimestamptzType.\"\"\"\n\n    @abstractmethod\n    def visit_string(self, string_type: StringType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a StringType.\"\"\"\n\n    @abstractmethod\n    def visit_uuid(self, uuid_type: UUIDType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a UUIDType.\"\"\"\n\n    @abstractmethod\n    def visit_fixed(self, fixed_type: FixedType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a FixedType.\"\"\"\n\n    @abstractmethod\n    def visit_binary(self, binary_type: BinaryType, partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.primitive","title":"<code>primitive(primitive, primitive_partner)</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def primitive(self, primitive: PrimitiveType, primitive_partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n    if isinstance(primitive, BooleanType):\n        return self.visit_boolean(primitive, primitive_partner)\n    elif isinstance(primitive, IntegerType):\n        return self.visit_integer(primitive, primitive_partner)\n    elif isinstance(primitive, LongType):\n        return self.visit_long(primitive, primitive_partner)\n    elif isinstance(primitive, FloatType):\n        return self.visit_float(primitive, primitive_partner)\n    elif isinstance(primitive, DoubleType):\n        return self.visit_double(primitive, primitive_partner)\n    elif isinstance(primitive, DecimalType):\n        return self.visit_decimal(primitive, primitive_partner)\n    elif isinstance(primitive, DateType):\n        return self.visit_date(primitive, primitive_partner)\n    elif isinstance(primitive, TimeType):\n        return self.visit_time(primitive, primitive_partner)\n    elif isinstance(primitive, TimestampType):\n        return self.visit_timestamp(primitive, primitive_partner)\n    elif isinstance(primitive, TimestamptzType):\n        return self.visit_timestamptz(primitive, primitive_partner)\n    elif isinstance(primitive, StringType):\n        return self.visit_string(primitive, primitive_partner)\n    elif isinstance(primitive, UUIDType):\n        return self.visit_uuid(primitive, primitive_partner)\n    elif isinstance(primitive, FixedType):\n        return self.visit_fixed(primitive, primitive_partner)\n    elif isinstance(primitive, BinaryType):\n        return self.visit_binary(primitive, primitive_partner)\n    else:\n        raise ValueError(f\"Unknown type: {primitive}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_binary","title":"<code>visit_binary(binary_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a BinaryType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_binary(self, binary_type: BinaryType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_boolean","title":"<code>visit_boolean(boolean_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a BooleanType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_boolean(self, boolean_type: BooleanType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a BooleanType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_date","title":"<code>visit_date(date_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_date(self, date_type: DateType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_decimal","title":"<code>visit_decimal(decimal_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_decimal(self, decimal_type: DecimalType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_double","title":"<code>visit_double(double_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DoubleType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_double(self, double_type: DoubleType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a DoubleType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_fixed","title":"<code>visit_fixed(fixed_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a FixedType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_fixed(self, fixed_type: FixedType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a FixedType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_float","title":"<code>visit_float(float_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a FloatType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_float(self, float_type: FloatType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a FloatType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_integer","title":"<code>visit_integer(integer_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a IntegerType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_integer(self, integer_type: IntegerType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a IntegerType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_long","title":"<code>visit_long(long_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a LongType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_long(self, long_type: LongType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a LongType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_string","title":"<code>visit_string(string_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a StringType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_string(self, string_type: StringType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a StringType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_time","title":"<code>visit_time(time_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_time(self, time_type: TimeType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamp","title":"<code>visit_timestamp(timestamp_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp(self, timestamp_type: TimestampType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a TimestampType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamptz","title":"<code>visit_timestamptz(timestamptz_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz(self, timestamptz_type: TimestamptzType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a TimestamptzType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_uuid","title":"<code>visit_uuid(uuid_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a UUIDType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_uuid(self, uuid_type: UUIDType, partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a UUIDType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema","title":"<code>Schema</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A table Schema.</p> Example <p>from pyiceberg import schema from pyiceberg import types</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class Schema(IcebergBaseModel):\n    \"\"\"A table Schema.\n\n    Example:\n        &gt;&gt;&gt; from pyiceberg import schema\n        &gt;&gt;&gt; from pyiceberg import types\n    \"\"\"\n\n    type: Literal[\"struct\"] = \"struct\"\n    fields: Tuple[NestedField, ...] = Field(default_factory=tuple)\n    schema_id: int = Field(alias=\"schema-id\", default=INITIAL_SCHEMA_ID)\n    identifier_field_ids: List[int] = Field(alias=\"identifier-field-ids\", default_factory=list)\n\n    _name_to_id: Dict[str, int] = PrivateAttr()\n\n    def __init__(self, *fields: NestedField, **data: Any):\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n        self._name_to_id = index_by_name(self)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Schema class.\"\"\"\n        return \"table {\\n\" + \"\\n\".join([\"  \" + str(field) for field in self.columns]) + \"\\n}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Schema class.\"\"\"\n        return f\"Schema({', '.join(repr(column) for column in self.columns)}, schema_id={self.schema_id}, identifier_field_ids={self.identifier_field_ids})\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the Literal class.\"\"\"\n        return len(self.fields)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Schema class.\"\"\"\n        if not other:\n            return False\n\n        if not isinstance(other, Schema):\n            return False\n\n        if len(self.columns) != len(other.columns):\n            return False\n\n        identifier_field_ids_is_equal = self.identifier_field_ids == other.identifier_field_ids\n        schema_is_equal = all(lhs == rhs for lhs, rhs in zip(self.columns, other.columns))\n\n        return identifier_field_ids_is_equal and schema_is_equal\n\n    @model_validator(mode=\"after\")\n    def check_schema(self) -&gt; Schema:\n        if self.identifier_field_ids:\n            for field_id in self.identifier_field_ids:\n                self._validate_identifier_field(field_id)\n\n        return self\n\n    @property\n    def columns(self) -&gt; Tuple[NestedField, ...]:\n        \"\"\"A tuple of the top-level fields.\"\"\"\n        return self.fields\n\n    @cached_property\n    def _lazy_id_to_field(self) -&gt; Dict[int, NestedField]:\n        \"\"\"Return an index of field ID to NestedField instance.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return index_by_id(self)\n\n    @cached_property\n    def _lazy_id_to_parent(self) -&gt; Dict[int, int]:\n        \"\"\"Returns an index of field ID to parent field IDs.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return _index_parents(self)\n\n    @cached_property\n    def _lazy_name_to_id_lower(self) -&gt; Dict[str, int]:\n        \"\"\"Return an index of lower-case field names to field IDs.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return {name.lower(): field_id for name, field_id in self._name_to_id.items()}\n\n    @cached_property\n    def _lazy_id_to_name(self) -&gt; Dict[int, str]:\n        \"\"\"Return an index of field ID to full name.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return index_name_by_id(self)\n\n    @cached_property\n    def _lazy_id_to_accessor(self) -&gt; Dict[int, Accessor]:\n        \"\"\"Return an index of field ID to accessor.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return build_position_accessors(self)\n\n    def as_struct(self) -&gt; StructType:\n        \"\"\"Return the schema as a struct.\"\"\"\n        return StructType(*self.fields)\n\n    def as_arrow(self) -&gt; \"pa.Schema\":\n        \"\"\"Return the schema as an Arrow schema.\"\"\"\n        from pyiceberg.io.pyarrow import schema_to_pyarrow\n\n        return schema_to_pyarrow(self)\n\n    def find_field(self, name_or_id: Union[str, int], case_sensitive: bool = True) -&gt; NestedField:\n        \"\"\"Find a field using a field name or field ID.\n\n        Args:\n            name_or_id (Union[str, int]): Either a field name or a field ID.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n        Raises:\n            ValueError: When the value cannot be found.\n\n        Returns:\n            NestedField: The matched NestedField.\n        \"\"\"\n        if isinstance(name_or_id, int):\n            if name_or_id not in self._lazy_id_to_field:\n                raise ValueError(f\"Could not find field with id: {name_or_id}\")\n            return self._lazy_id_to_field[name_or_id]\n\n        if case_sensitive:\n            field_id = self._name_to_id.get(name_or_id)\n        else:\n            field_id = self._lazy_name_to_id_lower.get(name_or_id.lower())\n\n        if field_id is None:\n            raise ValueError(f\"Could not find field with name {name_or_id}, case_sensitive={case_sensitive}\")\n\n        return self._lazy_id_to_field[field_id]\n\n    def find_type(self, name_or_id: Union[str, int], case_sensitive: bool = True) -&gt; IcebergType:\n        \"\"\"Find a field type using a field name or field ID.\n\n        Args:\n            name_or_id (Union[str, int]): Either a field name or a field ID.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n        Returns:\n            NestedField: The type of the matched NestedField.\n        \"\"\"\n        field = self.find_field(name_or_id=name_or_id, case_sensitive=case_sensitive)\n        if not field:\n            raise ValueError(f\"Could not find field with name or id {name_or_id}, case_sensitive={case_sensitive}\")\n        return field.field_type\n\n    @property\n    def highest_field_id(self) -&gt; int:\n        return max(self._lazy_id_to_name.keys(), default=0)\n\n    @cached_property\n    def name_mapping(self) -&gt; NameMapping:\n        from pyiceberg.table.name_mapping import create_mapping_from_schema\n\n        return create_mapping_from_schema(self)\n\n    def find_column_name(self, column_id: int) -&gt; Optional[str]:\n        \"\"\"Find a column name given a column ID.\n\n        Args:\n            column_id (int): The ID of the column.\n\n        Returns:\n            str: The column name (or None if the column ID cannot be found).\n        \"\"\"\n        return self._lazy_id_to_name.get(column_id)\n\n    @property\n    def column_names(self) -&gt; List[str]:\n        \"\"\"\n        Return a list of all the column names, including nested fields.\n\n        Excludes short names.\n\n        Returns:\n            List[str]: The column names.\n        \"\"\"\n        return list(self._lazy_id_to_name.values())\n\n    def accessor_for_field(self, field_id: int) -&gt; Accessor:\n        \"\"\"Find a schema position accessor given a field ID.\n\n        Args:\n            field_id (int): The ID of the field.\n\n        Raises:\n            ValueError: When the value cannot be found.\n\n        Returns:\n            Accessor: An accessor for the given field ID.\n        \"\"\"\n        if field_id not in self._lazy_id_to_accessor:\n            raise ValueError(f\"Could not find accessor for field with id: {field_id}\")\n\n        return self._lazy_id_to_accessor[field_id]\n\n    def identifier_field_names(self) -&gt; Set[str]:\n        \"\"\"Return the names of the identifier fields.\n\n        Returns:\n            Set of names of the identifier fields\n        \"\"\"\n        ids = set()\n        for field_id in self.identifier_field_ids:\n            column_name = self.find_column_name(field_id)\n            if column_name is None:\n                raise ValueError(f\"Could not find identifier column id: {field_id}\")\n            ids.add(column_name)\n\n        return ids\n\n    def select(self, *names: str, case_sensitive: bool = True) -&gt; Schema:\n        \"\"\"Return a new schema instance pruned to a subset of columns.\n\n        Args:\n            names (List[str]): A list of column names.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup for each column name. Defaults to True.\n\n        Returns:\n            Schema: A new schema with pruned columns.\n\n        Raises:\n            ValueError: If a column is selected that doesn't exist.\n        \"\"\"\n        try:\n            if case_sensitive:\n                ids = {self._name_to_id[name] for name in names}\n            else:\n                ids = {self._lazy_name_to_id_lower[name.lower()] for name in names}\n        except KeyError as e:\n            raise ValueError(f\"Could not find column: {e}\") from e\n\n        return prune_columns(self, ids)\n\n    @property\n    def field_ids(self) -&gt; Set[int]:\n        \"\"\"Return the IDs of the current schema.\"\"\"\n        return set(self._name_to_id.values())\n\n    def _validate_identifier_field(self, field_id: int) -&gt; None:\n        \"\"\"Validate that the field with the given ID is a valid identifier field.\n\n        Args:\n          field_id: The ID of the field to validate.\n\n        Raises:\n          ValueError: If the field is not valid.\n        \"\"\"\n        field = self.find_field(field_id)\n        if not field.field_type.is_primitive:\n            raise ValueError(f\"Identifier field {field_id} invalid: not a primitive type field\")\n\n        if not field.required:\n            raise ValueError(f\"Identifier field {field_id} invalid: not a required field\")\n\n        if isinstance(field.field_type, (DoubleType, FloatType)):\n            raise ValueError(f\"Identifier field {field_id} invalid: must not be float or double field\")\n\n        # Check whether the nested field is in a chain of required struct fields\n        # Exploring from root for better error message for list and map types\n        parent_id = self._lazy_id_to_parent.get(field.field_id)\n        fields: List[int] = []\n        while parent_id is not None:\n            fields.append(parent_id)\n            parent_id = self._lazy_id_to_parent.get(parent_id)\n\n        while fields:\n            parent = self.find_field(fields.pop())\n            if not parent.field_type.is_struct:\n                raise ValueError(f\"Cannot add field {field.name} as an identifier field: must not be nested in {parent}\")\n\n            if not parent.required:\n                raise ValueError(\n                    f\"Cannot add field {field.name} as an identifier field: must not be nested in an optional field {parent}\"\n                )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._lazy_id_to_accessor","title":"<code>_lazy_id_to_accessor</code>  <code>cached</code> <code>property</code>","text":"<p>Return an index of field ID to accessor.</p> <p>This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._lazy_id_to_field","title":"<code>_lazy_id_to_field</code>  <code>cached</code> <code>property</code>","text":"<p>Return an index of field ID to NestedField instance.</p> <p>This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._lazy_id_to_name","title":"<code>_lazy_id_to_name</code>  <code>cached</code> <code>property</code>","text":"<p>Return an index of field ID to full name.</p> <p>This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._lazy_id_to_parent","title":"<code>_lazy_id_to_parent</code>  <code>cached</code> <code>property</code>","text":"<p>Returns an index of field ID to parent field IDs.</p> <p>This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._lazy_name_to_id_lower","title":"<code>_lazy_name_to_id_lower</code>  <code>cached</code> <code>property</code>","text":"<p>Return an index of lower-case field names to field IDs.</p> <p>This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.column_names","title":"<code>column_names</code>  <code>property</code>","text":"<p>Return a list of all the column names, including nested fields.</p> <p>Excludes short names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The column names.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>A tuple of the top-level fields.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.field_ids","title":"<code>field_ids</code>  <code>property</code>","text":"<p>Return the IDs of the current schema.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Schema class.\"\"\"\n    if not other:\n        return False\n\n    if not isinstance(other, Schema):\n        return False\n\n    if len(self.columns) != len(other.columns):\n        return False\n\n    identifier_field_ids_is_equal = self.identifier_field_ids == other.identifier_field_ids\n    schema_is_equal = all(lhs == rhs for lhs, rhs in zip(self.columns, other.columns))\n\n    return identifier_field_ids_is_equal and schema_is_equal\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the Literal class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the Literal class.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Schema class.\"\"\"\n    return f\"Schema({', '.join(repr(column) for column in self.columns)}, schema_id={self.schema_id}, identifier_field_ids={self.identifier_field_ids})\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Schema class.\"\"\"\n    return \"table {\\n\" + \"\\n\".join([\"  \" + str(field) for field in self.columns]) + \"\\n}\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema._validate_identifier_field","title":"<code>_validate_identifier_field(field_id)</code>","text":"<p>Validate that the field with the given ID is a valid identifier field.</p> <p>Parameters:</p> Name Type Description Default <code>field_id</code> <code>int</code> <p>The ID of the field to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the field is not valid.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def _validate_identifier_field(self, field_id: int) -&gt; None:\n    \"\"\"Validate that the field with the given ID is a valid identifier field.\n\n    Args:\n      field_id: The ID of the field to validate.\n\n    Raises:\n      ValueError: If the field is not valid.\n    \"\"\"\n    field = self.find_field(field_id)\n    if not field.field_type.is_primitive:\n        raise ValueError(f\"Identifier field {field_id} invalid: not a primitive type field\")\n\n    if not field.required:\n        raise ValueError(f\"Identifier field {field_id} invalid: not a required field\")\n\n    if isinstance(field.field_type, (DoubleType, FloatType)):\n        raise ValueError(f\"Identifier field {field_id} invalid: must not be float or double field\")\n\n    # Check whether the nested field is in a chain of required struct fields\n    # Exploring from root for better error message for list and map types\n    parent_id = self._lazy_id_to_parent.get(field.field_id)\n    fields: List[int] = []\n    while parent_id is not None:\n        fields.append(parent_id)\n        parent_id = self._lazy_id_to_parent.get(parent_id)\n\n    while fields:\n        parent = self.find_field(fields.pop())\n        if not parent.field_type.is_struct:\n            raise ValueError(f\"Cannot add field {field.name} as an identifier field: must not be nested in {parent}\")\n\n        if not parent.required:\n            raise ValueError(\n                f\"Cannot add field {field.name} as an identifier field: must not be nested in an optional field {parent}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.accessor_for_field","title":"<code>accessor_for_field(field_id)</code>","text":"<p>Find a schema position accessor given a field ID.</p> <p>Parameters:</p> Name Type Description Default <code>field_id</code> <code>int</code> <p>The ID of the field.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When the value cannot be found.</p> <p>Returns:</p> Name Type Description <code>Accessor</code> <code>Accessor</code> <p>An accessor for the given field ID.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def accessor_for_field(self, field_id: int) -&gt; Accessor:\n    \"\"\"Find a schema position accessor given a field ID.\n\n    Args:\n        field_id (int): The ID of the field.\n\n    Raises:\n        ValueError: When the value cannot be found.\n\n    Returns:\n        Accessor: An accessor for the given field ID.\n    \"\"\"\n    if field_id not in self._lazy_id_to_accessor:\n        raise ValueError(f\"Could not find accessor for field with id: {field_id}\")\n\n    return self._lazy_id_to_accessor[field_id]\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.as_arrow","title":"<code>as_arrow()</code>","text":"<p>Return the schema as an Arrow schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def as_arrow(self) -&gt; \"pa.Schema\":\n    \"\"\"Return the schema as an Arrow schema.\"\"\"\n    from pyiceberg.io.pyarrow import schema_to_pyarrow\n\n    return schema_to_pyarrow(self)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.as_struct","title":"<code>as_struct()</code>","text":"<p>Return the schema as a struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def as_struct(self) -&gt; StructType:\n    \"\"\"Return the schema as a struct.\"\"\"\n    return StructType(*self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_column_name","title":"<code>find_column_name(column_id)</code>","text":"<p>Find a column name given a column ID.</p> <p>Parameters:</p> Name Type Description Default <code>column_id</code> <code>int</code> <p>The ID of the column.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The column name (or None if the column ID cannot be found).</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_column_name(self, column_id: int) -&gt; Optional[str]:\n    \"\"\"Find a column name given a column ID.\n\n    Args:\n        column_id (int): The ID of the column.\n\n    Returns:\n        str: The column name (or None if the column ID cannot be found).\n    \"\"\"\n    return self._lazy_id_to_name.get(column_id)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_field","title":"<code>find_field(name_or_id, case_sensitive=True)</code>","text":"<p>Find a field using a field name or field ID.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>Union[str, int]</code> <p>Either a field name or a field ID.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup using a field name. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the value cannot be found.</p> <p>Returns:</p> Name Type Description <code>NestedField</code> <code>NestedField</code> <p>The matched NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_field(self, name_or_id: Union[str, int], case_sensitive: bool = True) -&gt; NestedField:\n    \"\"\"Find a field using a field name or field ID.\n\n    Args:\n        name_or_id (Union[str, int]): Either a field name or a field ID.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n    Raises:\n        ValueError: When the value cannot be found.\n\n    Returns:\n        NestedField: The matched NestedField.\n    \"\"\"\n    if isinstance(name_or_id, int):\n        if name_or_id not in self._lazy_id_to_field:\n            raise ValueError(f\"Could not find field with id: {name_or_id}\")\n        return self._lazy_id_to_field[name_or_id]\n\n    if case_sensitive:\n        field_id = self._name_to_id.get(name_or_id)\n    else:\n        field_id = self._lazy_name_to_id_lower.get(name_or_id.lower())\n\n    if field_id is None:\n        raise ValueError(f\"Could not find field with name {name_or_id}, case_sensitive={case_sensitive}\")\n\n    return self._lazy_id_to_field[field_id]\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_type","title":"<code>find_type(name_or_id, case_sensitive=True)</code>","text":"<p>Find a field type using a field name or field ID.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>Union[str, int]</code> <p>Either a field name or a field ID.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup using a field name. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>NestedField</code> <code>IcebergType</code> <p>The type of the matched NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_type(self, name_or_id: Union[str, int], case_sensitive: bool = True) -&gt; IcebergType:\n    \"\"\"Find a field type using a field name or field ID.\n\n    Args:\n        name_or_id (Union[str, int]): Either a field name or a field ID.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n    Returns:\n        NestedField: The type of the matched NestedField.\n    \"\"\"\n    field = self.find_field(name_or_id=name_or_id, case_sensitive=case_sensitive)\n    if not field:\n        raise ValueError(f\"Could not find field with name or id {name_or_id}, case_sensitive={case_sensitive}\")\n    return field.field_type\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.identifier_field_names","title":"<code>identifier_field_names()</code>","text":"<p>Return the names of the identifier fields.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set of names of the identifier fields</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def identifier_field_names(self) -&gt; Set[str]:\n    \"\"\"Return the names of the identifier fields.\n\n    Returns:\n        Set of names of the identifier fields\n    \"\"\"\n    ids = set()\n    for field_id in self.identifier_field_ids:\n        column_name = self.find_column_name(field_id)\n        if column_name is None:\n            raise ValueError(f\"Could not find identifier column id: {field_id}\")\n        ids.add(column_name)\n\n    return ids\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.select","title":"<code>select(*names, case_sensitive=True)</code>","text":"<p>Return a new schema instance pruned to a subset of columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>A list of column names.</p> <code>()</code> <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup for each column name. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Schema</code> <code>Schema</code> <p>A new schema with pruned columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a column is selected that doesn't exist.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def select(self, *names: str, case_sensitive: bool = True) -&gt; Schema:\n    \"\"\"Return a new schema instance pruned to a subset of columns.\n\n    Args:\n        names (List[str]): A list of column names.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup for each column name. Defaults to True.\n\n    Returns:\n        Schema: A new schema with pruned columns.\n\n    Raises:\n        ValueError: If a column is selected that doesn't exist.\n    \"\"\"\n    try:\n        if case_sensitive:\n            ids = {self._name_to_id[name] for name in names}\n        else:\n            ids = {self._lazy_name_to_id_lower[name.lower()] for name in names}\n    except KeyError as e:\n        raise ValueError(f\"Could not find column: {e}\") from e\n\n    return prune_columns(self, ids)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor","title":"<code>SchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaVisitor(Generic[T], ABC):\n    def before_field(self, field: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n        self.before_field(element)\n\n    def after_list_element(self, element: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n        self.after_field(element)\n\n    def before_map_key(self, key: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n        self.before_field(key)\n\n    def after_map_key(self, key: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n        self.after_field(key)\n\n    def before_map_value(self, value: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n        self.before_field(value)\n\n    def after_map_value(self, value: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n        self.after_field(value)\n\n    @abstractmethod\n    def schema(self, schema: Schema, struct_result: T) -&gt; T:\n        \"\"\"Visit a Schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, field_results: List[T]) -&gt; T:\n        \"\"\"Visit a StructType.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_result: T) -&gt; T:\n        \"\"\"Visit a NestedField.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, element_result: T) -&gt; T:\n        \"\"\"Visit a ListType.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a MapType.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_field","title":"<code>after_field(field)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_field(self, field: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_list_element","title":"<code>after_list_element(element)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_list_element(self, element: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n    self.after_field(element)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_map_key","title":"<code>after_map_key(key)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_key(self, key: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n    self.after_field(key)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_map_value","title":"<code>after_map_value(value)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_value(self, value: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n    self.after_field(value)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_field","title":"<code>before_field(field)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_field(self, field: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_list_element","title":"<code>before_list_element(element)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_list_element(self, element: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n    self.before_field(element)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_map_key","title":"<code>before_map_key(key)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_key(self, key: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n    self.before_field(key)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_map_value","title":"<code>before_map_value(value)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_value(self, value: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n    self.before_field(value)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_result: T) -&gt; T:\n    \"\"\"Visit a NestedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, element_result: T) -&gt; T:\n    \"\"\"Visit a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a Schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, struct_result: T) -&gt; T:\n    \"\"\"Visit a Schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a StructType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, field_results: List[T]) -&gt; T:\n    \"\"\"Visit a StructType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType","title":"<code>SchemaVisitorPerPrimitiveType</code>","text":"<p>               Bases: <code>SchemaVisitor[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaVisitorPerPrimitiveType(SchemaVisitor[T], ABC):\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n        if isinstance(primitive, FixedType):\n            return self.visit_fixed(primitive)\n        elif isinstance(primitive, DecimalType):\n            return self.visit_decimal(primitive)\n        elif isinstance(primitive, BooleanType):\n            return self.visit_boolean(primitive)\n        elif isinstance(primitive, IntegerType):\n            return self.visit_integer(primitive)\n        elif isinstance(primitive, LongType):\n            return self.visit_long(primitive)\n        elif isinstance(primitive, FloatType):\n            return self.visit_float(primitive)\n        elif isinstance(primitive, DoubleType):\n            return self.visit_double(primitive)\n        elif isinstance(primitive, DateType):\n            return self.visit_date(primitive)\n        elif isinstance(primitive, TimeType):\n            return self.visit_time(primitive)\n        elif isinstance(primitive, TimestampType):\n            return self.visit_timestamp(primitive)\n        elif isinstance(primitive, TimestamptzType):\n            return self.visit_timestamptz(primitive)\n        elif isinstance(primitive, StringType):\n            return self.visit_string(primitive)\n        elif isinstance(primitive, UUIDType):\n            return self.visit_uuid(primitive)\n        elif isinstance(primitive, BinaryType):\n            return self.visit_binary(primitive)\n        else:\n            raise ValueError(f\"Unknown type: {primitive}\")\n\n    @abstractmethod\n    def visit_fixed(self, fixed_type: FixedType) -&gt; T:\n        \"\"\"Visit a FixedType.\"\"\"\n\n    @abstractmethod\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; T:\n        \"\"\"Visit a BooleanType.\"\"\"\n\n    @abstractmethod\n    def visit_integer(self, integer_type: IntegerType) -&gt; T:\n        \"\"\"Visit a IntegerType.\"\"\"\n\n    @abstractmethod\n    def visit_long(self, long_type: LongType) -&gt; T:\n        \"\"\"Visit a LongType.\"\"\"\n\n    @abstractmethod\n    def visit_float(self, float_type: FloatType) -&gt; T:\n        \"\"\"Visit a FloatType.\"\"\"\n\n    @abstractmethod\n    def visit_double(self, double_type: DoubleType) -&gt; T:\n        \"\"\"Visit a DoubleType.\"\"\"\n\n    @abstractmethod\n    def visit_date(self, date_type: DateType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_time(self, time_type: TimeType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; T:\n        \"\"\"Visit a TimestampType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; T:\n        \"\"\"Visit a TimestamptzType.\"\"\"\n\n    @abstractmethod\n    def visit_string(self, string_type: StringType) -&gt; T:\n        \"\"\"Visit a StringType.\"\"\"\n\n    @abstractmethod\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; T:\n        \"\"\"Visit a UUIDType.\"\"\"\n\n    @abstractmethod\n    def visit_binary(self, binary_type: BinaryType) -&gt; T:\n        \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.primitive","title":"<code>primitive(primitive)</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n    if isinstance(primitive, FixedType):\n        return self.visit_fixed(primitive)\n    elif isinstance(primitive, DecimalType):\n        return self.visit_decimal(primitive)\n    elif isinstance(primitive, BooleanType):\n        return self.visit_boolean(primitive)\n    elif isinstance(primitive, IntegerType):\n        return self.visit_integer(primitive)\n    elif isinstance(primitive, LongType):\n        return self.visit_long(primitive)\n    elif isinstance(primitive, FloatType):\n        return self.visit_float(primitive)\n    elif isinstance(primitive, DoubleType):\n        return self.visit_double(primitive)\n    elif isinstance(primitive, DateType):\n        return self.visit_date(primitive)\n    elif isinstance(primitive, TimeType):\n        return self.visit_time(primitive)\n    elif isinstance(primitive, TimestampType):\n        return self.visit_timestamp(primitive)\n    elif isinstance(primitive, TimestamptzType):\n        return self.visit_timestamptz(primitive)\n    elif isinstance(primitive, StringType):\n        return self.visit_string(primitive)\n    elif isinstance(primitive, UUIDType):\n        return self.visit_uuid(primitive)\n    elif isinstance(primitive, BinaryType):\n        return self.visit_binary(primitive)\n    else:\n        raise ValueError(f\"Unknown type: {primitive}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_binary","title":"<code>visit_binary(binary_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a BinaryType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_binary(self, binary_type: BinaryType) -&gt; T:\n    \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_boolean","title":"<code>visit_boolean(boolean_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a BooleanType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_boolean(self, boolean_type: BooleanType) -&gt; T:\n    \"\"\"Visit a BooleanType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_date","title":"<code>visit_date(date_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_date(self, date_type: DateType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_decimal","title":"<code>visit_decimal(decimal_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_decimal(self, decimal_type: DecimalType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_double","title":"<code>visit_double(double_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DoubleType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_double(self, double_type: DoubleType) -&gt; T:\n    \"\"\"Visit a DoubleType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_fixed","title":"<code>visit_fixed(fixed_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a FixedType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_fixed(self, fixed_type: FixedType) -&gt; T:\n    \"\"\"Visit a FixedType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_float","title":"<code>visit_float(float_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a FloatType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_float(self, float_type: FloatType) -&gt; T:\n    \"\"\"Visit a FloatType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_integer","title":"<code>visit_integer(integer_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a IntegerType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_integer(self, integer_type: IntegerType) -&gt; T:\n    \"\"\"Visit a IntegerType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_long","title":"<code>visit_long(long_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a LongType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_long(self, long_type: LongType) -&gt; T:\n    \"\"\"Visit a LongType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_string","title":"<code>visit_string(string_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a StringType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_string(self, string_type: StringType) -&gt; T:\n    \"\"\"Visit a StringType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_time","title":"<code>visit_time(time_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_time(self, time_type: TimeType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamp","title":"<code>visit_timestamp(timestamp_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp(self, timestamp_type: TimestampType) -&gt; T:\n    \"\"\"Visit a TimestampType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamptz","title":"<code>visit_timestamptz(timestamptz_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; T:\n    \"\"\"Visit a TimestamptzType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_uuid","title":"<code>visit_uuid(uuid_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a UUIDType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_uuid(self, uuid_type: UUIDType) -&gt; T:\n    \"\"\"Visit a UUIDType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor","title":"<code>SchemaWithPartnerVisitor</code>","text":"<p>               Bases: <code>Generic[P, T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaWithPartnerVisitor(Generic[P, T], ABC):\n    def before_field(self, field: NestedField, field_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: NestedField, field_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: NestedField, element_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n        self.before_field(element, element_partner)\n\n    def after_list_element(self, element: NestedField, element_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n        self.after_field(element, element_partner)\n\n    def before_map_key(self, key: NestedField, key_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n        self.before_field(key, key_partner)\n\n    def after_map_key(self, key: NestedField, key_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n        self.after_field(key, key_partner)\n\n    def before_map_value(self, value: NestedField, value_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n        self.before_field(value, value_partner)\n\n    def after_map_value(self, value: NestedField, value_partner: Optional[P]) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n        self.after_field(value, value_partner)\n\n    @abstractmethod\n    def schema(self, schema: Schema, schema_partner: Optional[P], struct_result: T) -&gt; T:\n        \"\"\"Visit a schema with a partner.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, struct_partner: Optional[P], field_results: List[T]) -&gt; T:\n        \"\"\"Visit a struct type with a partner.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_partner: Optional[P], field_result: T) -&gt; T:\n        \"\"\"Visit a nested field with a partner.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, list_partner: Optional[P], element_result: T) -&gt; T:\n        \"\"\"Visit a list type with a partner.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, map_partner: Optional[P], key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a map type with a partner.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType, primitive_partner: Optional[P]) -&gt; T:\n        \"\"\"Visit a primitive type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_field","title":"<code>after_field(field, field_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_field(self, field: NestedField, field_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_list_element","title":"<code>after_list_element(element, element_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_list_element(self, element: NestedField, element_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n    self.after_field(element, element_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_map_key","title":"<code>after_map_key(key, key_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_key(self, key: NestedField, key_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n    self.after_field(key, key_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_map_value","title":"<code>after_map_value(value, value_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_value(self, value: NestedField, value_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n    self.after_field(value, value_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_field","title":"<code>before_field(field, field_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_field(self, field: NestedField, field_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_list_element","title":"<code>before_list_element(element, element_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_list_element(self, element: NestedField, element_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n    self.before_field(element, element_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_map_key","title":"<code>before_map_key(key, key_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_key(self, key: NestedField, key_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n    self.before_field(key, key_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_map_value","title":"<code>before_map_value(value, value_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_value(self, value: NestedField, value_partner: Optional[P]) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n    self.before_field(value, value_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.field","title":"<code>field(field, field_partner, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a nested field with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_partner: Optional[P], field_result: T) -&gt; T:\n    \"\"\"Visit a nested field with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.list","title":"<code>list(list_type, list_partner, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a list type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, list_partner: Optional[P], element_result: T) -&gt; T:\n    \"\"\"Visit a list type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.map","title":"<code>map(map_type, map_partner, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a map type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, map_partner: Optional[P], key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a map type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.primitive","title":"<code>primitive(primitive, primitive_partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a primitive type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType, primitive_partner: Optional[P]) -&gt; T:\n    \"\"\"Visit a primitive type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.schema","title":"<code>schema(schema, schema_partner, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a schema with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, schema_partner: Optional[P], struct_result: T) -&gt; T:\n    \"\"\"Visit a schema with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.struct","title":"<code>struct(struct, struct_partner, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a struct type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, struct_partner: Optional[P], field_results: List[T]) -&gt; T:\n    \"\"\"Visit a struct type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._BuildPositionAccessors","title":"<code>_BuildPositionAccessors</code>","text":"<p>               Bases: <code>SchemaVisitor[Dict[Position, Accessor]]</code></p> <p>A schema visitor for generating a field ID to accessor index.</p> Example <p>from pyiceberg.schema import Schema from pyiceberg.types import * schema = Schema( ...     NestedField(field_id=2, name=\"id\", field_type=IntegerType(), required=False), ...     NestedField(field_id=1, name=\"data\", field_type=StringType(), required=True), ...     NestedField( ...         field_id=3, ...         name=\"location\", ...         field_type=StructType( ...             NestedField(field_id=5, name=\"latitude\", field_type=FloatType(), required=False), ...             NestedField(field_id=6, name=\"longitude\", field_type=FloatType(), required=False), ...         ), ...         required=True, ...     ), ...     schema_id=1, ...     identifier_field_ids=[1], ... ) result = build_position_accessors(schema) expected = { ...     2: Accessor(position=0, inner=None), ...     1: Accessor(position=1, inner=None), ...     5: Accessor(position=2, inner=Accessor(position=0, inner=None)), ...     6: Accessor(position=2, inner=Accessor(position=1, inner=None)) ... } result == expected True</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class _BuildPositionAccessors(SchemaVisitor[Dict[Position, Accessor]]):\n    \"\"\"A schema visitor for generating a field ID to accessor index.\n\n    Example:\n        &gt;&gt;&gt; from pyiceberg.schema import Schema\n        &gt;&gt;&gt; from pyiceberg.types import *\n        &gt;&gt;&gt; schema = Schema(\n        ...     NestedField(field_id=2, name=\"id\", field_type=IntegerType(), required=False),\n        ...     NestedField(field_id=1, name=\"data\", field_type=StringType(), required=True),\n        ...     NestedField(\n        ...         field_id=3,\n        ...         name=\"location\",\n        ...         field_type=StructType(\n        ...             NestedField(field_id=5, name=\"latitude\", field_type=FloatType(), required=False),\n        ...             NestedField(field_id=6, name=\"longitude\", field_type=FloatType(), required=False),\n        ...         ),\n        ...         required=True,\n        ...     ),\n        ...     schema_id=1,\n        ...     identifier_field_ids=[1],\n        ... )\n        &gt;&gt;&gt; result = build_position_accessors(schema)\n        &gt;&gt;&gt; expected = {\n        ...     2: Accessor(position=0, inner=None),\n        ...     1: Accessor(position=1, inner=None),\n        ...     5: Accessor(position=2, inner=Accessor(position=0, inner=None)),\n        ...     6: Accessor(position=2, inner=Accessor(position=1, inner=None))\n        ... }\n        &gt;&gt;&gt; result == expected\n        True\n    \"\"\"\n\n    def schema(self, schema: Schema, struct_result: Dict[Position, Accessor]) -&gt; Dict[Position, Accessor]:\n        return struct_result\n\n    def struct(self, struct: StructType, field_results: List[Dict[Position, Accessor]]) -&gt; Dict[Position, Accessor]:\n        result = {}\n\n        for position, field in enumerate(struct.fields):\n            if field_results[position]:\n                for inner_field_id, acc in field_results[position].items():\n                    result[inner_field_id] = Accessor(position, inner=acc)\n            else:\n                result[field.field_id] = Accessor(position)\n\n        return result\n\n    def field(self, field: NestedField, field_result: Dict[Position, Accessor]) -&gt; Dict[Position, Accessor]:\n        return field_result\n\n    def list(self, list_type: ListType, element_result: Dict[Position, Accessor]) -&gt; Dict[Position, Accessor]:\n        return {}\n\n    def map(\n        self, map_type: MapType, key_result: Dict[Position, Accessor], value_result: Dict[Position, Accessor]\n    ) -&gt; Dict[Position, Accessor]:\n        return {}\n\n    def primitive(self, primitive: PrimitiveType) -&gt; Dict[Position, Accessor]:\n        return {}\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexById","title":"<code>_IndexById</code>","text":"<p>               Bases: <code>SchemaVisitor[Dict[int, NestedField]]</code></p> <p>A schema visitor for generating a field ID to NestedField index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class _IndexById(SchemaVisitor[Dict[int, NestedField]]):\n    \"\"\"A schema visitor for generating a field ID to NestedField index.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._index: Dict[int, NestedField] = {}\n\n    def schema(self, schema: Schema, struct_result: Dict[int, NestedField]) -&gt; Dict[int, NestedField]:\n        return self._index\n\n    def struct(self, struct: StructType, field_results: List[Dict[int, NestedField]]) -&gt; Dict[int, NestedField]:\n        return self._index\n\n    def field(self, field: NestedField, field_result: Dict[int, NestedField]) -&gt; Dict[int, NestedField]:\n        \"\"\"Add the field ID to the index.\"\"\"\n        self._index[field.field_id] = field\n        return self._index\n\n    def list(self, list_type: ListType, element_result: Dict[int, NestedField]) -&gt; Dict[int, NestedField]:\n        \"\"\"Add the list element ID to the index.\"\"\"\n        self._index[list_type.element_field.field_id] = list_type.element_field\n        return self._index\n\n    def map(\n        self, map_type: MapType, key_result: Dict[int, NestedField], value_result: Dict[int, NestedField]\n    ) -&gt; Dict[int, NestedField]:\n        \"\"\"Add the key ID and value ID as individual items in the index.\"\"\"\n        self._index[map_type.key_field.field_id] = map_type.key_field\n        self._index[map_type.value_field.field_id] = map_type.value_field\n        return self._index\n\n    def primitive(self, primitive: PrimitiveType) -&gt; Dict[int, NestedField]:\n        return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexById.field","title":"<code>field(field, field_result)</code>","text":"<p>Add the field ID to the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def field(self, field: NestedField, field_result: Dict[int, NestedField]) -&gt; Dict[int, NestedField]:\n    \"\"\"Add the field ID to the index.\"\"\"\n    self._index[field.field_id] = field\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexById.list","title":"<code>list(list_type, element_result)</code>","text":"<p>Add the list element ID to the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def list(self, list_type: ListType, element_result: Dict[int, NestedField]) -&gt; Dict[int, NestedField]:\n    \"\"\"Add the list element ID to the index.\"\"\"\n    self._index[list_type.element_field.field_id] = list_type.element_field\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexById.map","title":"<code>map(map_type, key_result, value_result)</code>","text":"<p>Add the key ID and value ID as individual items in the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def map(\n    self, map_type: MapType, key_result: Dict[int, NestedField], value_result: Dict[int, NestedField]\n) -&gt; Dict[int, NestedField]:\n    \"\"\"Add the key ID and value ID as individual items in the index.\"\"\"\n    self._index[map_type.key_field.field_id] = map_type.key_field\n    self._index[map_type.value_field.field_id] = map_type.value_field\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName","title":"<code>_IndexByName</code>","text":"<p>               Bases: <code>SchemaVisitor[Dict[str, int]]</code></p> <p>A schema visitor for generating a field name to field ID index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class _IndexByName(SchemaVisitor[Dict[str, int]]):\n    \"\"\"A schema visitor for generating a field name to field ID index.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._index: Dict[str, int] = {}\n        self._short_name_to_id: Dict[str, int] = {}\n        self._combined_index: Dict[str, int] = {}\n        self._field_names: List[str] = []\n        self._short_field_names: List[str] = []\n\n    def before_map_value(self, value: NestedField) -&gt; None:\n        if not isinstance(value.field_type, StructType):\n            self._short_field_names.append(value.name)\n        self._field_names.append(value.name)\n\n    def after_map_value(self, value: NestedField) -&gt; None:\n        if not isinstance(value.field_type, StructType):\n            self._short_field_names.pop()\n        self._field_names.pop()\n\n    def before_list_element(self, element: NestedField) -&gt; None:\n        \"\"\"Short field names omit element when the element is a StructType.\"\"\"\n        if not isinstance(element.field_type, StructType):\n            self._short_field_names.append(element.name)\n        self._field_names.append(element.name)\n\n    def after_list_element(self, element: NestedField) -&gt; None:\n        if not isinstance(element.field_type, StructType):\n            self._short_field_names.pop()\n        self._field_names.pop()\n\n    def before_field(self, field: NestedField) -&gt; None:\n        \"\"\"Store the field name.\"\"\"\n        self._field_names.append(field.name)\n        self._short_field_names.append(field.name)\n\n    def after_field(self, field: NestedField) -&gt; None:\n        \"\"\"Remove the last field name stored.\"\"\"\n        self._field_names.pop()\n        self._short_field_names.pop()\n\n    def schema(self, schema: Schema, struct_result: Dict[str, int]) -&gt; Dict[str, int]:\n        return self._index\n\n    def struct(self, struct: StructType, field_results: List[Dict[str, int]]) -&gt; Dict[str, int]:\n        return self._index\n\n    def field(self, field: NestedField, field_result: Dict[str, int]) -&gt; Dict[str, int]:\n        \"\"\"Add the field name to the index.\"\"\"\n        self._add_field(field.name, field.field_id)\n        return self._index\n\n    def list(self, list_type: ListType, element_result: Dict[str, int]) -&gt; Dict[str, int]:\n        \"\"\"Add the list element name to the index.\"\"\"\n        self._add_field(list_type.element_field.name, list_type.element_field.field_id)\n        return self._index\n\n    def map(self, map_type: MapType, key_result: Dict[str, int], value_result: Dict[str, int]) -&gt; Dict[str, int]:\n        \"\"\"Add the key name and value name as individual items in the index.\"\"\"\n        self._add_field(map_type.key_field.name, map_type.key_field.field_id)\n        self._add_field(map_type.value_field.name, map_type.value_field.field_id)\n        return self._index\n\n    def _add_field(self, name: str, field_id: int) -&gt; None:\n        \"\"\"Add a field name to the index, mapping its full name to its field ID.\n\n        Args:\n            name (str): The field name.\n            field_id (int): The field ID.\n\n        Raises:\n            ValueError: If the field name is already contained in the index.\n        \"\"\"\n        full_name = name\n\n        if self._field_names:\n            full_name = \".\".join([\".\".join(self._field_names), name])\n\n        if full_name in self._index:\n            raise ValueError(f\"Invalid schema, multiple fields for name {full_name}: {self._index[full_name]} and {field_id}\")\n        self._index[full_name] = field_id\n\n        if self._short_field_names:\n            short_name = \".\".join([\".\".join(self._short_field_names), name])\n            self._short_name_to_id[short_name] = field_id\n\n    def primitive(self, primitive: PrimitiveType) -&gt; Dict[str, int]:\n        return self._index\n\n    def by_name(self) -&gt; Dict[str, int]:\n        \"\"\"Return an index of combined full and short names.\n\n        Note: Only short names that do not conflict with full names are included.\n        \"\"\"\n        combined_index = self._short_name_to_id.copy()\n        combined_index.update(self._index)\n        return combined_index\n\n    def by_id(self) -&gt; Dict[int, str]:\n        \"\"\"Return an index of ID to full names.\"\"\"\n        id_to_full_name = {value: key for key, value in self._index.items()}\n        return id_to_full_name\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName._add_field","title":"<code>_add_field(name, field_id)</code>","text":"<p>Add a field name to the index, mapping its full name to its field ID.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The field name.</p> required <code>field_id</code> <code>int</code> <p>The field ID.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the field name is already contained in the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def _add_field(self, name: str, field_id: int) -&gt; None:\n    \"\"\"Add a field name to the index, mapping its full name to its field ID.\n\n    Args:\n        name (str): The field name.\n        field_id (int): The field ID.\n\n    Raises:\n        ValueError: If the field name is already contained in the index.\n    \"\"\"\n    full_name = name\n\n    if self._field_names:\n        full_name = \".\".join([\".\".join(self._field_names), name])\n\n    if full_name in self._index:\n        raise ValueError(f\"Invalid schema, multiple fields for name {full_name}: {self._index[full_name]} and {field_id}\")\n    self._index[full_name] = field_id\n\n    if self._short_field_names:\n        short_name = \".\".join([\".\".join(self._short_field_names), name])\n        self._short_name_to_id[short_name] = field_id\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.after_field","title":"<code>after_field(field)</code>","text":"<p>Remove the last field name stored.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_field(self, field: NestedField) -&gt; None:\n    \"\"\"Remove the last field name stored.\"\"\"\n    self._field_names.pop()\n    self._short_field_names.pop()\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.before_field","title":"<code>before_field(field)</code>","text":"<p>Store the field name.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_field(self, field: NestedField) -&gt; None:\n    \"\"\"Store the field name.\"\"\"\n    self._field_names.append(field.name)\n    self._short_field_names.append(field.name)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.before_list_element","title":"<code>before_list_element(element)</code>","text":"<p>Short field names omit element when the element is a StructType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_list_element(self, element: NestedField) -&gt; None:\n    \"\"\"Short field names omit element when the element is a StructType.\"\"\"\n    if not isinstance(element.field_type, StructType):\n        self._short_field_names.append(element.name)\n    self._field_names.append(element.name)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.by_id","title":"<code>by_id()</code>","text":"<p>Return an index of ID to full names.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def by_id(self) -&gt; Dict[int, str]:\n    \"\"\"Return an index of ID to full names.\"\"\"\n    id_to_full_name = {value: key for key, value in self._index.items()}\n    return id_to_full_name\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.by_name","title":"<code>by_name()</code>","text":"<p>Return an index of combined full and short names.</p> <p>Note: Only short names that do not conflict with full names are included.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def by_name(self) -&gt; Dict[str, int]:\n    \"\"\"Return an index of combined full and short names.\n\n    Note: Only short names that do not conflict with full names are included.\n    \"\"\"\n    combined_index = self._short_name_to_id.copy()\n    combined_index.update(self._index)\n    return combined_index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.field","title":"<code>field(field, field_result)</code>","text":"<p>Add the field name to the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def field(self, field: NestedField, field_result: Dict[str, int]) -&gt; Dict[str, int]:\n    \"\"\"Add the field name to the index.\"\"\"\n    self._add_field(field.name, field.field_id)\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.list","title":"<code>list(list_type, element_result)</code>","text":"<p>Add the list element name to the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def list(self, list_type: ListType, element_result: Dict[str, int]) -&gt; Dict[str, int]:\n    \"\"\"Add the list element name to the index.\"\"\"\n    self._add_field(list_type.element_field.name, list_type.element_field.field_id)\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._IndexByName.map","title":"<code>map(map_type, key_result, value_result)</code>","text":"<p>Add the key name and value name as individual items in the index.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def map(self, map_type: MapType, key_result: Dict[str, int], value_result: Dict[str, int]) -&gt; Dict[str, int]:\n    \"\"\"Add the key name and value name as individual items in the index.\"\"\"\n    self._add_field(map_type.key_field.name, map_type.key_field.field_id)\n    self._add_field(map_type.value_field.name, map_type.value_field.field_id)\n    return self._index\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._SetFreshIDs","title":"<code>_SetFreshIDs</code>","text":"<p>               Bases: <code>PreOrderSchemaVisitor[IcebergType]</code></p> <p>Traverses the schema and assigns monotonically increasing ids.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class _SetFreshIDs(PreOrderSchemaVisitor[IcebergType]):\n    \"\"\"Traverses the schema and assigns monotonically increasing ids.\"\"\"\n\n    old_id_to_new_id: Dict[int, int]\n\n    def __init__(self, next_id_func: Optional[Callable[[], int]] = None) -&gt; None:\n        self.old_id_to_new_id = {}\n        counter = itertools.count(1)\n        self.next_id_func = next_id_func if next_id_func is not None else lambda: next(counter)\n\n    def _get_and_increment(self, current_id: int) -&gt; int:\n        new_id = self.next_id_func()\n        self.old_id_to_new_id[current_id] = new_id\n        return new_id\n\n    def schema(self, schema: Schema, struct_result: Callable[[], StructType]) -&gt; Schema:\n        return Schema(\n            *struct_result().fields,\n            identifier_field_ids=[self.old_id_to_new_id[field_id] for field_id in schema.identifier_field_ids],\n        )\n\n    def struct(self, struct: StructType, field_results: List[Callable[[], IcebergType]]) -&gt; StructType:\n        new_ids = [self._get_and_increment(field.field_id) for field in struct.fields]\n        new_fields = []\n        for field_id, field, field_type in zip(new_ids, struct.fields, field_results):\n            new_fields.append(\n                NestedField(\n                    field_id=field_id,\n                    name=field.name,\n                    field_type=field_type(),\n                    required=field.required,\n                    doc=field.doc,\n                )\n            )\n        return StructType(*new_fields)\n\n    def field(self, field: NestedField, field_result: Callable[[], IcebergType]) -&gt; IcebergType:\n        return field_result()\n\n    def list(self, list_type: ListType, element_result: Callable[[], IcebergType]) -&gt; ListType:\n        element_id = self._get_and_increment(list_type.element_id)\n        return ListType(\n            element_id=element_id,\n            element=element_result(),\n            element_required=list_type.element_required,\n        )\n\n    def map(self, map_type: MapType, key_result: Callable[[], IcebergType], value_result: Callable[[], IcebergType]) -&gt; MapType:\n        key_id = self._get_and_increment(map_type.key_id)\n        value_id = self._get_and_increment(map_type.value_id)\n        return MapType(\n            key_id=key_id,\n            key_type=key_result(),\n            value_id=value_id,\n            value_type=value_result(),\n            value_required=map_type.value_required,\n        )\n\n    def primitive(self, primitive: PrimitiveType) -&gt; PrimitiveType:\n        return primitive\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._check_schema_compatible","title":"<code>_check_schema_compatible(requested_schema, provided_schema)</code>","text":"<p>Check if the <code>provided_schema</code> is compatible with <code>requested_schema</code>.</p> <p>Both Schemas must have valid IDs and share the same ID for the same field names.</p> <p>Two schemas are considered compatible when: 1. All <code>required</code> fields in <code>requested_schema</code> are present and are also <code>required</code> in the <code>provided_schema</code> 2. Field Types are consistent for fields that are present in both schemas. I.e. the field type    in the <code>provided_schema</code> can be promoted to the field type of the same field ID in <code>requested_schema</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the schemas are not compatible.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def _check_schema_compatible(requested_schema: Schema, provided_schema: Schema) -&gt; None:\n    \"\"\"\n    Check if the `provided_schema` is compatible with `requested_schema`.\n\n    Both Schemas must have valid IDs and share the same ID for the same field names.\n\n    Two schemas are considered compatible when:\n    1. All `required` fields in `requested_schema` are present and are also `required` in the `provided_schema`\n    2. Field Types are consistent for fields that are present in both schemas. I.e. the field type\n       in the `provided_schema` can be promoted to the field type of the same field ID in `requested_schema`\n\n    Raises:\n        ValueError: If the schemas are not compatible.\n    \"\"\"\n    pre_order_visit(requested_schema, _SchemaCompatibilityVisitor(provided_schema))\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema._index_parents","title":"<code>_index_parents(schema_or_type)</code>","text":"<p>Generate an index of field IDs to their parent field IDs.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>Dict[int, int]</code> <p>Dict[int, int]: An index of field IDs to their parent field IDs.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def _index_parents(schema_or_type: Union[Schema, IcebergType]) -&gt; Dict[int, int]:\n    \"\"\"Generate an index of field IDs to their parent field IDs.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[int, int]: An index of field IDs to their parent field IDs.\n    \"\"\"\n    return visit(schema_or_type, _IndexParents())\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.assign_fresh_schema_ids","title":"<code>assign_fresh_schema_ids(schema_or_type, next_id=None)</code>","text":"<p>Traverses the schema, and sets new IDs.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def assign_fresh_schema_ids(schema_or_type: Union[Schema, IcebergType], next_id: Optional[Callable[[], int]] = None) -&gt; Schema:\n    \"\"\"Traverses the schema, and sets new IDs.\"\"\"\n    return pre_order_visit(schema_or_type, _SetFreshIDs(next_id_func=next_id))\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.build_position_accessors","title":"<code>build_position_accessors(schema_or_type)</code>","text":"<p>Generate an index of field IDs to schema position accessors.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>Dict[int, Accessor]</code> <p>Dict[int, Accessor]: An index of field IDs to accessors.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def build_position_accessors(schema_or_type: Union[Schema, IcebergType]) -&gt; Dict[int, Accessor]:\n    \"\"\"Generate an index of field IDs to schema position accessors.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[int, Accessor]: An index of field IDs to accessors.\n    \"\"\"\n    return visit(schema_or_type, _BuildPositionAccessors())\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_by_id","title":"<code>index_by_id(schema_or_type)</code>","text":"<p>Generate an index of field IDs to NestedField instances.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>Dict[int, NestedField]</code> <p>Dict[int, NestedField]: An index of field IDs to NestedField instances.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_by_id(schema_or_type: Union[Schema, IcebergType]) -&gt; Dict[int, NestedField]:\n    \"\"\"Generate an index of field IDs to NestedField instances.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[int, NestedField]: An index of field IDs to NestedField instances.\n    \"\"\"\n    return visit(schema_or_type, _IndexById())\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_by_name","title":"<code>index_by_name(schema_or_type)</code>","text":"<p>Generate an index of field names to field IDs.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: An index of field names to field IDs.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_by_name(schema_or_type: Union[Schema, IcebergType]) -&gt; Dict[str, int]:\n    \"\"\"Generate an index of field names to field IDs.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[str, int]: An index of field names to field IDs.\n    \"\"\"\n    if len(schema_or_type.fields) &gt; 0:\n        indexer = _IndexByName()\n        visit(schema_or_type, indexer)\n        return indexer.by_name()\n    else:\n        return EMPTY_DICT\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_name_by_id","title":"<code>index_name_by_id(schema_or_type)</code>","text":"<p>Generate an index of field IDs full field names.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>Dict[int, str]</code> <p>Dict[str, int]: An index of field IDs to full names.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_name_by_id(schema_or_type: Union[Schema, IcebergType]) -&gt; Dict[int, str]:\n    \"\"\"Generate an index of field IDs full field names.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[str, int]: An index of field IDs to full names.\n    \"\"\"\n    indexer = _IndexByName()\n    visit(schema_or_type, indexer)\n    return indexer.by_id()\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.pre_order_visit","title":"<code>pre_order_visit(obj, visitor)</code>","text":"<p>Apply a schema visitor to any point within a schema.</p> <p>The function traverses the schema in pre-order fashion. This is a slimmed down version compared to the post-order traversal (missing before and after methods), mostly because we don't use the pre-order traversal much.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Schema, IcebergType]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>PreOrderSchemaVisitor[T]</code> <p>An instance of an implementation of the generic PreOrderSchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef pre_order_visit(obj: Union[Schema, IcebergType], visitor: PreOrderSchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a schema visitor to any point within a schema.\n\n    The function traverses the schema in pre-order fashion. This is a slimmed down version\n    compared to the post-order traversal (missing before and after methods), mostly\n    because we don't use the pre-order traversal much.\n\n    Args:\n        obj (Union[Schema, IcebergType]): An instance of a Schema or an IcebergType.\n        visitor (PreOrderSchemaVisitor[T]): An instance of an implementation of the generic PreOrderSchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.promote","title":"<code>promote(file_type, read_type)</code>","text":"<p>Promotes reading a file type to a read type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>IcebergType</code> <p>The type of the Avro file.</p> required <code>read_type</code> <code>IcebergType</code> <p>The requested read type.</p> required <p>Raises:</p> Type Description <code>ResolveError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef promote(file_type: IcebergType, read_type: IcebergType) -&gt; IcebergType:\n    \"\"\"Promotes reading a file type to a read type.\n\n    Args:\n        file_type (IcebergType): The type of the Avro file.\n        read_type (IcebergType): The requested read type.\n\n    Raises:\n        ResolveError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    if file_type == read_type:\n        return file_type\n    else:\n        raise ResolveError(f\"Cannot promote {file_type} to {read_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.prune_columns","title":"<code>prune_columns(schema, selected, select_full_types=True)</code>","text":"<p>Prunes a column by only selecting a set of field-ids.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The schema to be pruned.</p> required <code>selected</code> <code>Set[int]</code> <p>The field-ids to be included.</p> required <code>select_full_types</code> <code>bool</code> <p>Return the full struct when a subset is recorded</p> <code>True</code> <p>Returns:</p> Type Description <code>Schema</code> <p>The pruned schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def prune_columns(schema: Schema, selected: Set[int], select_full_types: bool = True) -&gt; Schema:\n    \"\"\"Prunes a column by only selecting a set of field-ids.\n\n    Args:\n        schema: The schema to be pruned.\n        selected: The field-ids to be included.\n        select_full_types: Return the full struct when a subset is recorded\n\n    Returns:\n        The pruned schema.\n    \"\"\"\n    result = visit(schema.as_struct(), _PruneColumnsVisitor(selected, select_full_types))\n    return Schema(\n        *(result or StructType()).fields,\n        schema_id=schema.schema_id,\n        identifier_field_ids=list(selected.intersection(schema.identifier_field_ids)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.sanitize_column_names","title":"<code>sanitize_column_names(schema)</code>","text":"<p>Sanitize column names to make them compatible with Avro.</p> <p>The column name should be starting with '' or digit followed by a string only contains '', digit or alphabet, otherwise it will be sanitized to conform the avro naming convention.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The schema to be sanitized.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>The sanitized schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def sanitize_column_names(schema: Schema) -&gt; Schema:\n    \"\"\"Sanitize column names to make them compatible with Avro.\n\n    The column name should be starting with '_' or digit followed by a string only contains '_', digit or alphabet,\n    otherwise it will be sanitized to conform the avro naming convention.\n\n    Args:\n        schema: The schema to be sanitized.\n\n    Returns:\n        The sanitized schema.\n    \"\"\"\n    result = visit(schema.as_struct(), _SanitizeColumnsVisitor())\n    return Schema(\n        *(result or StructType()).fields,\n        schema_id=schema.schema_id,\n        identifier_field_ids=schema.identifier_field_ids,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.visit","title":"<code>visit(obj, visitor)</code>","text":"<p>Apply a schema visitor to any point within a schema.</p> <p>The function traverses the schema in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Schema, IcebergType]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>SchemaVisitor[T]</code> <p>An instance of an implementation of the generic SchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef visit(obj: Union[Schema, IcebergType], visitor: SchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a schema visitor to any point within a schema.\n\n    The function traverses the schema in post-order fashion.\n\n    Args:\n        obj (Union[Schema, IcebergType]): An instance of a Schema or an IcebergType.\n        visitor (SchemaVisitor[T]): An instance of an implementation of the generic SchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/serializers/","title":"serializers","text":""},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor","title":"<code>Compressor</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class Compressor(ABC):\n    @staticmethod\n    def get_compressor(location: str) -&gt; Compressor:\n        return GzipCompressor() if location.endswith(\".gz.metadata.json\") else NOOP_COMPRESSOR\n\n    @abstractmethod\n    def stream_decompressor(self, inp: InputStream) -&gt; InputStream:\n        \"\"\"Return a stream decompressor.\n\n        Args:\n            inp: The input stream that needs decompressing.\n\n        Returns:\n            The wrapped stream\n        \"\"\"\n\n    @abstractmethod\n    def bytes_compressor(self) -&gt; Callable[[bytes], bytes]:\n        \"\"\"Return a function to compress bytes.\n\n        Returns:\n            A function that can be used to compress bytes.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor.bytes_compressor","title":"<code>bytes_compressor()</code>  <code>abstractmethod</code>","text":"<p>Return a function to compress bytes.</p> <p>Returns:</p> Type Description <code>Callable[[bytes], bytes]</code> <p>A function that can be used to compress bytes.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@abstractmethod\ndef bytes_compressor(self) -&gt; Callable[[bytes], bytes]:\n    \"\"\"Return a function to compress bytes.\n\n    Returns:\n        A function that can be used to compress bytes.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor.stream_decompressor","title":"<code>stream_decompressor(inp)</code>  <code>abstractmethod</code>","text":"<p>Return a stream decompressor.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>InputStream</code> <p>The input stream that needs decompressing.</p> required <p>Returns:</p> Type Description <code>InputStream</code> <p>The wrapped stream</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@abstractmethod\ndef stream_decompressor(self, inp: InputStream) -&gt; InputStream:\n    \"\"\"Return a stream decompressor.\n\n    Args:\n        inp: The input stream that needs decompressing.\n\n    Returns:\n        The wrapped stream\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromByteStream","title":"<code>FromByteStream</code>","text":"<p>A collection of methods that deserialize dictionaries into Iceberg objects.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class FromByteStream:\n    \"\"\"A collection of methods that deserialize dictionaries into Iceberg objects.\"\"\"\n\n    @staticmethod\n    def table_metadata(\n        byte_stream: InputStream, encoding: str = UTF8, compression: Compressor = NOOP_COMPRESSOR\n    ) -&gt; TableMetadata:\n        \"\"\"Instantiate a TableMetadata object from a byte stream.\n\n        Args:\n            byte_stream: A file-like byte stream object.\n            encoding (default \"utf-8\"): The byte encoder to use for the reader.\n            compression: Optional compression method\n        \"\"\"\n        with compression.stream_decompressor(byte_stream) as byte_stream:\n            reader = codecs.getreader(encoding)\n            json_bytes = reader(byte_stream)\n            metadata = json_bytes.read()\n\n        return TableMetadataUtil.parse_raw(metadata)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromByteStream.table_metadata","title":"<code>table_metadata(byte_stream, encoding=UTF8, compression=NOOP_COMPRESSOR)</code>  <code>staticmethod</code>","text":"<p>Instantiate a TableMetadata object from a byte stream.</p> <p>Parameters:</p> Name Type Description Default <code>byte_stream</code> <code>InputStream</code> <p>A file-like byte stream object.</p> required <code>encoding</code> <code>default \"utf-8\"</code> <p>The byte encoder to use for the reader.</p> <code>UTF8</code> <code>compression</code> <code>Compressor</code> <p>Optional compression method</p> <code>NOOP_COMPRESSOR</code> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(\n    byte_stream: InputStream, encoding: str = UTF8, compression: Compressor = NOOP_COMPRESSOR\n) -&gt; TableMetadata:\n    \"\"\"Instantiate a TableMetadata object from a byte stream.\n\n    Args:\n        byte_stream: A file-like byte stream object.\n        encoding (default \"utf-8\"): The byte encoder to use for the reader.\n        compression: Optional compression method\n    \"\"\"\n    with compression.stream_decompressor(byte_stream) as byte_stream:\n        reader = codecs.getreader(encoding)\n        json_bytes = reader(byte_stream)\n        metadata = json_bytes.read()\n\n    return TableMetadataUtil.parse_raw(metadata)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromInputFile","title":"<code>FromInputFile</code>","text":"<p>A collection of methods that deserialize InputFiles into Iceberg objects.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class FromInputFile:\n    \"\"\"A collection of methods that deserialize InputFiles into Iceberg objects.\"\"\"\n\n    @staticmethod\n    def table_metadata(input_file: InputFile, encoding: str = UTF8) -&gt; TableMetadata:\n        \"\"\"Create a TableMetadata instance from an input file.\n\n        Args:\n            input_file (InputFile): A custom implementation of the iceberg.io.file.InputFile abstract base class.\n            encoding (str): Encoding to use when loading bytestream.\n\n        Returns:\n            TableMetadata: A table metadata instance.\n\n        \"\"\"\n        with input_file.open() as input_stream:\n            return FromByteStream.table_metadata(\n                byte_stream=input_stream, encoding=encoding, compression=Compressor.get_compressor(location=input_file.location)\n            )\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromInputFile.table_metadata","title":"<code>table_metadata(input_file, encoding=UTF8)</code>  <code>staticmethod</code>","text":"<p>Create a TableMetadata instance from an input file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>InputFile</code> <p>A custom implementation of the iceberg.io.file.InputFile abstract base class.</p> required <code>encoding</code> <code>str</code> <p>Encoding to use when loading bytestream.</p> <code>UTF8</code> <p>Returns:</p> Name Type Description <code>TableMetadata</code> <code>TableMetadata</code> <p>A table metadata instance.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(input_file: InputFile, encoding: str = UTF8) -&gt; TableMetadata:\n    \"\"\"Create a TableMetadata instance from an input file.\n\n    Args:\n        input_file (InputFile): A custom implementation of the iceberg.io.file.InputFile abstract base class.\n        encoding (str): Encoding to use when loading bytestream.\n\n    Returns:\n        TableMetadata: A table metadata instance.\n\n    \"\"\"\n    with input_file.open() as input_stream:\n        return FromByteStream.table_metadata(\n            byte_stream=input_stream, encoding=encoding, compression=Compressor.get_compressor(location=input_file.location)\n        )\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.ToOutputFile","title":"<code>ToOutputFile</code>","text":"<p>A collection of methods that serialize Iceberg objects into files given an OutputFile instance.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class ToOutputFile:\n    \"\"\"A collection of methods that serialize Iceberg objects into files given an OutputFile instance.\"\"\"\n\n    @staticmethod\n    def table_metadata(metadata: TableMetadata, output_file: OutputFile, overwrite: bool = False) -&gt; None:\n        \"\"\"Write a TableMetadata instance to an output file.\n\n        Args:\n            output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\n            overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\n        \"\"\"\n        with output_file.create(overwrite=overwrite) as output_stream:\n            # We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\n            exclude_none = False if Config().get_bool(\"legacy-current-snapshot-id\") else True\n\n            json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n            json_bytes = Compressor.get_compressor(output_file.location).bytes_compressor()(json_bytes)\n            output_stream.write(json_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.ToOutputFile.table_metadata","title":"<code>table_metadata(metadata, output_file, overwrite=False)</code>  <code>staticmethod</code>","text":"<p>Write a TableMetadata instance to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>OutputFile</code> <p>A custom implementation of the iceberg.io.file.OutputFile abstract base class.</p> required <code>overwrite</code> <code>bool</code> <p>Where to overwrite the file if it already exists. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(metadata: TableMetadata, output_file: OutputFile, overwrite: bool = False) -&gt; None:\n    \"\"\"Write a TableMetadata instance to an output file.\n\n    Args:\n        output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\n        overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\n    \"\"\"\n    with output_file.create(overwrite=overwrite) as output_stream:\n        # We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\n        exclude_none = False if Config().get_bool(\"legacy-current-snapshot-id\") else True\n\n        json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n        json_bytes = Compressor.get_compressor(output_file.location).bytes_compressor()(json_bytes)\n        output_stream.write(json_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/","title":"transforms","text":""},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BoundTransform","title":"<code>BoundTransform</code>","text":"<p>               Bases: <code>BoundTerm[L]</code></p> <p>A transform expression.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class BoundTransform(BoundTerm[L]):\n    \"\"\"A transform expression.\"\"\"\n\n    transform: Transform[L, Any]\n\n    def __init__(self, term: BoundTerm[L], transform: Transform[L, Any]):\n        self.term: BoundTerm[L] = term\n        self.transform = transform\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BucketTransform","title":"<code>BucketTransform</code>","text":"<p>               Bases: <code>Transform[S, int]</code></p> <p>Base Transform class to transform a value into a bucket partition value.</p> <p>Transforms are parameterized by a number of buckets. Bucket partition transforms use a 32-bit hash of the source value to produce a positive value by mod the bucket number.</p> <p>Parameters:</p> Name Type Description Default <code>num_buckets</code> <code>int</code> <p>The number of buckets.</p> required Source code in <code>pyiceberg/transforms.py</code> <pre><code>class BucketTransform(Transform[S, int]):\n    \"\"\"Base Transform class to transform a value into a bucket partition value.\n\n    Transforms are parameterized by a number of buckets. Bucket partition transforms use a 32-bit\n    hash of the source value to produce a positive value by mod the bucket number.\n\n    Args:\n      num_buckets (int): The number of buckets.\n    \"\"\"\n\n    root: str = Field()\n    _num_buckets: PositiveInt = PrivateAttr()\n\n    def __init__(self, num_buckets: int, **data: Any) -&gt; None:\n        self._num_buckets = num_buckets\n        super().__init__(f\"bucket[{num_buckets}]\", **data)\n\n    @property\n    def num_buckets(self) -&gt; int:\n        return self._num_buckets\n\n    def hash(self, value: S) -&gt; int:\n        raise NotImplementedError()\n\n    def apply(self, value: Optional[S]) -&gt; Optional[int]:\n        return (self.hash(value) &amp; IntegerType.max) % self._num_buckets if value else None\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return IntegerType()\n\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        transformer = self.transform(pred.term.ref().field.field_type)\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundEqualTo):\n            return pred.as_unbound(Reference(name), _transform_literal(transformer, pred.literal))\n        elif isinstance(pred, BoundIn):  # NotIn can't be projected\n            return pred.as_unbound(Reference(name), {_transform_literal(transformer, literal) for literal in pred.literals})\n        else:\n            # - Comparison predicates can't be projected, notEq can't be projected\n            # - Small ranges can be projected:\n            #   For example, (x &gt; 0) and (x &lt; 3) can be turned into in({1, 2}) and projected.\n            return None\n\n    def strict_project(self, name: str, pred: BoundPredicate[Any]) -&gt; Optional[UnboundPredicate[Any]]:\n        transformer = self.transform(pred.term.ref().field.field_type)\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundNotEqualTo):\n            return pred.as_unbound(Reference(name), _transform_literal(transformer, pred.literal))\n        elif isinstance(pred, BoundNotIn):\n            return pred.as_unbound(Reference(name), {_transform_literal(transformer, literal) for literal in pred.literals})\n        else:\n            # no strict projection for comparison or equality\n            return None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(\n            source,\n            (\n                IntegerType,\n                DateType,\n                LongType,\n                TimeType,\n                TimestampType,\n                TimestamptzType,\n                DecimalType,\n                StringType,\n                FixedType,\n                BinaryType,\n                UUIDType,\n            ),\n        )\n\n    def transform(self, source: IcebergType, bucket: bool = True) -&gt; Callable[[Optional[Any]], Optional[int]]:\n        if isinstance(source, TimeType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.time):\n                    v = datetime.time_to_micros(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, DateType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, (IntegerType, LongType)):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, DecimalType):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(decimal_to_bytes(v))\n\n        elif isinstance(source, (StringType, FixedType, BinaryType)):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(v)\n\n        elif isinstance(source, UUIDType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, UUID):\n                    return mmh3.hash(v.bytes)\n                return mmh3.hash(v)\n\n        else:\n            raise ValueError(f\"Unknown type {source}\")\n\n        if bucket:\n            return lambda v: (hash_func(v) &amp; IntegerType.max) % self._num_buckets if v is not None else None\n        return hash_func\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BucketTransform class.\"\"\"\n        return f\"BucketTransform(num_buckets={self._num_buckets})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        from pyiceberg_core import transform as pyiceberg_core_transform\n\n        return self._pyiceberg_transform_wrapper(pyiceberg_core_transform.bucket, self._num_buckets)\n\n    @property\n    def supports_pyarrow_transform(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BucketTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BucketTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BucketTransform class.\"\"\"\n    return f\"BucketTransform(num_buckets={self._num_buckets})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform","title":"<code>DayTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a day value.</p> Example <p>transform = DayTransform() transform.transform(DateType())(17501) 17501</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class DayTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a day value.\n\n    Example:\n        &gt;&gt;&gt; transform = DayTransform()\n        &gt;&gt;&gt; transform.transform(DateType())(17501)\n        17501\n    \"\"\"\n\n    root: LiteralType[\"day\"] = Field(default=\"day\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[int]]:\n        if isinstance(source, DateType):\n\n            def day_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return v\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def day_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_days(v)\n\n        else:\n            raise ValueError(f\"Cannot apply day transform for type: {source}\")\n\n        return lambda v: day_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType))\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        \"\"\"Return the result type of a day transform.\n\n        The physical representation conforms to the Iceberg spec as DateType is internally converted to int.\n        The DateType returned here provides a more human-readable way to display the partition field.\n        \"\"\"\n        return DateType()\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.DAY\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return datetime.to_human_day(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DayTransform class.\"\"\"\n        return \"DayTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        import pyarrow as pa\n        import pyarrow.compute as pc\n\n        if isinstance(source, DateType):\n            epoch = datetime.EPOCH_DATE\n        elif isinstance(source, TimestampType):\n            epoch = datetime.EPOCH_TIMESTAMP\n        elif isinstance(source, TimestamptzType):\n            epoch = datetime.EPOCH_TIMESTAMPTZ\n        else:\n            raise ValueError(f\"Cannot apply day transform for type: {source}\")\n\n        return lambda v: pc.days_between(pa.scalar(epoch), v) if v is not None else None\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DayTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DayTransform class.\"\"\"\n    return \"DayTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform.result_type","title":"<code>result_type(source)</code>","text":"<p>Return the result type of a day transform.</p> <p>The physical representation conforms to the Iceberg spec as DateType is internally converted to int. The DateType returned here provides a more human-readable way to display the partition field.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def result_type(self, source: IcebergType) -&gt; IcebergType:\n    \"\"\"Return the result type of a day transform.\n\n    The physical representation conforms to the Iceberg spec as DateType is internally converted to int.\n    The DateType returned here provides a more human-readable way to display the partition field.\n    \"\"\"\n    return DateType()\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.HourTransform","title":"<code>HourTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a hour value.</p> Example <p>transform = HourTransform() transform.transform(TimestampType())(1512151975038194) 420042</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class HourTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a hour value.\n\n    Example:\n        &gt;&gt;&gt; transform = HourTransform()\n        &gt;&gt;&gt; transform.transform(TimestampType())(1512151975038194)\n        420042\n    \"\"\"\n\n    root: LiteralType[\"hour\"] = Field(default=\"hour\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[int]]:\n        if isinstance(source, (TimestampType, TimestamptzType)):\n\n            def hour_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_hours(v)\n\n        else:\n            raise ValueError(f\"Cannot apply hour transform for type: {source}\")\n\n        return lambda v: hour_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (TimestampType, TimestamptzType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.HOUR\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return datetime.to_human_hour(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the HourTransform class.\"\"\"\n        return \"HourTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        import pyarrow as pa\n        import pyarrow.compute as pc\n\n        if isinstance(source, TimestampType):\n            epoch = datetime.EPOCH_TIMESTAMP\n        elif isinstance(source, TimestamptzType):\n            epoch = datetime.EPOCH_TIMESTAMPTZ\n        else:\n            raise ValueError(f\"Cannot apply hour transform for type: {source}\")\n\n        return lambda v: pc.hours_between(pa.scalar(epoch), v) if v is not None else None\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.HourTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the HourTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the HourTransform class.\"\"\"\n    return \"HourTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform","title":"<code>IdentityTransform</code>","text":"<p>               Bases: <code>Transform[S, S]</code></p> <p>Transforms a value into itself.</p> Example <p>transform = IdentityTransform() transform.transform(StringType())('hello-world') 'hello-world'</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class IdentityTransform(Transform[S, S]):\n    \"\"\"Transforms a value into itself.\n\n    Example:\n        &gt;&gt;&gt; transform = IdentityTransform()\n        &gt;&gt;&gt; transform.transform(StringType())('hello-world')\n        'hello-world'\n    \"\"\"\n\n    root: LiteralType[\"identity\"] = Field(default=\"identity\")  # noqa: F821\n\n    def __init__(self) -&gt; None:\n        super().__init__(\"identity\")\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[S]]:\n        return lambda v: v\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return source.is_primitive\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundLiteralPredicate):\n            return pred.as_unbound(Reference(name), pred.literal)\n        elif isinstance(pred, BoundSetPredicate):\n            return pred.as_unbound(Reference(name), pred.literals)\n        else:\n            return None\n\n    def strict_project(self, name: str, pred: BoundPredicate[Any]) -&gt; Optional[UnboundPredicate[Any]]:\n        if isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundLiteralPredicate):\n            return pred.as_unbound(Reference(name), pred.literal)\n        elif isinstance(pred, BoundSetPredicate):\n            return pred.as_unbound(Reference(name), pred.literals)\n        else:\n            return None\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return True\n\n    def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n        \"\"\"Ordering by value is the same as long as the other preserves order.\"\"\"\n        return other.preserves_order\n\n    def to_human_string(self, source_type: IcebergType, value: Optional[S]) -&gt; str:\n        return _human_string(value, source_type) if value is not None else \"null\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n        return \"identity\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n        return \"IdentityTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        return lambda v: v\n\n    @property\n    def supports_pyarrow_transform(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the IdentityTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n    return \"IdentityTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the IdentityTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n    return \"identity\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.satisfies_order_of","title":"<code>satisfies_order_of(other)</code>","text":"<p>Ordering by value is the same as long as the other preserves order.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n    \"\"\"Ordering by value is the same as long as the other preserves order.\"\"\"\n    return other.preserves_order\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.MonthTransform","title":"<code>MonthTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a month value.</p> Example <p>transform = MonthTransform() transform.transform(DateType())(17501) 575</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class MonthTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a month value.\n\n    Example:\n        &gt;&gt;&gt; transform = MonthTransform()\n        &gt;&gt;&gt; transform.transform(DateType())(17501)\n        575\n    \"\"\"\n\n    root: LiteralType[\"month\"] = Field(default=\"month\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[int]]:\n        if isinstance(source, DateType):\n\n            def month_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return datetime.days_to_months(v)\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def month_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_months(v)\n\n        else:\n            raise ValueError(f\"Cannot apply month transform for type: {source}\")\n\n        return lambda v: month_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.MONTH\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return datetime.to_human_month(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the MonthTransform class.\"\"\"\n        return \"MonthTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        import pyarrow as pa\n        import pyarrow.compute as pc\n\n        if isinstance(source, DateType):\n            epoch = datetime.EPOCH_DATE\n        elif isinstance(source, TimestampType):\n            epoch = datetime.EPOCH_TIMESTAMP\n        elif isinstance(source, TimestamptzType):\n            epoch = datetime.EPOCH_TIMESTAMPTZ\n        else:\n            raise ValueError(f\"Cannot apply month transform for type: {source}\")\n\n        def month_func(v: pa.Array) -&gt; pa.Array:\n            return pc.add(\n                pc.multiply(pc.years_between(pa.scalar(epoch), v), pa.scalar(12)),\n                pc.add(pc.month(v), pa.scalar(-1)),\n            )\n\n        return lambda v: month_func(v) if v is not None else None\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.MonthTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the MonthTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the MonthTransform class.\"\"\"\n    return \"MonthTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform","title":"<code>Transform</code>","text":"<p>               Bases: <code>IcebergRootModel[str]</code>, <code>ABC</code>, <code>Generic[S, T]</code></p> <p>Transform base class for concrete transforms.</p> <p>A base class to transform values and project predicates on partition values. This class is not used directly. Instead, use one of module method to create the child classes.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class Transform(IcebergRootModel[str], ABC, Generic[S, T]):\n    \"\"\"Transform base class for concrete transforms.\n\n    A base class to transform values and project predicates on partition values.\n    This class is not used directly. Instead, use one of module method to create the child classes.\n    \"\"\"\n\n    root: str = Field()\n\n    @abstractmethod\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[T]]: ...\n\n    @abstractmethod\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return False\n\n    @abstractmethod\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        \"\"\"Return the `IcebergType` produced by this transform given a source type.\n\n        This method defines both the physical and display representation of the partition field.\n\n        The physical representation must conform to the Iceberg spec. The display representation\n        can deviate from the spec, such as by transforming the value into a more human-readable format.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]: ...\n\n    @abstractmethod\n    def strict_project(self, name: str, pred: BoundPredicate[Any]) -&gt; Optional[UnboundPredicate[Any]]: ...\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return False\n\n    def satisfies_order_of(self, other: Any) -&gt; bool:\n        return self == other\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return str(value) if value is not None else \"null\"\n\n    @property\n    def dedup_name(self) -&gt; str:\n        return self.__str__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Transform class.\"\"\"\n        return self.root\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Transform class.\"\"\"\n        if isinstance(other, Transform):\n            return self.root == other.root\n        return False\n\n    @property\n    def supports_pyarrow_transform(self) -&gt; bool:\n        return False\n\n    @abstractmethod\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\": ...\n\n    def _pyiceberg_transform_wrapper(\n        self, transform_func: Callable[[\"ArrayLike\", Any], \"ArrayLike\"], *args: Any\n    ) -&gt; Callable[[\"ArrayLike\"], \"ArrayLike\"]:\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For bucket/truncate transforms, PyArrow needs to be installed\") from e\n\n        def _transform(array: \"ArrayLike\") -&gt; \"ArrayLike\":\n            if isinstance(array, pa.Array):\n                return transform_func(array, *args)\n            elif isinstance(array, pa.ChunkedArray):\n                result_chunks = []\n                for arr in array.iterchunks():\n                    result_chunks.append(transform_func(arr, *args))\n                return pa.chunked_array(result_chunks)\n            else:\n                raise ValueError(f\"PyArrow array can only be of type pa.Array or pa.ChunkedArray, but found {type(array)}\")\n\n        return _transform\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Transform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Transform class.\"\"\"\n    if isinstance(other, Transform):\n        return self.root == other.root\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Transform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Transform class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.result_type","title":"<code>result_type(source)</code>  <code>abstractmethod</code>","text":"<p>Return the <code>IcebergType</code> produced by this transform given a source type.</p> <p>This method defines both the physical and display representation of the partition field.</p> <p>The physical representation must conform to the Iceberg spec. The display representation can deviate from the spec, such as by transforming the value into a more human-readable format.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>@abstractmethod\ndef result_type(self, source: IcebergType) -&gt; IcebergType:\n    \"\"\"Return the `IcebergType` produced by this transform given a source type.\n\n    This method defines both the physical and display representation of the partition field.\n\n    The physical representation must conform to the Iceberg spec. The display representation\n    can deviate from the spec, such as by transforming the value into a more human-readable format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.TruncateTransform","title":"<code>TruncateTransform</code>","text":"<p>               Bases: <code>Transform[S, S]</code></p> <p>A transform for truncating a value to a specified width.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>The truncate width, should be positive.</p> required <p>Raises:   ValueError: If a type is provided that is incompatible with a Truncate transform.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class TruncateTransform(Transform[S, S]):\n    \"\"\"A transform for truncating a value to a specified width.\n\n    Args:\n      width (int): The truncate width, should be positive.\n    Raises:\n      ValueError: If a type is provided that is incompatible with a Truncate transform.\n    \"\"\"\n\n    root: str = Field()\n    _source_type: IcebergType = PrivateAttr()\n    _width: PositiveInt = PrivateAttr()\n\n    def __init__(self, width: int, **data: Any):\n        super().__init__(root=f\"truncate[{width}]\", **data)\n        self._width = width\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (IntegerType, LongType, StringType, BinaryType, DecimalType))\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return True\n\n    @property\n    def source_type(self) -&gt; IcebergType:\n        return self._source_type\n\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        field_type = pred.term.ref().field.field_type\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n\n        if isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundIn):\n            return _set_apply_transform(name, pred, self.transform(field_type))\n        elif isinstance(field_type, (IntegerType, LongType, DecimalType)):\n            if isinstance(pred, BoundLiteralPredicate):\n                return _truncate_number(name, pred, self.transform(field_type))\n        elif isinstance(field_type, (BinaryType, StringType)):\n            if isinstance(pred, BoundLiteralPredicate):\n                return _truncate_array(name, pred, self.transform(field_type))\n        return None\n\n    def strict_project(self, name: str, pred: BoundPredicate[Any]) -&gt; Optional[UnboundPredicate[Any]]:\n        field_type = pred.term.ref().field.field_type\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n\n        if isinstance(field_type, (IntegerType, LongType, DecimalType)):\n            if isinstance(pred, BoundUnaryPredicate):\n                return pred.as_unbound(Reference(name))\n            elif isinstance(pred, BoundLiteralPredicate):\n                return _truncate_number_strict(name, pred, self.transform(field_type))\n            elif isinstance(pred, BoundNotIn):\n                return _set_apply_transform(name, pred, self.transform(field_type))\n            else:\n                return None\n\n        if isinstance(pred, BoundLiteralPredicate):\n            if isinstance(pred, BoundStartsWith):\n                literal_width = len(pred.literal.value)\n                if literal_width &lt; self.width:\n                    return pred.as_unbound(name, pred.literal.value)\n                elif literal_width == self.width:\n                    return EqualTo(name, pred.literal.value)\n                else:\n                    return None\n            elif isinstance(pred, BoundNotStartsWith):\n                literal_width = len(pred.literal.value)\n                if literal_width &lt; self.width:\n                    return pred.as_unbound(name, pred.literal.value)\n                elif literal_width == self.width:\n                    return NotEqualTo(name, pred.literal.value)\n                else:\n                    return pred.as_unbound(name, self.transform(field_type)(pred.literal.value))\n            else:\n                # ProjectionUtil.truncateArrayStrict(name, pred, this);\n                return _truncate_array_strict(name, pred, self.transform(field_type))\n        elif isinstance(pred, BoundNotIn):\n            return _set_apply_transform(name, pred, self.transform(field_type))\n        else:\n            return None\n\n    @property\n    def width(self) -&gt; int:\n        return self._width\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[S]]:\n        if isinstance(source, (IntegerType, LongType)):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return v - v % self._width\n\n        elif isinstance(source, (StringType, BinaryType)):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return v[0 : min(self._width, len(v))]\n\n        elif isinstance(source, DecimalType):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return truncate_decimal(v, self._width)\n\n        else:\n            raise ValueError(f\"Cannot truncate for type: {source}\")\n\n        return lambda v: truncate_func(v) if v is not None else None\n\n    def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n        if self == other:\n            return True\n        elif (\n            isinstance(self.source_type, StringType)\n            and isinstance(other, TruncateTransform)\n            and isinstance(other.source_type, StringType)\n        ):\n            return self.width &gt;= other.width\n\n        return False\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        if value is None:\n            return \"null\"\n        elif isinstance(value, bytes):\n            return _base64encode(value)\n        else:\n            return str(value)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the TruncateTransform class.\"\"\"\n        return f\"TruncateTransform(width={self._width})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        from pyiceberg_core import transform as pyiceberg_core_transform\n\n        return self._pyiceberg_transform_wrapper(pyiceberg_core_transform.truncate, self._width)\n\n    @property\n    def supports_pyarrow_transform(self) -&gt; bool:\n        return True\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.TruncateTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the TruncateTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the TruncateTransform class.\"\"\"\n    return f\"TruncateTransform(width={self._width})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.UnknownTransform","title":"<code>UnknownTransform</code>","text":"<p>               Bases: <code>Transform[S, T]</code></p> <p>A transform that represents when an unknown transform is provided.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>str</code> <p>A string name of a transform.</p> required <p>Other Parameters:</p> Name Type Description <code>source_type</code> <code>IcebergType</code> <p>An Iceberg <code>Type</code>.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class UnknownTransform(Transform[S, T]):\n    \"\"\"A transform that represents when an unknown transform is provided.\n\n    Args:\n      transform (str): A string name of a transform.\n\n    Keyword Args:\n      source_type (IcebergType): An Iceberg `Type`.\n    \"\"\"\n\n    root: LiteralType[\"unknown\"] = Field(default=\"unknown\")  # noqa: F821\n    _transform: str = PrivateAttr()\n\n    def __init__(self, transform: str, **data: Any):\n        super().__init__(**data)\n        self._transform = transform\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[T]]:\n        raise AttributeError(f\"Cannot apply unsupported transform: {self}\")\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return False\n\n    def result_type(self, source: IcebergType) -&gt; StringType:\n        return StringType()\n\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        return None\n\n    def strict_project(self, name: str, pred: BoundPredicate[Any]) -&gt; Optional[UnboundPredicate[Any]]:\n        return None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the UnknownTransform class.\"\"\"\n        return f\"UnknownTransform(transform={repr(self._transform)})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.UnknownTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the UnknownTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the UnknownTransform class.\"\"\"\n    return f\"UnknownTransform(transform={repr(self._transform)})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.VoidTransform","title":"<code>VoidTransform</code>","text":"<p>               Bases: <code>Transform[S, None]</code>, <code>Singleton</code></p> <p>A transform that always returns None.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class VoidTransform(Transform[S, None], Singleton):\n    \"\"\"A transform that always returns None.\"\"\"\n\n    root: str = \"void\"\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[T]]:\n        return lambda v: None\n\n    def can_transform(self, _: IcebergType) -&gt; bool:\n        return True\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    def project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        return None\n\n    def strict_project(self, name: str, pred: BoundPredicate[L]) -&gt; Optional[UnboundPredicate[Any]]:\n        return None\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the VoidTransform class.\"\"\"\n        return \"VoidTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.VoidTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the VoidTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the VoidTransform class.\"\"\"\n    return \"VoidTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.YearTransform","title":"<code>YearTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a year value.</p> Example <p>transform = YearTransform() transform.transform(TimestampType())(1512151975038194) 47</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class YearTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a year value.\n\n    Example:\n        &gt;&gt;&gt; transform = YearTransform()\n        &gt;&gt;&gt; transform.transform(TimestampType())(1512151975038194)\n        47\n    \"\"\"\n\n    root: LiteralType[\"year\"] = Field(default=\"year\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[Optional[S]], Optional[int]]:\n        if isinstance(source, DateType):\n\n            def year_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return datetime.days_to_years(v)\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def year_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_years(v)\n\n        else:\n            raise ValueError(f\"Cannot apply year transform for type: {source}\")\n\n        return lambda v: year_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.YEAR\n\n    def to_human_string(self, _: IcebergType, value: Optional[S]) -&gt; str:\n        return datetime.to_human_year(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the YearTransform class.\"\"\"\n        return \"YearTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        import pyarrow as pa\n        import pyarrow.compute as pc\n\n        if isinstance(source, DateType):\n            epoch = datetime.EPOCH_DATE\n        elif isinstance(source, TimestampType):\n            epoch = datetime.EPOCH_TIMESTAMP\n        elif isinstance(source, TimestamptzType):\n            epoch = datetime.EPOCH_TIMESTAMPTZ\n        else:\n            raise ValueError(f\"Cannot apply year transform for type: {source}\")\n\n        return lambda v: pc.years_between(pa.scalar(epoch), v) if v is not None else None\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.YearTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the YearTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the YearTransform class.\"\"\"\n    return \"YearTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms._base64encode","title":"<code>_base64encode(buffer)</code>","text":"<p>Convert bytes to base64 string.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def _base64encode(buffer: bytes) -&gt; str:\n    \"\"\"Convert bytes to base64 string.\"\"\"\n    return base64.b64encode(buffer).decode(\"ISO-8859-1\")\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms._transform_literal","title":"<code>_transform_literal(func, lit)</code>","text":"<p>Small helper to upwrap the value from the literal, and wrap it again.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def _transform_literal(func: Callable[[L], L], lit: Literal[L]) -&gt; Literal[L]:\n    \"\"\"Small helper to upwrap the value from the literal, and wrap it again.\"\"\"\n    return literal(func(lit.value))\n</code></pre>"},{"location":"reference/pyiceberg/typedef/","title":"typedef","text":""},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Identifier","title":"<code>Identifier = Tuple[str, ...]</code>  <code>module-attribute</code>","text":"<p>A tuple of strings representing a table identifier.</p> <p>Each string in the tuple represents a part of the table's unique path. For example, a table in a namespace might be identified as:</p> <pre><code>(\"namespace\", \"table_name\")\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; identifier: Identifier = (\"namespace\", \"table_name\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Properties","title":"<code>Properties = Dict[str, Any]</code>  <code>module-attribute</code>","text":"<p>A dictionary type for properties in PyIceberg.</p>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.RecursiveDict","title":"<code>RecursiveDict = Dict[str, Union[str, 'RecursiveDict']]</code>  <code>module-attribute</code>","text":"<p>A recursive dictionary type for nested structures in PyIceberg.</p>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.FrozenDict","title":"<code>FrozenDict</code>","text":"<p>               Bases: <code>Dict[Any, Any]</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class FrozenDict(Dict[Any, Any]):\n    def __setitem__(self, instance: Any, value: Any) -&gt; None:\n        \"\"\"Assign a value to a FrozenDict.\"\"\"\n        raise AttributeError(\"FrozenDict does not support assignment\")\n\n    def update(self, *args: Any, **kwargs: Any) -&gt; None:\n        raise AttributeError(\"FrozenDict does not support .update()\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.FrozenDict.__setitem__","title":"<code>__setitem__(instance, value)</code>","text":"<p>Assign a value to a FrozenDict.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __setitem__(self, instance: Any, value: Any) -&gt; None:\n    \"\"\"Assign a value to a FrozenDict.\"\"\"\n    raise AttributeError(\"FrozenDict does not support assignment\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.IcebergBaseModel","title":"<code>IcebergBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This class extends the Pydantic BaseModel to set default values by overriding them.</p> <p>This is because we always want to set by_alias to True. In Python, the dash can't be used in variable names, and this is used throughout the Iceberg spec.</p> <p>The same goes for exclude_none, if a field is None we want to omit it from serialization, for example, the doc attribute on the NestedField object. Default non-null values will be serialized.</p> <p>This is recommended by Pydantic: https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class IcebergBaseModel(BaseModel):\n    \"\"\"\n    This class extends the Pydantic BaseModel to set default values by overriding them.\n\n    This is because we always want to set by_alias to True. In Python, the dash can't\n    be used in variable names, and this is used throughout the Iceberg spec.\n\n    The same goes for exclude_none, if a field is None we want to omit it from\n    serialization, for example, the doc attribute on the NestedField object.\n    Default non-null values will be serialized.\n\n    This is recommended by Pydantic:\n    https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally\n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True, frozen=True)\n\n    def _exclude_private_properties(self, exclude: Optional[Set[str]] = None) -&gt; Set[str]:\n        # A small trick to exclude private properties. Properties are serialized by pydantic,\n        # regardless if they start with an underscore.\n        # This will look at the dict, and find the fields and exclude them\n        return set.union(\n            {field for field in self.__dict__ if field.startswith(\"_\") and not field == \"__root__\"}, exclude or set()\n        )\n\n    def model_dump(\n        self, exclude_none: bool = True, exclude: Optional[Set[str]] = None, by_alias: bool = True, **kwargs: Any\n    ) -&gt; Dict[str, Any]:\n        return super().model_dump(\n            exclude_none=exclude_none, exclude=self._exclude_private_properties(exclude), by_alias=by_alias, **kwargs\n        )\n\n    def model_dump_json(\n        self, exclude_none: bool = True, exclude: Optional[Set[str]] = None, by_alias: bool = True, **kwargs: Any\n    ) -&gt; str:\n        return super().model_dump_json(\n            exclude_none=exclude_none, exclude=self._exclude_private_properties(exclude), by_alias=by_alias, **kwargs\n        )\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.IcebergRootModel","title":"<code>IcebergRootModel</code>","text":"<p>               Bases: <code>RootModel[T]</code>, <code>Generic[T]</code></p> <p>This class extends the Pydantic BaseModel to set default values by overriding them.</p> <p>This is because we always want to set by_alias to True. In Python, the dash can't be used in variable names, and this is used throughout the Iceberg spec.</p> <p>The same goes for exclude_none, if a field is None we want to omit it from serialization, for example, the doc attribute on the NestedField object. Default non-null values will be serialized.</p> <p>This is recommended by Pydantic: https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class IcebergRootModel(RootModel[T], Generic[T]):\n    \"\"\"\n    This class extends the Pydantic BaseModel to set default values by overriding them.\n\n    This is because we always want to set by_alias to True. In Python, the dash can't\n    be used in variable names, and this is used throughout the Iceberg spec.\n\n    The same goes for exclude_none, if a field is None we want to omit it from\n    serialization, for example, the doc attribute on the NestedField object.\n    Default non-null values will be serialized.\n\n    This is recommended by Pydantic:\n    https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.KeyDefaultDict","title":"<code>KeyDefaultDict</code>","text":"<p>               Bases: <code>Dict[K, V]</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class KeyDefaultDict(Dict[K, V]):\n    def __init__(self, default_factory: Callable[[K], V]):\n        super().__init__()\n        self.default_factory = default_factory\n\n    def __missing__(self, key: K) -&gt; V:\n        \"\"\"Define behavior if you access a non-existent key in a KeyDefaultDict.\"\"\"\n        val = self.default_factory(key)\n        self[key] = val\n        return val\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.KeyDefaultDict.__missing__","title":"<code>__missing__(key)</code>","text":"<p>Define behavior if you access a non-existent key in a KeyDefaultDict.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __missing__(self, key: K) -&gt; V:\n    \"\"\"Define behavior if you access a non-existent key in a KeyDefaultDict.\"\"\"\n    val = self.default_factory(key)\n    self[key] = val\n    return val\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record","title":"<code>Record</code>","text":"<p>               Bases: <code>StructProtocol</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class Record(StructProtocol):\n    __slots__ = (\"_position_to_field_name\",)\n    _position_to_field_name: Tuple[str, ...]\n\n    def __init__(self, *data: Any, struct: Optional[StructType] = None, **named_data: Any) -&gt; None:\n        if struct is not None:\n            self._position_to_field_name = _get_struct_fields(struct)\n        elif named_data:\n            # Order of named_data is preserved (PEP 468) so this can be used to generate the position dict\n            self._position_to_field_name = tuple(named_data.keys())\n        else:\n            self._position_to_field_name = tuple(f\"field{idx + 1}\" for idx in range(len(data)))\n\n        for idx, d in enumerate(data):\n            self[idx] = d\n\n        for field_name, d in named_data.items():\n            self.__setattr__(field_name, d)\n\n    def __setitem__(self, pos: int, value: Any) -&gt; None:\n        \"\"\"Assign a value to a Record.\"\"\"\n        self.__setattr__(self._position_to_field_name[pos], value)\n\n    def __getitem__(self, pos: int) -&gt; Any:\n        \"\"\"Fetch a value from a Record.\"\"\"\n        return self.__getattribute__(self._position_to_field_name[pos])\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Record class.\"\"\"\n        if not isinstance(other, Record):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Record class.\"\"\"\n        return f\"{self.__class__.__name__}[{', '.join(f'{key}={repr(value)}' for key, value in self.__dict__.items() if not key.startswith('_'))}]\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of fields in the Record class.\"\"\"\n        return len(self._position_to_field_name)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return hash value of the Record class.\"\"\"\n        return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Record class.\"\"\"\n    if not isinstance(other, Record):\n        return False\n    return self.__dict__ == other.__dict__\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__getitem__","title":"<code>__getitem__(pos)</code>","text":"<p>Fetch a value from a Record.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __getitem__(self, pos: int) -&gt; Any:\n    \"\"\"Fetch a value from a Record.\"\"\"\n    return self.__getattribute__(self._position_to_field_name[pos])\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__hash__","title":"<code>__hash__()</code>","text":"<p>Return hash value of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash value of the Record class.\"\"\"\n    return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of fields in the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of fields in the Record class.\"\"\"\n    return len(self._position_to_field_name)\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Record class.\"\"\"\n    return f\"{self.__class__.__name__}[{', '.join(f'{key}={repr(value)}' for key, value in self.__dict__.items() if not key.startswith('_'))}]\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__setitem__","title":"<code>__setitem__(pos, value)</code>","text":"<p>Assign a value to a Record.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __setitem__(self, pos: int, value: Any) -&gt; None:\n    \"\"\"Assign a value to a Record.\"\"\"\n    self.__setattr__(self._position_to_field_name[pos], value)\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol","title":"<code>StructProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A generic protocol used by accessors to get and set at positions of an object.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@runtime_checkable\nclass StructProtocol(Protocol):  # pragma: no cover\n    \"\"\"A generic protocol used by accessors to get and set at positions of an object.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, pos: int) -&gt; Any:\n        \"\"\"Fetch a value from a StructProtocol.\"\"\"\n\n    @abstractmethod\n    def __setitem__(self, pos: int, value: Any) -&gt; None:\n        \"\"\"Assign a value to a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol.__getitem__","title":"<code>__getitem__(pos)</code>  <code>abstractmethod</code>","text":"<p>Fetch a value from a StructProtocol.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, pos: int) -&gt; Any:\n    \"\"\"Fetch a value from a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol.__setitem__","title":"<code>__setitem__(pos, value)</code>  <code>abstractmethod</code>","text":"<p>Assign a value to a StructProtocol.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@abstractmethod\ndef __setitem__(self, pos: int, value: Any) -&gt; None:\n    \"\"\"Assign a value to a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/types/","title":"types","text":"<p>Data types used in describing Iceberg schemas.</p> <p>This module implements the data types described in the Iceberg specification for Iceberg schemas. To describe an Iceberg table schema, these classes can be used in the construction of a StructType instance.</p> Example <p>str(StructType( ...     NestedField(1, \"required_field\", StringType(), True), ...     NestedField(2, \"optional_field\", IntegerType()) ... )) 'struct&lt;1: required_field: required string, 2: optional_field: optional int&gt;'</p> Notes <ul> <li>https://iceberg.apache.org/spec/#primitive-types</li> </ul>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.BinaryType","title":"<code>BinaryType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Binary data type in Iceberg can be represented using an instance of this class.</p> <p>Binaries in Iceberg are arbitrary-length byte arrays.</p> Example <p>column_foo = BinaryType() isinstance(column_foo, BinaryType) True column_foo BinaryType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class BinaryType(PrimitiveType):\n    \"\"\"A Binary data type in Iceberg can be represented using an instance of this class.\n\n    Binaries in Iceberg are arbitrary-length byte arrays.\n\n    Example:\n        &gt;&gt;&gt; column_foo = BinaryType()\n        &gt;&gt;&gt; isinstance(column_foo, BinaryType)\n        True\n        &gt;&gt;&gt; column_foo\n        BinaryType()\n    \"\"\"\n\n    root: Literal[\"binary\"] = Field(default=\"binary\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.BooleanType","title":"<code>BooleanType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A boolean data type in Iceberg can be represented using an instance of this class.</p> Example <p>column_foo = BooleanType() isinstance(column_foo, BooleanType) True column_foo BooleanType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class BooleanType(PrimitiveType):\n    \"\"\"A boolean data type in Iceberg can be represented using an instance of this class.\n\n    Example:\n        &gt;&gt;&gt; column_foo = BooleanType()\n        &gt;&gt;&gt; isinstance(column_foo, BooleanType)\n        True\n        &gt;&gt;&gt; column_foo\n        BooleanType()\n    \"\"\"\n\n    root: Literal[\"boolean\"] = Field(default=\"boolean\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DateType","title":"<code>DateType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Date data type in Iceberg can be represented using an instance of this class.</p> <p>Dates in Iceberg are calendar dates without a timezone or time.</p> Example <p>column_foo = DateType() isinstance(column_foo, DateType) True column_foo DateType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DateType(PrimitiveType):\n    \"\"\"A Date data type in Iceberg can be represented using an instance of this class.\n\n    Dates in Iceberg are calendar dates without a timezone or time.\n\n    Example:\n        &gt;&gt;&gt; column_foo = DateType()\n        &gt;&gt;&gt; isinstance(column_foo, DateType)\n        True\n        &gt;&gt;&gt; column_foo\n        DateType()\n    \"\"\"\n\n    root: Literal[\"date\"] = Field(default=\"date\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType","title":"<code>DecimalType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A decimal data type in Iceberg.</p> Example <p>DecimalType(32, 3) DecimalType(precision=32, scale=3) DecimalType(8, 3) == DecimalType(8, 3) True</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DecimalType(PrimitiveType):\n    \"\"\"A decimal data type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; DecimalType(32, 3)\n        DecimalType(precision=32, scale=3)\n        &gt;&gt;&gt; DecimalType(8, 3) == DecimalType(8, 3)\n        True\n    \"\"\"\n\n    root: Tuple[int, int]\n\n    def __init__(self, precision: int, scale: int) -&gt; None:\n        super().__init__(root=(precision, scale))\n\n    @model_serializer\n    def ser_model(self) -&gt; str:\n        \"\"\"Serialize the model to a string.\"\"\"\n        return f\"decimal({self.precision}, {self.scale})\"\n\n    @property\n    def precision(self) -&gt; int:\n        \"\"\"Return the precision of the decimal.\"\"\"\n        return self.root[0]\n\n    @property\n    def scale(self) -&gt; int:\n        \"\"\"Return the scale of the decimal.\"\"\"\n        return self.root[1]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DecimalType class.\"\"\"\n        return f\"DecimalType(precision={self.precision}, scale={self.scale})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return f\"decimal({self.precision}, {self.scale})\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the tuple.\"\"\"\n        return hash(self.root)\n\n    def __getnewargs__(self) -&gt; Tuple[int, int]:\n        \"\"\"Pickle the DecimalType class.\"\"\"\n        return self.precision, self.scale\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare to root to another object.\"\"\"\n        return self.root == other.root if isinstance(other, DecimalType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.precision","title":"<code>precision</code>  <code>property</code>","text":"<p>Return the precision of the decimal.</p>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.scale","title":"<code>scale</code>  <code>property</code>","text":"<p>Return the scale of the decimal.</p>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare to root to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare to root to another object.\"\"\"\n    return self.root == other.root if isinstance(other, DecimalType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the DecimalType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[int, int]:\n    \"\"\"Pickle the DecimalType class.\"\"\"\n    return self.precision, self.scale\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the tuple.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the tuple.\"\"\"\n    return hash(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DecimalType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DecimalType class.\"\"\"\n    return f\"DecimalType(precision={self.precision}, scale={self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return f\"decimal({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.ser_model","title":"<code>ser_model()</code>","text":"<p>Serialize the model to a string.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>@model_serializer\ndef ser_model(self) -&gt; str:\n    \"\"\"Serialize the model to a string.\"\"\"\n    return f\"decimal({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DoubleType","title":"<code>DoubleType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Double data type in Iceberg can be represented using an instance of this class.</p> <p>Doubles in Iceberg are 64-bit IEEE 754 floating points.</p> Example <p>column_foo = DoubleType() isinstance(column_foo, DoubleType) True column_foo DoubleType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DoubleType(PrimitiveType):\n    \"\"\"A Double data type in Iceberg can be represented using an instance of this class.\n\n    Doubles in Iceberg are 64-bit IEEE 754 floating points.\n\n    Example:\n        &gt;&gt;&gt; column_foo = DoubleType()\n        &gt;&gt;&gt; isinstance(column_foo, DoubleType)\n        True\n        &gt;&gt;&gt; column_foo\n        DoubleType()\n    \"\"\"\n\n    root: Literal[\"double\"] = Field(default=\"double\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType","title":"<code>FixedType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A fixed data type in Iceberg.</p> Example <p>FixedType(8) FixedType(length=8) FixedType(8) == FixedType(8) True FixedType(19) == FixedType(25) False</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class FixedType(PrimitiveType):\n    \"\"\"A fixed data type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; FixedType(8)\n        FixedType(length=8)\n        &gt;&gt;&gt; FixedType(8) == FixedType(8)\n        True\n        &gt;&gt;&gt; FixedType(19) == FixedType(25)\n        False\n    \"\"\"\n\n    root: int = Field()\n\n    def __init__(self, length: int) -&gt; None:\n        super().__init__(root=length)\n\n    @model_serializer\n    def ser_model(self) -&gt; str:\n        return f\"fixed[{self.root}]\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the FixedType class.\"\"\"\n        return self.root\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return f\"fixed[{self.root}]\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FixedType class.\"\"\"\n        return f\"FixedType(length={self.root})\"\n\n    def __getnewargs__(self) -&gt; tuple[int]:\n        \"\"\"Pickle the FixedType class.\"\"\"\n        return (self.root,)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int]:\n    \"\"\"Pickle the FixedType class.\"\"\"\n    return (self.root,)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the FixedType class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FixedType class.\"\"\"\n    return f\"FixedType(length={self.root})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return f\"fixed[{self.root}]\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FloatType","title":"<code>FloatType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Float data type in Iceberg can be represented using an instance of this class.</p> <p>Floats in Iceberg are 32-bit IEEE 754 floating points and can be promoted to Doubles.</p> Example <p>column_foo = FloatType() isinstance(column_foo, FloatType) True column_foo FloatType()</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>float</code> <p>The maximum allowed value for Floats, inherited from the canonical Iceberg implementation in Java. (returns <code>3.4028235e38</code>)</p> <code>min</code> <code>float</code> <p>The minimum allowed value for Floats, inherited from the canonical Iceberg implementation in Java (returns <code>-3.4028235e38</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class FloatType(PrimitiveType):\n    \"\"\"A Float data type in Iceberg can be represented using an instance of this class.\n\n    Floats in Iceberg are 32-bit IEEE 754 floating points and can be promoted to Doubles.\n\n    Example:\n        &gt;&gt;&gt; column_foo = FloatType()\n        &gt;&gt;&gt; isinstance(column_foo, FloatType)\n        True\n        &gt;&gt;&gt; column_foo\n        FloatType()\n\n    Attributes:\n        max (float): The maximum allowed value for Floats, inherited from the canonical Iceberg implementation\n            in Java. (returns `3.4028235e38`)\n        min (float): The minimum allowed value for Floats, inherited from the canonical Iceberg implementation\n            in Java (returns `-3.4028235e38`)\n    \"\"\"\n\n    max: ClassVar[float] = 3.4028235e38\n    min: ClassVar[float] = -3.4028235e38\n\n    root: Literal[\"float\"] = Field(default=\"float\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.IcebergType","title":"<code>IcebergType</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Base type for all Iceberg Types.</p> Example <p>str(IcebergType()) 'IcebergType()' repr(IcebergType()) 'IcebergType()'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class IcebergType(IcebergBaseModel):\n    \"\"\"Base type for all Iceberg Types.\n\n    Example:\n        &gt;&gt;&gt; str(IcebergType())\n        'IcebergType()'\n        &gt;&gt;&gt; repr(IcebergType())\n        'IcebergType()'\n    \"\"\"\n\n    @model_validator(mode=\"wrap\")\n    @classmethod\n    def handle_primitive_type(cls, v: Any, handler: ValidatorFunctionWrapHandler) -&gt; IcebergType:\n        # Pydantic works mostly around dicts, and there seems to be something\n        # by not serializing into a RootModel, might revisit this.\n        if isinstance(v, str):\n            if v == \"boolean\":\n                return BooleanType()\n            elif v == \"string\":\n                return StringType()\n            elif v == \"int\":\n                return IntegerType()\n            elif v == \"long\":\n                return LongType()\n            if v == \"float\":\n                return FloatType()\n            if v == \"double\":\n                return DoubleType()\n            if v == \"timestamp\":\n                return TimestampType()\n            if v == \"timestamptz\":\n                return TimestamptzType()\n            if v == \"date\":\n                return DateType()\n            if v == \"time\":\n                return TimeType()\n            if v == \"uuid\":\n                return UUIDType()\n            if v == \"binary\":\n                return BinaryType()\n            if v.startswith(\"fixed\"):\n                return FixedType(_parse_fixed_type(v))\n            if v.startswith(\"decimal\"):\n                precision, scale = _parse_decimal_type(v)\n                return DecimalType(precision, scale)\n            else:\n                raise ValueError(f\"Unknown type: {v}\")\n        if isinstance(v, dict) and cls == IcebergType:\n            complex_type = v.get(\"type\")\n            if complex_type == \"list\":\n                return ListType(**v)\n            elif complex_type == \"map\":\n                return MapType(**v)\n            elif complex_type == \"struct\":\n                return StructType(**v)\n            else:\n                return NestedField(**v)\n        return handler(v)\n\n    @property\n    def is_primitive(self) -&gt; bool:\n        return isinstance(self, PrimitiveType)\n\n    @property\n    def is_struct(self) -&gt; bool:\n        return isinstance(self, StructType)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.IntegerType","title":"<code>IntegerType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>An Integer data type in Iceberg can be represented using an instance of this class.</p> <p>Integers in Iceberg are 32-bit signed and can be promoted to Longs.</p> Example <p>column_foo = IntegerType() isinstance(column_foo, IntegerType) True</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>int</code> <p>The maximum allowed value for Integers, inherited from the canonical Iceberg implementation in Java (returns <code>2147483647</code>)</p> <code>min</code> <code>int</code> <p>The minimum allowed value for Integers, inherited from the canonical Iceberg implementation in Java (returns <code>-2147483648</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class IntegerType(PrimitiveType):\n    \"\"\"An Integer data type in Iceberg can be represented using an instance of this class.\n\n    Integers in Iceberg are 32-bit signed and can be promoted to Longs.\n\n    Example:\n        &gt;&gt;&gt; column_foo = IntegerType()\n        &gt;&gt;&gt; isinstance(column_foo, IntegerType)\n        True\n\n    Attributes:\n        max (int): The maximum allowed value for Integers, inherited from the canonical Iceberg implementation\n            in Java (returns `2147483647`)\n        min (int): The minimum allowed value for Integers, inherited from the canonical Iceberg implementation\n            in Java (returns `-2147483648`)\n    \"\"\"\n\n    root: Literal[\"int\"] = Field(default=\"int\")\n\n    max: ClassVar[int] = 2147483647\n    min: ClassVar[int] = -2147483648\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType","title":"<code>ListType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A list type in Iceberg.</p> Example <p>ListType(element_id=3, element_type=StringType(), element_required=True) ListType(element_id=3, element_type=StringType(), element_required=True)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class ListType(IcebergType):\n    \"\"\"A list type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; ListType(element_id=3, element_type=StringType(), element_required=True)\n        ListType(element_id=3, element_type=StringType(), element_required=True)\n    \"\"\"\n\n    type: Literal[\"list\"] = Field(default=\"list\")\n    element_id: int = Field(alias=\"element-id\")\n    element_type: SerializeAsAny[IcebergType] = Field(alias=\"element\")\n    element_required: bool = Field(alias=\"element-required\", default=True)\n    _element_field: NestedField = PrivateAttr()\n    _hash: int = PrivateAttr()\n\n    def __init__(\n        self, element_id: Optional[int] = None, element: Optional[IcebergType] = None, element_required: bool = True, **data: Any\n    ):\n        data[\"element-id\"] = data[\"element-id\"] if \"element-id\" in data else element_id\n        data[\"element\"] = element or data[\"element_type\"]\n        data[\"element-required\"] = data[\"element-required\"] if \"element-required\" in data else element_required\n        super().__init__(**data)\n        self._hash = hash(data.values())\n\n    @cached_property\n    def element_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"element\",\n            field_id=self.element_id,\n            field_type=self.element_type,\n            required=self.element_required,\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the ListType class.\"\"\"\n        return f\"list&lt;{self.element_type}&gt;\"\n\n    def __getnewargs__(self) -&gt; Tuple[int, IcebergType, bool]:\n        \"\"\"Pickle the ListType class.\"\"\"\n        return (self.element_id, self.element_type, self.element_required)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Use the cache hash value of the StructType class.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the list type to another list type.\"\"\"\n        return self.element_field == other.element_field if isinstance(other, ListType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the list type to another list type.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the list type to another list type.\"\"\"\n    return self.element_field == other.element_field if isinstance(other, ListType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the ListType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[int, IcebergType, bool]:\n    \"\"\"Pickle the ListType class.\"\"\"\n    return (self.element_id, self.element_type, self.element_required)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__hash__","title":"<code>__hash__()</code>","text":"<p>Use the cache hash value of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Use the cache hash value of the StructType class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the ListType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the ListType class.\"\"\"\n    return f\"list&lt;{self.element_type}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.LongType","title":"<code>LongType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Long data type in Iceberg can be represented using an instance of this class.</p> <p>Longs in Iceberg are 64-bit signed integers.</p> Example <p>column_foo = LongType() isinstance(column_foo, LongType) True column_foo LongType() str(column_foo) 'long'</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>int</code> <p>The maximum allowed value for Longs, inherited from the canonical Iceberg implementation in Java. (returns <code>9223372036854775807</code>)</p> <code>min</code> <code>int</code> <p>The minimum allowed value for Longs, inherited from the canonical Iceberg implementation in Java (returns <code>-9223372036854775808</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class LongType(PrimitiveType):\n    \"\"\"A Long data type in Iceberg can be represented using an instance of this class.\n\n    Longs in Iceberg are 64-bit signed integers.\n\n    Example:\n        &gt;&gt;&gt; column_foo = LongType()\n        &gt;&gt;&gt; isinstance(column_foo, LongType)\n        True\n        &gt;&gt;&gt; column_foo\n        LongType()\n        &gt;&gt;&gt; str(column_foo)\n        'long'\n\n    Attributes:\n        max (int): The maximum allowed value for Longs, inherited from the canonical Iceberg implementation\n            in Java. (returns `9223372036854775807`)\n        min (int): The minimum allowed value for Longs, inherited from the canonical Iceberg implementation\n            in Java (returns `-9223372036854775808`)\n    \"\"\"\n\n    root: Literal[\"long\"] = Field(default=\"long\")\n\n    max: ClassVar[int] = 9223372036854775807\n    min: ClassVar[int] = -9223372036854775808\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType","title":"<code>MapType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A map type in Iceberg.</p> Example <p>MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True) MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class MapType(IcebergType):\n    \"\"\"A map type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)\n        MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)\n    \"\"\"\n\n    type: Literal[\"map\"] = Field(default=\"map\")\n    key_id: int = Field(alias=\"key-id\")\n    key_type: SerializeAsAny[IcebergType] = Field(alias=\"key\")\n    value_id: int = Field(alias=\"value-id\")\n    value_type: SerializeAsAny[IcebergType] = Field(alias=\"value\")\n    value_required: bool = Field(alias=\"value-required\", default=True)\n    _hash: int = PrivateAttr()\n\n    def __init__(\n        self,\n        key_id: Optional[int] = None,\n        key_type: Optional[IcebergType] = None,\n        value_id: Optional[int] = None,\n        value_type: Optional[IcebergType] = None,\n        value_required: bool = True,\n        **data: Any,\n    ):\n        data[\"key-id\"] = data[\"key-id\"] if \"key-id\" in data else key_id\n        data[\"key\"] = data[\"key\"] if \"key\" in data else key_type\n        data[\"value-id\"] = data[\"value-id\"] if \"value-id\" in data else value_id\n        data[\"value\"] = data[\"value\"] if \"value\" in data else value_type\n        data[\"value-required\"] = data[\"value-required\"] if \"value-required\" in data else value_required\n        super().__init__(**data)\n        self._hash = hash(self.__getnewargs__())\n\n    @cached_property\n    def key_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"key\",\n            field_id=self.key_id,\n            field_type=self.key_type,\n            required=True,\n        )\n\n    @cached_property\n    def value_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"value\",\n            field_id=self.value_id,\n            field_type=self.value_type,\n            required=self.value_required,\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the MapType class.\"\"\"\n        return f\"map&lt;{self.key_type}, {self.value_type}&gt;\"\n\n    def __getnewargs__(self) -&gt; Tuple[int, IcebergType, int, IcebergType, bool]:\n        \"\"\"Pickle the MapType class.\"\"\"\n        return (self.key_id, self.key_type, self.value_id, self.value_type, self.value_required)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the MapType.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the MapType to another object.\"\"\"\n        return (\n            self.key_field == other.key_field and self.value_field == other.value_field if isinstance(other, MapType) else False\n        )\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the MapType to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the MapType to another object.\"\"\"\n    return (\n        self.key_field == other.key_field and self.value_field == other.value_field if isinstance(other, MapType) else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the MapType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[int, IcebergType, int, IcebergType, bool]:\n    \"\"\"Pickle the MapType class.\"\"\"\n    return (self.key_id, self.key_type, self.value_id, self.value_type, self.value_required)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the MapType.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the MapType.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the MapType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the MapType class.\"\"\"\n    return f\"map&lt;{self.key_type}, {self.value_type}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField","title":"<code>NestedField</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>Represents a field of a struct, a map key, a map value, or a list element.</p> <p>This is where field IDs, names, docs, and nullability are tracked.</p> Example <p>str(NestedField( ...     field_id=1, ...     name='foo', ...     field_type=FixedType(22), ...     required=False, ... )) '1: foo: optional fixed[22]' str(NestedField( ...     field_id=2, ...     name='bar', ...     field_type=LongType(), ...     is_optional=False, ...     doc=\"Just a long\" ... )) '2: bar: required long (Just a long)'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class NestedField(IcebergType):\n    \"\"\"Represents a field of a struct, a map key, a map value, or a list element.\n\n    This is where field IDs, names, docs, and nullability are tracked.\n\n    Example:\n        &gt;&gt;&gt; str(NestedField(\n        ...     field_id=1,\n        ...     name='foo',\n        ...     field_type=FixedType(22),\n        ...     required=False,\n        ... ))\n        '1: foo: optional fixed[22]'\n        &gt;&gt;&gt; str(NestedField(\n        ...     field_id=2,\n        ...     name='bar',\n        ...     field_type=LongType(),\n        ...     is_optional=False,\n        ...     doc=\"Just a long\"\n        ... ))\n        '2: bar: required long (Just a long)'\n    \"\"\"\n\n    field_id: int = Field(alias=\"id\")\n    name: str = Field()\n    field_type: SerializeAsAny[IcebergType] = Field(alias=\"type\")\n    required: bool = Field(default=False)\n    doc: Optional[str] = Field(default=None, repr=False)\n    initial_default: Optional[Any] = Field(alias=\"initial-default\", default=None, repr=False)\n    write_default: Optional[L] = Field(alias=\"write-default\", default=None, repr=False)  # type: ignore\n\n    def __init__(\n        self,\n        field_id: Optional[int] = None,\n        name: Optional[str] = None,\n        field_type: Optional[IcebergType] = None,\n        required: bool = False,\n        doc: Optional[str] = None,\n        initial_default: Optional[Any] = None,\n        write_default: Optional[L] = None,\n        **data: Any,\n    ):\n        # We need an init when we want to use positional arguments, but\n        # need also to support the aliases.\n        data[\"id\"] = data[\"id\"] if \"id\" in data else field_id\n        data[\"name\"] = name\n        data[\"type\"] = data[\"type\"] if \"type\" in data else field_type\n        data[\"required\"] = required\n        data[\"doc\"] = doc\n        data[\"initial-default\"] = data[\"initial-default\"] if \"initial-default\" in data else initial_default\n        data[\"write-default\"] = data[\"write-default\"] if \"write-default\" in data else write_default\n        super().__init__(**data)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the NestedField class.\"\"\"\n        doc = \"\" if not self.doc else f\" ({self.doc})\"\n        req = \"required\" if self.required else \"optional\"\n        return f\"{self.field_id}: {self.name}: {req} {self.field_type}{doc}\"\n\n    def __getnewargs__(self) -&gt; Tuple[int, str, IcebergType, bool, Optional[str]]:\n        \"\"\"Pickle the NestedField class.\"\"\"\n        return (self.field_id, self.name, self.field_type, self.required, self.doc)\n\n    @property\n    def optional(self) -&gt; bool:\n        return not self.required\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the NestedField class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[int, str, IcebergType, bool, Optional[str]]:\n    \"\"\"Pickle the NestedField class.\"\"\"\n    return (self.field_id, self.name, self.field_type, self.required, self.doc)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the NestedField class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the NestedField class.\"\"\"\n    doc = \"\" if not self.doc else f\" ({self.doc})\"\n    req = \"required\" if self.required else \"optional\"\n    return f\"{self.field_id}: {self.name}: {req} {self.field_type}{doc}\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType","title":"<code>PrimitiveType</code>","text":"<p>               Bases: <code>Singleton</code>, <code>IcebergRootModel[str]</code>, <code>IcebergType</code></p> <p>Base class for all Iceberg Primitive Types.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class PrimitiveType(Singleton, IcebergRootModel[str], IcebergType):\n    \"\"\"Base class for all Iceberg Primitive Types.\"\"\"\n\n    root: Any = Field()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n        return f\"{type(self).__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n        return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the PrimitiveType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n    return f\"{type(self).__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the PrimitiveType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StringType","title":"<code>StringType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A String data type in Iceberg can be represented using an instance of this class.</p> <p>Strings in Iceberg are arbitrary-length character sequences and are encoded with UTF-8.</p> Example <p>column_foo = StringType() isinstance(column_foo, StringType) True column_foo StringType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class StringType(PrimitiveType):\n    \"\"\"A String data type in Iceberg can be represented using an instance of this class.\n\n    Strings in Iceberg are arbitrary-length character sequences and are encoded with UTF-8.\n\n    Example:\n        &gt;&gt;&gt; column_foo = StringType()\n        &gt;&gt;&gt; isinstance(column_foo, StringType)\n        True\n        &gt;&gt;&gt; column_foo\n        StringType()\n    \"\"\"\n\n    root: Literal[\"string\"] = Field(default=\"string\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType","title":"<code>StructType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A struct type in Iceberg.</p> Example <p>str(StructType( ...     NestedField(1, \"required_field\", StringType(), True), ...     NestedField(2, \"optional_field\", IntegerType()) ... )) 'struct&lt;1: required_field: optional string, 2: optional_field: optional int&gt;'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class StructType(IcebergType):\n    \"\"\"A struct type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; str(StructType(\n        ...     NestedField(1, \"required_field\", StringType(), True),\n        ...     NestedField(2, \"optional_field\", IntegerType())\n        ... ))\n        'struct&lt;1: required_field: optional string, 2: optional_field: optional int&gt;'\n    \"\"\"\n\n    type: Literal[\"struct\"] = Field(default=\"struct\")\n    fields: Tuple[NestedField, ...] = Field(default_factory=tuple)\n    _hash: int = PrivateAttr()\n\n    def __init__(self, *fields: NestedField, **data: Any):\n        # In case we use positional arguments, instead of keyword args\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n        self._hash = hash(self.fields)\n\n    def field(self, field_id: int) -&gt; Optional[NestedField]:\n        for field in self.fields:\n            if field.field_id == field_id:\n                return field\n        return None\n\n    def field_by_name(self, name: str, case_sensitive: bool = True) -&gt; Optional[NestedField]:\n        if case_sensitive:\n            for field in self.fields:\n                if field.name == name:\n                    return field\n        else:\n            name_lower = name.lower()\n            for field in self.fields:\n                if field.name.lower() == name_lower:\n                    return field\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructType class.\"\"\"\n        return f\"struct&lt;{', '.join(map(str, self.fields))}&gt;\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructType class.\"\"\"\n        return f\"StructType(fields=({', '.join(map(repr, self.fields))},))\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the StructType class.\"\"\"\n        return len(self.fields)\n\n    def __getnewargs__(self) -&gt; Tuple[NestedField, ...]:\n        \"\"\"Pickle the StructType class.\"\"\"\n        return self.fields\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Use the cache hash value of the StructType class.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the object if it is equal to another object.\"\"\"\n        return self.fields == other.fields if isinstance(other, StructType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the object if it is equal to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the object if it is equal to another object.\"\"\"\n    return self.fields == other.fields if isinstance(other, StructType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[NestedField, ...]:\n    \"\"\"Pickle the StructType class.\"\"\"\n    return self.fields\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__hash__","title":"<code>__hash__()</code>","text":"<p>Use the cache hash value of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Use the cache hash value of the StructType class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the StructType class.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructType class.\"\"\"\n    return f\"StructType(fields=({', '.join(map(repr, self.fields))},))\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructType class.\"\"\"\n    return f\"struct&lt;{', '.join(map(str, self.fields))}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimeType","title":"<code>TimeType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Time data type in Iceberg can be represented using an instance of this class.</p> <p>Times in Iceberg have microsecond precision and are a time of day without a date or timezone.</p> Example <p>column_foo = TimeType() isinstance(column_foo, TimeType) True column_foo TimeType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimeType(PrimitiveType):\n    \"\"\"A Time data type in Iceberg can be represented using an instance of this class.\n\n    Times in Iceberg have microsecond precision and are a time of day without a date or timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimeType()\n        &gt;&gt;&gt; isinstance(column_foo, TimeType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimeType()\n    \"\"\"\n\n    root: Literal[\"time\"] = Field(default=\"time\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestampType","title":"<code>TimestampType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Timestamp data type in Iceberg can be represented using an instance of this class.</p> <p>Timestamps in Iceberg have microsecond precision and include a date and a time of day without a timezone.</p> Example <p>column_foo = TimestampType() isinstance(column_foo, TimestampType) True column_foo TimestampType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestampType(PrimitiveType):\n    \"\"\"A Timestamp data type in Iceberg can be represented using an instance of this class.\n\n    Timestamps in Iceberg have microsecond precision and include a date and a time of day without a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestampType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestampType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestampType()\n    \"\"\"\n\n    root: Literal[\"timestamp\"] = Field(default=\"timestamp\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestamptzType","title":"<code>TimestamptzType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Timestamptz data type in Iceberg can be represented using an instance of this class.</p> <p>Timestamptzs in Iceberg are stored as UTC and include a date and a time of day with a timezone.</p> Example <p>column_foo = TimestamptzType() isinstance(column_foo, TimestamptzType) True column_foo TimestamptzType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestamptzType(PrimitiveType):\n    \"\"\"A Timestamptz data type in Iceberg can be represented using an instance of this class.\n\n    Timestamptzs in Iceberg are stored as UTC and include a date and a time of day with a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestamptzType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestamptzType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestamptzType()\n    \"\"\"\n\n    root: Literal[\"timestamptz\"] = Field(default=\"timestamptz\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.UUIDType","title":"<code>UUIDType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A UUID data type in Iceberg can be represented using an instance of this class.</p> <p>UUIDs in Iceberg are universally unique identifiers.</p> Example <p>column_foo = UUIDType() isinstance(column_foo, UUIDType) True column_foo UUIDType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class UUIDType(PrimitiveType):\n    \"\"\"A UUID data type in Iceberg can be represented using an instance of this class.\n\n    UUIDs in Iceberg are universally unique identifiers.\n\n    Example:\n        &gt;&gt;&gt; column_foo = UUIDType()\n        &gt;&gt;&gt; isinstance(column_foo, UUIDType)\n        True\n        &gt;&gt;&gt; column_foo\n        UUIDType()\n    \"\"\"\n\n    root: Literal[\"uuid\"] = Field(default=\"uuid\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.strtobool","title":"<code>strtobool(val)</code>","text":"<p>Convert a string representation of truth to true (1) or false (0).</p> <p>True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if 'val' is anything else.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def strtobool(val: str) -&gt; bool:\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    else:\n        raise ValueError(f\"Invalid truth value: {val!r}\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.transform_dict_value_to_str","title":"<code>transform_dict_value_to_str(dict)</code>","text":"<p>Transform all values in the dictionary to string. Raise an error if any value is None.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def transform_dict_value_to_str(dict: Dict[str, Any]) -&gt; Dict[str, str]:\n    \"\"\"Transform all values in the dictionary to string. Raise an error if any value is None.\"\"\"\n    for key, value in dict.items():\n        if value is None:\n            raise ValueError(f\"None type is not a supported value in properties: {key}\")\n    return {k: str(v).lower() if isinstance(v, bool) else str(v) for k, v in dict.items()}\n</code></pre>"},{"location":"reference/pyiceberg/avro/","title":"avro","text":""},{"location":"reference/pyiceberg/avro/decoder/","title":"decoder","text":""},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder","title":"<code>BinaryDecoder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Decodes bytes into Python physical primitives.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>class BinaryDecoder(ABC):\n    \"\"\"Decodes bytes into Python physical primitives.\"\"\"\n\n    @abstractmethod\n    def tell(self) -&gt; int:\n        \"\"\"Return the current position.\"\"\"\n\n    @abstractmethod\n    def read(self, n: int) -&gt; bytes:\n        \"\"\"Read n bytes.\"\"\"\n\n    @abstractmethod\n    def skip(self, n: int) -&gt; None:\n        \"\"\"Skip n bytes.\"\"\"\n\n    def read_boolean(self) -&gt; bool:\n        \"\"\"Read a value from the stream as a boolean.\n\n        A boolean is written as a single byte\n        whose value is either 0 (false) or 1 (true).\n        \"\"\"\n        return ord(self.read(1)) == 1\n\n    def read_int(self) -&gt; int:\n        \"\"\"Read an int/long value.\n\n        int/long values are written using variable-length, zigzag coding.\n        \"\"\"\n        b = ord(self.read(1))\n        n = b &amp; 0x7F\n        shift = 7\n        while (b &amp; 0x80) != 0:\n            b = ord(self.read(1))\n            n |= (b &amp; 0x7F) &lt;&lt; shift\n            shift += 7\n        datum = (n &gt;&gt; 1) ^ -(n &amp; 1)\n        return datum\n\n    def read_ints(self, n: int) -&gt; Tuple[int, ...]:\n        \"\"\"Read a list of integers.\"\"\"\n        return tuple(self.read_int() for _ in range(n))\n\n    def read_int_bytes_dict(self, n: int, dest: Dict[int, bytes]) -&gt; None:\n        \"\"\"Read a dictionary of integers for keys and bytes for values into a destination dictionary.\"\"\"\n        for _ in range(n):\n            k = self.read_int()\n            v = self.read_bytes()\n            dest[k] = v\n\n    def read_float(self) -&gt; float:\n        \"\"\"Read a value from the stream as a float.\n\n        A float is written as 4 bytes.\n        The float is converted into a 32-bit integer using a method equivalent to\n        Java's floatToIntBits and then encoded in little-endian format.\n        \"\"\"\n        return float(cast(Tuple[float, ...], STRUCT_FLOAT.unpack(self.read(4)))[0])\n\n    def read_double(self) -&gt; float:\n        \"\"\"Read a value from the stream as a double.\n\n        A double is written as 8 bytes.\n        The double is converted into a 64-bit integer using a method equivalent to\n        Java's doubleToLongBits and then encoded in little-endian format.\n        \"\"\"\n        return float(cast(Tuple[float, ...], STRUCT_DOUBLE.unpack(self.read(8)))[0])\n\n    def read_bytes(self) -&gt; bytes:\n        \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n        num_bytes = self.read_int()\n        return self.read(num_bytes) if num_bytes &gt; 0 else b\"\"\n\n    def read_utf8(self) -&gt; str:\n        \"\"\"Read an utf-8 encoded string from the stream.\n\n        A string is encoded as a long followed by\n        that many bytes of UTF-8 encoded character data.\n        \"\"\"\n        return self.read_bytes().decode(UTF8)\n\n    def skip_boolean(self) -&gt; None:\n        self.skip(1)\n\n    def skip_int(self) -&gt; None:\n        b = ord(self.read(1))\n        while (b &amp; 0x80) != 0:\n            b = ord(self.read(1))\n\n    def skip_float(self) -&gt; None:\n        self.skip(4)\n\n    def skip_double(self) -&gt; None:\n        self.skip(8)\n\n    def skip_bytes(self) -&gt; None:\n        self.skip(self.read_int())\n\n    def skip_utf8(self) -&gt; None:\n        self.skip_bytes()\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read","title":"<code>read(n)</code>  <code>abstractmethod</code>","text":"<p>Read n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef read(self, n: int) -&gt; bytes:\n    \"\"\"Read n bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_boolean","title":"<code>read_boolean()</code>","text":"<p>Read a value from the stream as a boolean.</p> <p>A boolean is written as a single byte whose value is either 0 (false) or 1 (true).</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_boolean(self) -&gt; bool:\n    \"\"\"Read a value from the stream as a boolean.\n\n    A boolean is written as a single byte\n    whose value is either 0 (false) or 1 (true).\n    \"\"\"\n    return ord(self.read(1)) == 1\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_bytes","title":"<code>read_bytes()</code>","text":"<p>Bytes are encoded as a long followed by that many bytes of data.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_bytes(self) -&gt; bytes:\n    \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n    num_bytes = self.read_int()\n    return self.read(num_bytes) if num_bytes &gt; 0 else b\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_double","title":"<code>read_double()</code>","text":"<p>Read a value from the stream as a double.</p> <p>A double is written as 8 bytes. The double is converted into a 64-bit integer using a method equivalent to Java's doubleToLongBits and then encoded in little-endian format.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_double(self) -&gt; float:\n    \"\"\"Read a value from the stream as a double.\n\n    A double is written as 8 bytes.\n    The double is converted into a 64-bit integer using a method equivalent to\n    Java's doubleToLongBits and then encoded in little-endian format.\n    \"\"\"\n    return float(cast(Tuple[float, ...], STRUCT_DOUBLE.unpack(self.read(8)))[0])\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_float","title":"<code>read_float()</code>","text":"<p>Read a value from the stream as a float.</p> <p>A float is written as 4 bytes. The float is converted into a 32-bit integer using a method equivalent to Java's floatToIntBits and then encoded in little-endian format.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_float(self) -&gt; float:\n    \"\"\"Read a value from the stream as a float.\n\n    A float is written as 4 bytes.\n    The float is converted into a 32-bit integer using a method equivalent to\n    Java's floatToIntBits and then encoded in little-endian format.\n    \"\"\"\n    return float(cast(Tuple[float, ...], STRUCT_FLOAT.unpack(self.read(4)))[0])\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_int","title":"<code>read_int()</code>","text":"<p>Read an int/long value.</p> <p>int/long values are written using variable-length, zigzag coding.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_int(self) -&gt; int:\n    \"\"\"Read an int/long value.\n\n    int/long values are written using variable-length, zigzag coding.\n    \"\"\"\n    b = ord(self.read(1))\n    n = b &amp; 0x7F\n    shift = 7\n    while (b &amp; 0x80) != 0:\n        b = ord(self.read(1))\n        n |= (b &amp; 0x7F) &lt;&lt; shift\n        shift += 7\n    datum = (n &gt;&gt; 1) ^ -(n &amp; 1)\n    return datum\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_int_bytes_dict","title":"<code>read_int_bytes_dict(n, dest)</code>","text":"<p>Read a dictionary of integers for keys and bytes for values into a destination dictionary.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_int_bytes_dict(self, n: int, dest: Dict[int, bytes]) -&gt; None:\n    \"\"\"Read a dictionary of integers for keys and bytes for values into a destination dictionary.\"\"\"\n    for _ in range(n):\n        k = self.read_int()\n        v = self.read_bytes()\n        dest[k] = v\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_ints","title":"<code>read_ints(n)</code>","text":"<p>Read a list of integers.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_ints(self, n: int) -&gt; Tuple[int, ...]:\n    \"\"\"Read a list of integers.\"\"\"\n    return tuple(self.read_int() for _ in range(n))\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_utf8","title":"<code>read_utf8()</code>","text":"<p>Read an utf-8 encoded string from the stream.</p> <p>A string is encoded as a long followed by that many bytes of UTF-8 encoded character data.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_utf8(self) -&gt; str:\n    \"\"\"Read an utf-8 encoded string from the stream.\n\n    A string is encoded as a long followed by\n    that many bytes of UTF-8 encoded character data.\n    \"\"\"\n    return self.read_bytes().decode(UTF8)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.skip","title":"<code>skip(n)</code>  <code>abstractmethod</code>","text":"<p>Skip n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef skip(self, n: int) -&gt; None:\n    \"\"\"Skip n bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.tell","title":"<code>tell()</code>  <code>abstractmethod</code>","text":"<p>Return the current position.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef tell(self) -&gt; int:\n    \"\"\"Return the current position.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder","title":"<code>StreamingBinaryDecoder</code>","text":"<p>               Bases: <code>BinaryDecoder</code></p> <p>Decodes bytes into Python physical primitives.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>class StreamingBinaryDecoder(BinaryDecoder):\n    \"\"\"Decodes bytes into Python physical primitives.\"\"\"\n\n    __slots__ = \"_input_stream\"\n    _input_stream: InputStream\n\n    def __init__(self, input_stream: Union[bytes, InputStream]) -&gt; None:\n        \"\"\"Reader is a Python object on which we can call read, seek, and tell.\"\"\"\n        if isinstance(input_stream, bytes):\n            # In the case of bytes, we wrap it into a BytesIO to make it a stream\n            self._input_stream = io.BytesIO(input_stream)\n        else:\n            self._input_stream = input_stream\n\n    def tell(self) -&gt; int:\n        \"\"\"Return the current stream position.\"\"\"\n        return self._input_stream.tell()\n\n    def read(self, n: int) -&gt; bytes:\n        \"\"\"Read n bytes.\"\"\"\n        if n &lt; 0:\n            raise ValueError(f\"Requested {n} bytes to read, expected positive integer.\")\n        data: List[bytes] = []\n\n        n_remaining = n\n        while n_remaining &gt; 0:\n            data_read = self._input_stream.read(n_remaining)\n            read_len = len(data_read)\n            if read_len == n:\n                # If we read everything, we return directly\n                # otherwise we'll continue to fetch the rest\n                return data_read\n            elif read_len &lt;= 0:\n                raise EOFError(f\"EOF: read {read_len} bytes\")\n            data.append(data_read)\n            n_remaining -= read_len\n\n        return b\"\".join(data)\n\n    def skip(self, n: int) -&gt; None:\n        self._input_stream.seek(n, SEEK_CUR)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.__init__","title":"<code>__init__(input_stream)</code>","text":"<p>Reader is a Python object on which we can call read, seek, and tell.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def __init__(self, input_stream: Union[bytes, InputStream]) -&gt; None:\n    \"\"\"Reader is a Python object on which we can call read, seek, and tell.\"\"\"\n    if isinstance(input_stream, bytes):\n        # In the case of bytes, we wrap it into a BytesIO to make it a stream\n        self._input_stream = io.BytesIO(input_stream)\n    else:\n        self._input_stream = input_stream\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.read","title":"<code>read(n)</code>","text":"<p>Read n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read(self, n: int) -&gt; bytes:\n    \"\"\"Read n bytes.\"\"\"\n    if n &lt; 0:\n        raise ValueError(f\"Requested {n} bytes to read, expected positive integer.\")\n    data: List[bytes] = []\n\n    n_remaining = n\n    while n_remaining &gt; 0:\n        data_read = self._input_stream.read(n_remaining)\n        read_len = len(data_read)\n        if read_len == n:\n            # If we read everything, we return directly\n            # otherwise we'll continue to fetch the rest\n            return data_read\n        elif read_len &lt;= 0:\n            raise EOFError(f\"EOF: read {read_len} bytes\")\n        data.append(data_read)\n        n_remaining -= read_len\n\n    return b\"\".join(data)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.tell","title":"<code>tell()</code>","text":"<p>Return the current stream position.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def tell(self) -&gt; int:\n    \"\"\"Return the current stream position.\"\"\"\n    return self._input_stream.tell()\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/","title":"encoder","text":""},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder","title":"<code>BinaryEncoder</code>","text":"<p>Encodes Python physical types into bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>class BinaryEncoder:\n    \"\"\"Encodes Python physical types into bytes.\"\"\"\n\n    _output_stream: OutputStream\n\n    def __init__(self, output_stream: OutputStream) -&gt; None:\n        self._output_stream = output_stream\n\n    def write(self, b: bytes) -&gt; None:\n        self._output_stream.write(b)\n\n    def write_boolean(self, boolean: bool) -&gt; None:\n        \"\"\"Write a boolean as a single byte whose value is either 0 (false) or 1 (true).\n\n        Args:\n            boolean: The boolean to write.\n        \"\"\"\n        self.write(bytearray([bool(boolean)]))\n\n    def write_int(self, integer: int) -&gt; None:\n        \"\"\"Integer and long values are written using variable-length zig-zag coding.\"\"\"\n        datum = (integer &lt;&lt; 1) ^ (integer &gt;&gt; 63)\n        while (datum &amp; ~0x7F) != 0:\n            self.write(bytearray([(datum &amp; 0x7F) | 0x80]))\n            datum &gt;&gt;= 7\n        self.write(bytearray([datum]))\n\n    def write_float(self, f: float) -&gt; None:\n        \"\"\"Write a float as 4 bytes.\"\"\"\n        self.write(STRUCT_FLOAT.pack(f))\n\n    def write_double(self, f: float) -&gt; None:\n        \"\"\"Write a double as 8 bytes.\"\"\"\n        self.write(STRUCT_DOUBLE.pack(f))\n\n    def write_bytes(self, b: bytes) -&gt; None:\n        \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n        self.write_int(len(b))\n        self.write(b)\n\n    def write_utf8(self, s: str) -&gt; None:\n        \"\"\"Encode a string as a long followed by that many bytes of UTF-8 encoded character data.\"\"\"\n        self.write_bytes(s.encode(UTF8))\n\n    def write_uuid(self, uuid: UUID) -&gt; None:\n        \"\"\"Write UUID as a fixed[16].\n\n        The uuid logical type represents a random generated universally unique identifier (UUID).\n        An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.\n        \"\"\"\n        if len(uuid.bytes) != 16:\n            raise ValueError(f\"Expected UUID to have 16 bytes, got: len({uuid.bytes!r})\")\n        return self.write(uuid.bytes)\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_boolean","title":"<code>write_boolean(boolean)</code>","text":"<p>Write a boolean as a single byte whose value is either 0 (false) or 1 (true).</p> <p>Parameters:</p> Name Type Description Default <code>boolean</code> <code>bool</code> <p>The boolean to write.</p> required Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_boolean(self, boolean: bool) -&gt; None:\n    \"\"\"Write a boolean as a single byte whose value is either 0 (false) or 1 (true).\n\n    Args:\n        boolean: The boolean to write.\n    \"\"\"\n    self.write(bytearray([bool(boolean)]))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_bytes","title":"<code>write_bytes(b)</code>","text":"<p>Bytes are encoded as a long followed by that many bytes of data.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_bytes(self, b: bytes) -&gt; None:\n    \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n    self.write_int(len(b))\n    self.write(b)\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_double","title":"<code>write_double(f)</code>","text":"<p>Write a double as 8 bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_double(self, f: float) -&gt; None:\n    \"\"\"Write a double as 8 bytes.\"\"\"\n    self.write(STRUCT_DOUBLE.pack(f))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_float","title":"<code>write_float(f)</code>","text":"<p>Write a float as 4 bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_float(self, f: float) -&gt; None:\n    \"\"\"Write a float as 4 bytes.\"\"\"\n    self.write(STRUCT_FLOAT.pack(f))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_int","title":"<code>write_int(integer)</code>","text":"<p>Integer and long values are written using variable-length zig-zag coding.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_int(self, integer: int) -&gt; None:\n    \"\"\"Integer and long values are written using variable-length zig-zag coding.\"\"\"\n    datum = (integer &lt;&lt; 1) ^ (integer &gt;&gt; 63)\n    while (datum &amp; ~0x7F) != 0:\n        self.write(bytearray([(datum &amp; 0x7F) | 0x80]))\n        datum &gt;&gt;= 7\n    self.write(bytearray([datum]))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_utf8","title":"<code>write_utf8(s)</code>","text":"<p>Encode a string as a long followed by that many bytes of UTF-8 encoded character data.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_utf8(self, s: str) -&gt; None:\n    \"\"\"Encode a string as a long followed by that many bytes of UTF-8 encoded character data.\"\"\"\n    self.write_bytes(s.encode(UTF8))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_uuid","title":"<code>write_uuid(uuid)</code>","text":"<p>Write UUID as a fixed[16].</p> <p>The uuid logical type represents a random generated universally unique identifier (UUID). An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_uuid(self, uuid: UUID) -&gt; None:\n    \"\"\"Write UUID as a fixed[16].\n\n    The uuid logical type represents a random generated universally unique identifier (UUID).\n    An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.\n    \"\"\"\n    if len(uuid.bytes) != 16:\n        raise ValueError(f\"Expected UUID to have 16 bytes, got: len({uuid.bytes!r})\")\n    return self.write(uuid.bytes)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/","title":"file","text":"<p>Avro reader for reading Avro files.</p>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile","title":"<code>AvroFile</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroFile(Generic[D]):\n    __slots__ = (\n        \"input_file\",\n        \"read_schema\",\n        \"read_types\",\n        \"read_enums\",\n        \"header\",\n        \"schema\",\n        \"reader\",\n        \"decoder\",\n        \"block\",\n    )\n    input_file: InputFile\n    read_schema: Optional[Schema]\n    read_types: Dict[int, Callable[..., StructProtocol]]\n    read_enums: Dict[int, Callable[..., Enum]]\n    header: AvroFileHeader\n    schema: Schema\n    reader: Reader\n\n    decoder: BinaryDecoder\n    block: Optional[Block[D]]\n\n    def __init__(\n        self,\n        input_file: InputFile,\n        read_schema: Optional[Schema] = None,\n        read_types: Dict[int, Callable[..., StructProtocol]] = EMPTY_DICT,\n        read_enums: Dict[int, Callable[..., Enum]] = EMPTY_DICT,\n    ) -&gt; None:\n        self.input_file = input_file\n        self.read_schema = read_schema\n        self.read_types = read_types\n        self.read_enums = read_enums\n        self.block = None\n\n    def __enter__(self) -&gt; AvroFile[D]:\n        \"\"\"Generate a reader tree for the payload within an avro file.\n\n        Return:\n            A generator returning the AvroStructs.\n        \"\"\"\n        with self.input_file.open() as f:\n            self.decoder = new_decoder(f.read())\n        self.header = self._read_header()\n        self.schema = self.header.get_schema()\n        if not self.read_schema:\n            self.read_schema = self.schema\n\n        self.reader = resolve_reader(self.schema, self.read_schema, self.read_types, self.read_enums)\n\n        return self\n\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n\n    def __iter__(self) -&gt; AvroFile[D]:\n        \"\"\"Return an iterator for the AvroFile class.\"\"\"\n        return self\n\n    def _read_block(self) -&gt; int:\n        # If there is already a block, we'll have the sync bytes\n        if self.block:\n            sync_marker = self.decoder.read(SYNC_SIZE)\n            if sync_marker != self.header.sync:\n                raise ValueError(f\"Expected sync bytes {self.header.sync!r}, but got {sync_marker!r}\")\n        block_records = self.decoder.read_int()\n\n        block_bytes = self.decoder.read_bytes()\n        if codec := self.header.compression_codec():\n            block_bytes = codec.decompress(block_bytes)\n\n        self.block = Block(reader=self.reader, block_records=block_records, block_decoder=new_decoder(block_bytes))\n        return block_records\n\n    def __next__(self) -&gt; D:\n        \"\"\"Return the next item when iterating over the AvroFile class.\"\"\"\n        if self.block and self.block.has_next():\n            return next(self.block)\n\n        try:\n            new_block = self._read_block()\n        except EOFError as exc:\n            raise StopIteration from exc\n\n        if new_block &gt; 0:\n            return self.__next__()\n        raise StopIteration\n\n    def _read_header(self) -&gt; AvroFileHeader:\n        return construct_reader(META_SCHEMA, {-1: AvroFileHeader}).read(self.decoder)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__enter__","title":"<code>__enter__()</code>","text":"<p>Generate a reader tree for the payload within an avro file.</p> Return <p>A generator returning the AvroStructs.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __enter__(self) -&gt; AvroFile[D]:\n    \"\"\"Generate a reader tree for the payload within an avro file.\n\n    Return:\n        A generator returning the AvroStructs.\n    \"\"\"\n    with self.input_file.open() as f:\n        self.decoder = new_decoder(f.read())\n    self.header = self._read_header()\n    self.schema = self.header.get_schema()\n    if not self.read_schema:\n        self.read_schema = self.schema\n\n    self.reader = resolve_reader(self.schema, self.read_schema, self.read_types, self.read_enums)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __exit__(\n    self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the AvroFile class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __iter__(self) -&gt; AvroFile[D]:\n    \"\"\"Return an iterator for the AvroFile class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the AvroFile class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __next__(self) -&gt; D:\n    \"\"\"Return the next item when iterating over the AvroFile class.\"\"\"\n    if self.block and self.block.has_next():\n        return next(self.block)\n\n    try:\n        new_block = self._read_block()\n    except EOFError as exc:\n        raise StopIteration from exc\n\n    if new_block &gt; 0:\n        return self.__next__()\n    raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFileHeader","title":"<code>AvroFileHeader</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroFileHeader(Record):\n    __slots__ = (\"magic\", \"meta\", \"sync\")\n    magic: bytes\n    meta: Dict[str, str]\n    sync: bytes\n\n    def compression_codec(self) -&gt; Optional[Type[Codec]]:\n        \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n        In the case of a null codec, we return a None indicating that we\n        don't need to compress/decompress.\n        \"\"\"\n        codec_name = self.meta.get(_CODEC_KEY, \"null\")\n        if codec_name not in KNOWN_CODECS:\n            raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n        return KNOWN_CODECS[codec_name]\n\n    def get_schema(self) -&gt; Schema:\n        if _SCHEMA_KEY in self.meta:\n            avro_schema_string = self.meta[_SCHEMA_KEY]\n            avro_schema = json.loads(avro_schema_string)\n            return AvroSchemaConversion().avro_to_iceberg(avro_schema)\n        else:\n            raise ValueError(\"No schema found in Avro file headers\")\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFileHeader.compression_codec","title":"<code>compression_codec()</code>","text":"<p>Get the file's compression codec algorithm from the file's metadata.</p> <p>In the case of a null codec, we return a None indicating that we don't need to compress/decompress.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def compression_codec(self) -&gt; Optional[Type[Codec]]:\n    \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n    In the case of a null codec, we return a None indicating that we\n    don't need to compress/decompress.\n    \"\"\"\n    codec_name = self.meta.get(_CODEC_KEY, \"null\")\n    if codec_name not in KNOWN_CODECS:\n        raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n    return KNOWN_CODECS[codec_name]\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile","title":"<code>AvroOutputFile</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroOutputFile(Generic[D]):\n    output_file: OutputFile\n    output_stream: OutputStream\n    file_schema: Schema\n    schema_name: str\n    encoder: BinaryEncoder\n    sync_bytes: bytes\n    writer: Writer\n\n    def __init__(\n        self,\n        output_file: OutputFile,\n        file_schema: Schema,\n        schema_name: str,\n        record_schema: Optional[Schema] = None,\n        metadata: Dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        self.output_file = output_file\n        self.file_schema = file_schema\n        self.schema_name = schema_name\n        self.sync_bytes = os.urandom(SYNC_SIZE)\n        self.writer = (\n            construct_writer(file_schema=self.file_schema)\n            if record_schema is None\n            else resolve_writer(record_schema=record_schema, file_schema=self.file_schema)\n        )\n        self.metadata = metadata\n\n    def __enter__(self) -&gt; AvroOutputFile[D]:\n        \"\"\"\n        Open the file and writes the header.\n\n        Returns:\n            The file object to write records to\n        \"\"\"\n        self.output_stream = self.output_file.create(overwrite=True)\n        self.encoder = BinaryEncoder(self.output_stream)\n\n        self._write_header()\n\n        return self\n\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n        self.output_stream.close()\n\n    def _write_header(self) -&gt; None:\n        json_schema = json.dumps(AvroSchemaConversion().iceberg_to_avro(self.file_schema, schema_name=self.schema_name))\n        meta = {**self.metadata, _SCHEMA_KEY: json_schema, _CODEC_KEY: \"null\"}\n        header = AvroFileHeader(magic=MAGIC, meta=meta, sync=self.sync_bytes)\n        construct_writer(META_SCHEMA).write(self.encoder, header)\n\n    def write_block(self, objects: List[D]) -&gt; None:\n        in_memory = io.BytesIO()\n        block_content_encoder = BinaryEncoder(output_stream=in_memory)\n        for obj in objects:\n            self.writer.write(block_content_encoder, obj)\n        block_content = in_memory.getvalue()\n\n        self.encoder.write_int(len(objects))\n        self.encoder.write_int(len(block_content))\n        self.encoder.write(block_content)\n        self.encoder.write(self.sync_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the file and writes the header.</p> <p>Returns:</p> Type Description <code>AvroOutputFile[D]</code> <p>The file object to write records to</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __enter__(self) -&gt; AvroOutputFile[D]:\n    \"\"\"\n    Open the file and writes the header.\n\n    Returns:\n        The file object to write records to\n    \"\"\"\n    self.output_stream = self.output_file.create(overwrite=True)\n    self.encoder = BinaryEncoder(self.output_stream)\n\n    self._write_header()\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __exit__(\n    self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n    self.output_stream.close()\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block","title":"<code>Block</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>@dataclass\nclass Block(Generic[D]):\n    reader: Reader\n    block_records: int\n    block_decoder: BinaryDecoder\n    position: int = 0\n\n    def __iter__(self) -&gt; Block[D]:\n        \"\"\"Return an iterator for the Block class.\"\"\"\n        return self\n\n    def has_next(self) -&gt; bool:\n        return self.position &lt; self.block_records\n\n    def __next__(self) -&gt; D:\n        \"\"\"Return the next item when iterating over the Block class.\"\"\"\n        if self.has_next():\n            self.position += 1\n            return self.reader.read(self.block_decoder)\n        raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the Block class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __iter__(self) -&gt; Block[D]:\n    \"\"\"Return an iterator for the Block class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the Block class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __next__(self) -&gt; D:\n    \"\"\"Return the next item when iterating over the Block class.\"\"\"\n    if self.has_next():\n        self.position += 1\n        return self.reader.read(self.block_decoder)\n    raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/","title":"reader","text":"<p>Classes for building the Reader tree.</p> <p>Constructing a reader tree from the schema makes it easy to decouple the reader implementation from the schema.</p> <p>The reader tree can be changed in such a way that the read schema is different, while respecting the read schema.</p>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.BinaryReader","title":"<code>BinaryReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Read a binary value.</p> <p>First reads an integer, to get the length of the binary value, then reads the binary field itself.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class BinaryReader(Reader):\n    \"\"\"Read a binary value.\n\n    First reads an integer, to get the length of the binary value,\n    then reads the binary field itself.\n    \"\"\"\n\n    def read(self, decoder: BinaryDecoder) -&gt; bytes:\n        return decoder.read_bytes()\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_bytes()\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DateReader","title":"<code>DateReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a day granularity date from the stream.</p> <p>The number of days from 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class DateReader(IntegerReader):\n    \"\"\"Reads a day granularity date from the stream.\n\n    The number of days from 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DecimalReader","title":"<code>DecimalReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Reads a value as a decimal.</p> <p>Decimal bytes are decoded as signed short, int or long depending on the size of bytes.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=True, init=False)\nclass DecimalReader(Reader):\n    \"\"\"Reads a value as a decimal.\n\n    Decimal bytes are decoded as signed short, int or long depending on the\n    size of bytes.\n    \"\"\"\n\n    precision: int = dataclassfield()\n    scale: int = dataclassfield()\n    _length: int\n\n    def __init__(self, precision: int, scale: int):\n        object.__setattr__(self, \"precision\", precision)\n        object.__setattr__(self, \"scale\", scale)\n        object.__setattr__(self, \"_length\", decimal_required_bytes(precision))\n\n    def read(self, decoder: BinaryDecoder) -&gt; Decimal:\n        return bytes_to_decimal(decoder.read(self._length), self.scale)\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_bytes()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DecimalReader class.\"\"\"\n        return f\"DecimalReader({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DecimalReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DecimalReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DecimalReader class.\"\"\"\n    return f\"DecimalReader({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader","title":"<code>FixedReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=True)\nclass FixedReader(Reader):\n    _len: int = dataclassfield()\n\n    def read(self, decoder: BinaryDecoder) -&gt; bytes:\n        return decoder.read(len(self))\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip(len(self))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the FixedReader class.\"\"\"\n        return self._len\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FixedReader class.\"\"\"\n        return f\"FixedReader({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the FixedReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the FixedReader class.\"\"\"\n    return self._len\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FixedReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FixedReader class.\"\"\"\n    return f\"FixedReader({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.IntegerReader","title":"<code>IntegerReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Longs and ints are encoded the same way, and there is no long in Python.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class IntegerReader(Reader):\n    \"\"\"Longs and ints are encoded the same way, and there is no long in Python.\"\"\"\n\n    def read(self, decoder: BinaryDecoder) -&gt; int:\n        return decoder.read_int()\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_int()\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.ListReader","title":"<code>ListReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=False, init=False)\nclass ListReader(Reader):\n    __slots__ = (\"element\", \"_is_int_list\", \"_hash\")\n    element: Reader\n\n    def __init__(self, element: Reader) -&gt; None:\n        super().__init__()\n        self.element = element\n        self._hash = hash(self.element)\n        self._is_int_list = isinstance(self.element, IntegerReader)\n\n    def read(self, decoder: BinaryDecoder) -&gt; List[Any]:\n        read_items: List[Any] = []\n        block_count = decoder.read_int()\n        while block_count != 0:\n            if block_count &lt; 0:\n                block_count = -block_count\n                _ = decoder.read_int()\n            if self._is_int_list:\n                read_items.extend(decoder.read_ints(block_count))\n            else:\n                for _ in range(block_count):\n                    read_items.append(self.element.read(decoder))\n            block_count = decoder.read_int()\n        return read_items\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        _skip_map_array(decoder, lambda: self.element.skip(decoder))\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the ListReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.ListReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the ListReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the ListReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.MapReader","title":"<code>MapReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=False, init=False)\nclass MapReader(Reader):\n    __slots__ = (\"key\", \"value\", \"_is_int_int\", \"_is_int_bytes\", \"_key_reader\", \"_value_reader\", \"_hash\")\n    key: Reader\n    value: Reader\n\n    def __init__(self, key: Reader, value: Reader) -&gt; None:\n        super().__init__()\n        self.key = key\n        self.value = value\n        if isinstance(self.key, IntegerReader):\n            self._is_int_int = isinstance(self.value, IntegerReader)\n            self._is_int_bytes = isinstance(self.value, BinaryReader)\n        else:\n            self._is_int_int = False\n            self._is_int_bytes = False\n            self._key_reader = self.key.read\n            self._value_reader = self.value.read\n        self._hash = hash((self.key, self.value))\n\n    def _read_int_int(self, decoder: BinaryDecoder) -&gt; Mapping[int, int]:\n        \"\"\"Read a mapping from int to int from the decoder.\n\n        Read a map of ints to ints from the decoder, since this is such a common\n        data type, it is optimized to be faster than the generic map reader, by\n        using a lazy dict.\n\n        The time it takes to create the python dictionary is much larger than\n        the time it takes to read the data from the decoder as an array, so the\n        lazy dict defers creating the python dictionary until it is actually\n        accessed.\n\n        \"\"\"\n        block_count = decoder.read_int()\n\n        # Often times the map is empty, so we can just return an empty dict without\n        # instancing the LazyDict\n        if block_count == 0:\n            return EMPTY_DICT\n\n        contents_array: List[Tuple[int, ...]] = []\n\n        while block_count != 0:\n            if block_count &lt; 0:\n                block_count = -block_count\n                # We ignore the block size for now\n                decoder.skip_int()\n\n            # Since the integers are encoding right next to each other\n            # just read them all at once.\n            contents_array.append(decoder.read_ints(block_count * 2))\n            block_count = decoder.read_int()\n\n        return LazyDict(contents_array)\n\n    def read(self, decoder: BinaryDecoder) -&gt; Mapping[Any, Any]:\n        read_items: dict[Any, Any] = {}\n\n        if self._is_int_int or self._is_int_bytes:\n            if self._is_int_int:\n                return self._read_int_int(decoder)\n\n            block_count = decoder.read_int()\n            while block_count != 0:\n                if block_count &lt; 0:\n                    block_count = -block_count\n                    # We ignore the block size for now\n                    _ = decoder.read_int()\n                decoder.read_int_bytes_dict(block_count, read_items)\n                block_count = decoder.read_int()\n        else:\n            block_count = decoder.read_int()\n            while block_count != 0:\n                if block_count &lt; 0:\n                    block_count = -block_count\n                    # We ignore the block size for now\n                    _ = decoder.read_int()\n                for _ in range(block_count):\n                    key = self._key_reader(decoder)\n                    read_items[key] = self._value_reader(decoder)\n                block_count = decoder.read_int()\n\n        return read_items\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        def skip() -&gt; None:\n            self.key.skip(decoder)\n            self.value.skip(decoder)\n\n        _skip_map_array(decoder, skip)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the MapReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.MapReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the MapReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the MapReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.MapReader._read_int_int","title":"<code>_read_int_int(decoder)</code>","text":"<p>Read a mapping from int to int from the decoder.</p> <p>Read a map of ints to ints from the decoder, since this is such a common data type, it is optimized to be faster than the generic map reader, by using a lazy dict.</p> <p>The time it takes to create the python dictionary is much larger than the time it takes to read the data from the decoder as an array, so the lazy dict defers creating the python dictionary until it is actually accessed.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def _read_int_int(self, decoder: BinaryDecoder) -&gt; Mapping[int, int]:\n    \"\"\"Read a mapping from int to int from the decoder.\n\n    Read a map of ints to ints from the decoder, since this is such a common\n    data type, it is optimized to be faster than the generic map reader, by\n    using a lazy dict.\n\n    The time it takes to create the python dictionary is much larger than\n    the time it takes to read the data from the decoder as an array, so the\n    lazy dict defers creating the python dictionary until it is actually\n    accessed.\n\n    \"\"\"\n    block_count = decoder.read_int()\n\n    # Often times the map is empty, so we can just return an empty dict without\n    # instancing the LazyDict\n    if block_count == 0:\n        return EMPTY_DICT\n\n    contents_array: List[Tuple[int, ...]] = []\n\n    while block_count != 0:\n        if block_count &lt; 0:\n            block_count = -block_count\n            # We ignore the block size for now\n            decoder.skip_int()\n\n        # Since the integers are encoding right next to each other\n        # just read them all at once.\n        contents_array.append(decoder.read_ints(block_count * 2))\n        block_count = decoder.read_int()\n\n    return LazyDict(contents_array)\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>Singleton</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class Reader(Singleton):\n    @abstractmethod\n    def read(self, decoder: BinaryDecoder) -&gt; Any: ...\n\n    @abstractmethod\n    def skip(self, decoder: BinaryDecoder) -&gt; None: ...\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Reader class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.Reader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Reader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Reader class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader","title":"<code>StructReader</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class StructReader(Reader):\n    __slots__ = (\"field_readers\", \"create_struct\", \"struct\", \"_create_with_keyword\", \"_field_reader_functions\", \"_hash\")\n    field_readers: Tuple[Tuple[Optional[int], Reader], ...]\n    create_struct: Callable[..., StructProtocol]\n    struct: StructType\n    field_reader_functions = Tuple[Tuple[Optional[str], int, Optional[Callable[[BinaryDecoder], Any]]], ...]\n\n    def __init__(\n        self,\n        field_readers: Tuple[Tuple[Optional[int], Reader], ...],\n        create_struct: Callable[..., StructProtocol],\n        struct: StructType,\n    ) -&gt; None:\n        self.field_readers = field_readers\n        self.create_struct = create_struct\n        self.struct = struct\n\n        try:\n            # Try initializing the struct, first with the struct keyword argument\n            created_struct = self.create_struct(struct=self.struct)\n            self._create_with_keyword = True\n        except TypeError as e:\n            if \"'struct' is an invalid keyword argument for\" in str(e):\n                created_struct = self.create_struct()\n                self._create_with_keyword = False\n            else:\n                raise ValueError(f\"Unable to initialize struct: {self.create_struct}\") from e\n\n        if not isinstance(created_struct, StructProtocol):\n            raise ValueError(f\"Incompatible with StructProtocol: {self.create_struct}\")\n\n        reading_callbacks: List[Tuple[Optional[int], Callable[[BinaryDecoder], Any]]] = []\n        for pos, field in field_readers:\n            if pos is not None:\n                reading_callbacks.append((pos, field.read))\n            else:\n                reading_callbacks.append((None, field.skip))\n\n        self._field_reader_functions = tuple(reading_callbacks)\n        self._hash = hash(self._field_reader_functions)\n\n    def read(self, decoder: BinaryDecoder) -&gt; StructProtocol:\n        struct = self.create_struct(struct=self.struct) if self._create_with_keyword else self.create_struct()\n        for pos, field_reader in self._field_reader_functions:\n            if pos is not None:\n                struct[pos] = field_reader(decoder)  # later: pass reuse in here\n            else:\n                field_reader(decoder)\n\n        return struct\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        for _, field in self.field_readers:\n            field.skip(decoder)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the StructReader class.\"\"\"\n        return (\n            self.field_readers == other.field_readers and self.create_struct == other.create_struct\n            if isinstance(other, StructReader)\n            else False\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructReader class.\"\"\"\n        return f\"StructReader(({','.join(repr(field) for field in self.field_readers)}), {repr(self.create_struct)})\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the StructReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the StructReader class.\"\"\"\n    return (\n        self.field_readers == other.field_readers and self.create_struct == other.create_struct\n        if isinstance(other, StructReader)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the StructReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructReader class.\"\"\"\n    return f\"StructReader(({','.join(repr(field) for field in self.field_readers)}), {repr(self.create_struct)})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimeReader","title":"<code>TimeReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamp from the stream.</p> <p>Long is decoded as an integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimeReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamp from the stream.\n\n    Long is decoded as an integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestampReader","title":"<code>TimestampReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamp from the stream.</p> <p>Long is decoded as python integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestampReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamp from the stream.\n\n    Long is decoded as python integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestamptzReader","title":"<code>TimestamptzReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamptz from the stream.</p> <p>Long is decoded as python integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> <p>Adjusted to UTC.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestamptzReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamptz from the stream.\n\n    Long is decoded as python integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n\n    Adjusted to UTC.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader._skip_map_array","title":"<code>_skip_map_array(decoder, skip_entry)</code>","text":"<p>Skips over an array or map.</p> <p>Both the array and map are encoded similar, and we can reuse the logic of skipping in an efficient way.</p> <p>From the Avro spec:</p> <p>Maps (and arrays) are encoded as a series of blocks. Each block consists of a long count value, followed by that many key/value pairs in the case of a map, and followed by that many array items in the case of an array. A block with count zero indicates the end of the map. Each item is encoded per the map's value schema.</p> <p>If a block's count is negative, its absolute value is used, and the count is followed immediately by a long block size indicating the number of bytes in the block. This block size permits fast skipping through data, e.g., when projecting a record to a subset of its fields.</p> <p>Parameters:</p> Name Type Description Default <code>decoder</code> <code>BinaryDecoder</code> <p>The decoder that reads the types from the underlying data.</p> required <code>skip_entry</code> <code>Callable[[], None]</code> <p>Function to skip over the underlying data, element in case of an array, and the key/value in the case of a map.</p> required Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def _skip_map_array(decoder: BinaryDecoder, skip_entry: Callable[[], None]) -&gt; None:\n    \"\"\"Skips over an array or map.\n\n    Both the array and map are encoded similar, and we can reuse\n    the logic of skipping in an efficient way.\n\n    From the Avro spec:\n\n    Maps (and arrays) are encoded as a series of blocks.\n    Each block consists of a long count value, followed by that many key/value pairs in the case of a map,\n    and followed by that many array items in the case of an array. A block with count zero indicates the\n    end of the map. Each item is encoded per the map's value schema.\n\n    If a block's count is negative, its absolute value is used, and the count is followed immediately by a\n    long block size indicating the number of bytes in the block. This block size permits fast skipping\n    through data, e.g., when projecting a record to a subset of its fields.\n\n    Args:\n        decoder:\n            The decoder that reads the types from the underlying data.\n        skip_entry:\n            Function to skip over the underlying data, element in case of an array, and the\n            key/value in the case of a map.\n    \"\"\"\n    block_count = decoder.read_int()\n    while block_count != 0:\n        if block_count &lt; 0:\n            # The length in bytes in encoded, so we can skip over it right away\n            block_size = decoder.read_int()\n            decoder.skip(block_size)\n        else:\n            for _ in range(block_count):\n                skip_entry()\n        block_count = decoder.read_int()\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/","title":"resolver","text":""},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.ConstructWriter","title":"<code>ConstructWriter</code>","text":"<p>               Bases: <code>SchemaVisitorPerPrimitiveType[Writer]</code></p> <p>Construct a writer tree from an Iceberg schema.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>class ConstructWriter(SchemaVisitorPerPrimitiveType[Writer]):\n    \"\"\"Construct a writer tree from an Iceberg schema.\"\"\"\n\n    def schema(self, schema: Schema, struct_result: Writer) -&gt; Writer:\n        return struct_result\n\n    def struct(self, struct: StructType, field_results: List[Writer]) -&gt; Writer:\n        return StructWriter(tuple((pos, result) for pos, result in enumerate(field_results)))\n\n    def field(self, field: NestedField, field_result: Writer) -&gt; Writer:\n        return field_result if field.required else OptionWriter(field_result)\n\n    def list(self, list_type: ListType, element_result: Writer) -&gt; Writer:\n        return ListWriter(element_result)\n\n    def map(self, map_type: MapType, key_result: Writer, value_result: Writer) -&gt; Writer:\n        return MapWriter(key_result, value_result)\n\n    def visit_fixed(self, fixed_type: FixedType) -&gt; Writer:\n        return FixedWriter(len(fixed_type))\n\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; Writer:\n        return DecimalWriter(decimal_type.precision, decimal_type.scale)\n\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; Writer:\n        return BooleanWriter()\n\n    def visit_integer(self, integer_type: IntegerType) -&gt; Writer:\n        return IntegerWriter()\n\n    def visit_long(self, long_type: LongType) -&gt; Writer:\n        return IntegerWriter()\n\n    def visit_float(self, float_type: FloatType) -&gt; Writer:\n        return FloatWriter()\n\n    def visit_double(self, double_type: DoubleType) -&gt; Writer:\n        return DoubleWriter()\n\n    def visit_date(self, date_type: DateType) -&gt; Writer:\n        return DateWriter()\n\n    def visit_time(self, time_type: TimeType) -&gt; Writer:\n        return TimeWriter()\n\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; Writer:\n        return TimestampWriter()\n\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; Writer:\n        return TimestamptzWriter()\n\n    def visit_string(self, string_type: StringType) -&gt; Writer:\n        return StringWriter()\n\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; Writer:\n        return UUIDWriter()\n\n    def visit_binary(self, binary_type: BinaryType) -&gt; Writer:\n        return BinaryWriter()\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.EnumReader","title":"<code>EnumReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>An Enum reader to wrap primitive values into an Enum.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>class EnumReader(Reader):\n    \"\"\"An Enum reader to wrap primitive values into an Enum.\"\"\"\n\n    __slots__ = (\"enum\", \"reader\")\n\n    enum: Callable[..., Enum]\n    reader: Reader\n\n    def __init__(self, enum: Callable[..., Enum], reader: Reader) -&gt; None:\n        self.enum = enum\n        self.reader = reader\n\n    def read(self, decoder: BinaryDecoder) -&gt; Enum:\n        return self.enum(self.reader.read(decoder))\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.construct_reader","title":"<code>construct_reader(file_schema, read_types=EMPTY_DICT)</code>","text":"<p>Construct a reader from a file schema.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <code>read_types</code> <code>Dict[int, Callable[..., StructProtocol]]</code> <p>Constructors for structs for certain field-ids</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def construct_reader(\n    file_schema: Union[Schema, IcebergType], read_types: Dict[int, Callable[..., StructProtocol]] = EMPTY_DICT\n) -&gt; Reader:\n    \"\"\"Construct a reader from a file schema.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n        read_types (Dict[int, Callable[..., StructProtocol]]): Constructors for structs for certain field-ids\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return resolve_reader(file_schema, file_schema, read_types)\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.construct_writer","title":"<code>construct_writer(file_schema)</code>","text":"<p>Construct a writer from a file schema.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def construct_writer(file_schema: Union[Schema, IcebergType]) -&gt; Writer:\n    \"\"\"Construct a writer from a file schema.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return visit(file_schema, CONSTRUCT_WRITER_VISITOR)\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.resolve_reader","title":"<code>resolve_reader(file_schema, read_schema, read_types=EMPTY_DICT, read_enums=EMPTY_DICT)</code>","text":"<p>Resolve the file and read schema to produce a reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <code>read_schema</code> <code>Schema | IcebergType</code> <p>The requested read schema which is equal, subset or superset of the file schema.</p> required <code>read_types</code> <code>Dict[int, Callable[..., StructProtocol]]</code> <p>A dict of types to use for struct data.</p> <code>EMPTY_DICT</code> <code>read_enums</code> <code>Dict[int, Callable[..., Enum]]</code> <p>A dict of fields that have to be converted to an enum.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def resolve_reader(\n    file_schema: Union[Schema, IcebergType],\n    read_schema: Union[Schema, IcebergType],\n    read_types: Dict[int, Callable[..., StructProtocol]] = EMPTY_DICT,\n    read_enums: Dict[int, Callable[..., Enum]] = EMPTY_DICT,\n) -&gt; Reader:\n    \"\"\"Resolve the file and read schema to produce a reader.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n        read_schema (Schema | IcebergType): The requested read schema which is equal, subset or superset of the file schema.\n        read_types (Dict[int, Callable[..., StructProtocol]]): A dict of types to use for struct data.\n        read_enums (Dict[int, Callable[..., Enum]]): A dict of fields that have to be converted to an enum.\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return visit_with_partner(file_schema, read_schema, ReadSchemaResolver(read_types, read_enums), SchemaPartnerAccessor())  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.resolve_writer","title":"<code>resolve_writer(record_schema, file_schema)</code>","text":"<p>Resolve the file and read schema to produce a reader.</p> <p>Parameters:</p> Name Type Description Default <code>record_schema</code> <code>Schema | IcebergType</code> <p>The schema of the record in memory.</p> required <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the file that will be written</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def resolve_writer(\n    record_schema: Union[Schema, IcebergType],\n    file_schema: Union[Schema, IcebergType],\n) -&gt; Writer:\n    \"\"\"Resolve the file and read schema to produce a reader.\n\n    Args:\n        record_schema (Schema | IcebergType): The schema of the record in memory.\n        file_schema (Schema | IcebergType): The schema of the file that will be written\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    if record_schema == file_schema:\n        return construct_writer(file_schema)\n    return visit_with_partner(file_schema, record_schema, WriteSchemaResolver(), SchemaPartnerAccessor())  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/","title":"writer","text":"<p>Classes for building the Writer tree.</p> <p>Constructing a writer tree from the schema makes it easy to decouple the writing implementation from the schema.</p>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.BinaryWriter","title":"<code>BinaryWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> <p>Variable byte length writer.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass BinaryWriter(Writer):\n    \"\"\"Variable byte length writer.\"\"\"\n\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; None:\n        encoder.write_bytes(val)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.DecimalWriter","title":"<code>DecimalWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass DecimalWriter(Writer):\n    precision: int = dataclassfield()\n    scale: int = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; None:\n        return encoder.write(decimal_to_bytes(val, byte_length=decimal_required_bytes(self.precision)))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"DecimalWriter({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.DecimalWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"DecimalWriter({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter","title":"<code>FixedWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass FixedWriter(Writer):\n    _len: int = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: bytes) -&gt; None:\n        if len(val) != self._len:\n            raise ValueError(f\"Expected {self._len} bytes, got {len(val)}\")\n        encoder.write(val)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of this object.\"\"\"\n        return self._len\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"FixedWriter({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of this object.\"\"\"\n    return self._len\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"FixedWriter({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.IntegerWriter","title":"<code>IntegerWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> <p>Longs and ints are encoded the same way, and there is no long in Python.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass IntegerWriter(Writer):\n    \"\"\"Longs and ints are encoded the same way, and there is no long in Python.\"\"\"\n\n    def write(self, encoder: BinaryEncoder, val: int) -&gt; None:\n        encoder.write_int(val)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter","title":"<code>StructWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass StructWriter(Writer):\n    field_writers: Tuple[Tuple[Optional[int], Writer], ...] = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: Record) -&gt; None:\n        for pos, writer in self.field_writers:\n            # When pos is None, then it is a default value\n            writer.write(encoder, val[pos] if pos is not None else None)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Implement the equality operator for this object.\"\"\"\n        return self.field_writers == other.field_writers if isinstance(other, StructWriter) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"StructWriter(tuple(({','.join(repr(field) for field in self.field_writers)})))\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the writer as hash of this object.\"\"\"\n        return hash(self.field_writers)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Implement the equality operator for this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Implement the equality operator for this object.\"\"\"\n    return self.field_writers == other.field_writers if isinstance(other, StructWriter) else False\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the writer as hash of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the writer as hash of this object.\"\"\"\n    return hash(self.field_writers)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"StructWriter(tuple(({','.join(repr(field) for field in self.field_writers)})))\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.Writer","title":"<code>Writer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Singleton</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass Writer(Singleton):\n    @abstractmethod\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; Any: ...\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.Writer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/codecs/","title":"codecs","text":"<p>Contains Codecs for Python Avro.</p> <p>Note that the word \"codecs\" means \"compression/decompression algorithms\" in the Avro world (https://avro.apache.org/docs/current/spec.html#Object+Container+Files), so don't confuse it with the Python's \"codecs\", which is a package mainly for converting character sets (https://docs.python.org/3/library/codecs.html).</p>"},{"location":"reference/pyiceberg/avro/codecs/bzip2/","title":"bzip2","text":""},{"location":"reference/pyiceberg/avro/codecs/codec/","title":"codec","text":""},{"location":"reference/pyiceberg/avro/codecs/codec/#pyiceberg.avro.codecs.codec.Codec","title":"<code>Codec</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all Avro codec classes.</p> Source code in <code>pyiceberg/avro/codecs/codec.py</code> <pre><code>class Codec(ABC):\n    \"\"\"Abstract base class for all Avro codec classes.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def compress(data: bytes) -&gt; tuple[bytes, int]: ...\n\n    @staticmethod\n    @abstractmethod\n    def decompress(data: bytes) -&gt; bytes: ...\n</code></pre>"},{"location":"reference/pyiceberg/avro/codecs/deflate/","title":"deflate","text":""},{"location":"reference/pyiceberg/avro/codecs/snappy_codec/","title":"snappy_codec","text":""},{"location":"reference/pyiceberg/avro/codecs/zstandard_codec/","title":"zstandard_codec","text":""},{"location":"reference/pyiceberg/catalog/","title":"catalog","text":""},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog","title":"<code>Catalog</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base Catalog for table operations like - create, drop, load, list and others.</p> <p>The catalog table APIs accept a table identifier, which is fully classified table name. The identifier can be a string or tuple of strings. If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.</p> <p>The catalog namespace APIs follow a similar convention wherein they also accept a namespace identifier that can be a string or tuple of strings.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the catalog.</p> <code>properties</code> <code>Properties</code> <p>Catalog properties.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>class Catalog(ABC):\n    \"\"\"Base Catalog for table operations like - create, drop, load, list and others.\n\n    The catalog table APIs accept a table identifier, which is fully classified table name. The identifier can be a string or\n    tuple of strings. If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n    The catalog namespace APIs follow a similar convention wherein they also accept a namespace identifier that can be a string\n    or tuple of strings.\n\n    Attributes:\n        name (str): Name of the catalog.\n        properties (Properties): Catalog properties.\n    \"\"\"\n\n    name: str\n    properties: Properties\n\n    def __init__(self, name: str, **properties: str):\n        self.name = name\n        self.properties = properties\n\n    @abstractmethod\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            TableAlreadyExistsError: If a table with the name already exists.\n        \"\"\"\n\n    @abstractmethod\n    def create_table_transaction(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        \"\"\"Create a CreateTableTransaction.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            CreateTableTransaction: createTableTransaction instance.\n        \"\"\"\n\n    def create_table_if_not_exists(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table if it does not exist.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance if the table does not exist, else the existing\n            table instance.\n        \"\"\"\n        try:\n            return self.create_table(identifier, schema, location, partition_spec, sort_order, properties)\n        except TableAlreadyExistsError:\n            return self.load_table(identifier)\n\n    @abstractmethod\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        \"\"\"Check if a table exists.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        \"\"\"Check if a view exists.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Returns:\n            bool: True if the view exists, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n\n    @abstractmethod\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table and purge all data and metadata files.\n\n        Note: This method only logs warning rather than raise exception when encountering file deletion failure.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n\n    @abstractmethod\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier (str | Identifier): Existing table identifier.\n            to_identifier (str | Identifier): New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n            CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n        \"\"\"\n\n    @abstractmethod\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n\n        Raises:\n            NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n\n    def create_namespace_if_not_exists(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace if it does not exist.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n        \"\"\"\n        try:\n            self.create_namespace(namespace, properties)\n        except NamespaceAlreadyExistsError:\n            pass\n\n    @abstractmethod\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n\n    @abstractmethod\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List views under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and updates properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n            updates (Properties): Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n\n    @abstractmethod\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a view.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Raises:\n            NoSuchViewError: If a view with the given name does not exist.\n        \"\"\"\n\n    @staticmethod\n    def identifier_to_tuple(identifier: Union[str, Identifier]) -&gt; Identifier:\n        \"\"\"Parse an identifier to a tuple.\n\n        If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n        Args:\n            identifier (str | Identifier): an identifier, either a string or tuple of strings.\n\n        Returns:\n            Identifier: a tuple of strings.\n        \"\"\"\n        return identifier if isinstance(identifier, tuple) else tuple(str.split(identifier, \".\"))\n\n    @staticmethod\n    def table_name_from(identifier: Union[str, Identifier]) -&gt; str:\n        \"\"\"Extract table name from a table identifier.\n\n        Args:\n            identifier (str | Identifier: a table identifier.\n\n        Returns:\n            str: Table name.\n        \"\"\"\n        return Catalog.identifier_to_tuple(identifier)[-1]\n\n    @staticmethod\n    def namespace_from(identifier: Union[str, Identifier]) -&gt; Identifier:\n        \"\"\"Extract table namespace from a table identifier.\n\n        Args:\n            identifier (Union[str, Identifier]): a table identifier.\n\n        Returns:\n            Identifier: Namespace identifier.\n        \"\"\"\n        return Catalog.identifier_to_tuple(identifier)[:-1]\n\n    @staticmethod\n    def namespace_to_string(\n        identifier: Union[str, Identifier], err: Union[Type[ValueError], Type[NoSuchNamespaceError]] = ValueError\n    ) -&gt; str:\n        \"\"\"Transform a namespace identifier into a string.\n\n        Args:\n            identifier (Union[str, Identifier]): a namespace identifier.\n            err (Union[Type[ValueError], Type[NoSuchNamespaceError]]): the error type to raise when identifier is empty.\n\n        Returns:\n            Identifier: Namespace identifier.\n        \"\"\"\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) &lt; 1:\n            raise err(\"Empty namespace identifier\")\n\n        # Check if any segment of the tuple is an empty string\n        if any(segment.strip() == \"\" for segment in tuple_identifier):\n            raise err(\"Namespace identifier contains an empty segment or a segment with only whitespace\")\n\n        return \".\".join(segment.strip() for segment in tuple_identifier)\n\n    @staticmethod\n    def identifier_to_database(\n        identifier: Union[str, Identifier], err: Union[Type[ValueError], Type[NoSuchNamespaceError]] = ValueError\n    ) -&gt; str:\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) != 1:\n            raise err(f\"Invalid database, hierarchical namespaces are not supported: {identifier}\")\n\n        return tuple_identifier[0]\n\n    @staticmethod\n    def identifier_to_database_and_table(\n        identifier: Union[str, Identifier],\n        err: Union[Type[ValueError], Type[NoSuchTableError], Type[NoSuchNamespaceError]] = ValueError,\n    ) -&gt; Tuple[str, str]:\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) != 2:\n            raise err(f\"Invalid path, hierarchical namespaces are not supported: {identifier}\")\n\n        return tuple_identifier[0], tuple_identifier[1]\n\n    def _load_file_io(self, properties: Properties = EMPTY_DICT, location: Optional[str] = None) -&gt; FileIO:\n        return load_file_io({**self.properties, **properties}, location)\n\n    @staticmethod\n    def _convert_schema_if_needed(schema: Union[Schema, \"pa.Schema\"]) -&gt; Schema:\n        if isinstance(schema, Schema):\n            return schema\n        try:\n            import pyarrow as pa\n\n            from pyiceberg.io.pyarrow import _ConvertToIcebergWithoutIDs, visit_pyarrow\n\n            downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n            if isinstance(schema, pa.Schema):\n                schema: Schema = visit_pyarrow(  # type: ignore\n                    schema, _ConvertToIcebergWithoutIDs(downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us)\n                )\n                return schema\n        except ModuleNotFoundError:\n            pass\n        raise ValueError(f\"{type(schema)=}, but it must be pyiceberg.schema.Schema or pyarrow.Schema\")\n\n    @staticmethod\n    def _delete_old_metadata(io: FileIO, base: TableMetadata, metadata: TableMetadata) -&gt; None:\n        \"\"\"Delete oldest metadata if config is set to true.\"\"\"\n        delete_after_commit: bool = property_as_bool(\n            metadata.properties,\n            TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED,\n            TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED_DEFAULT,\n        )\n\n        if delete_after_commit:\n            removed_previous_metadata_files: set[str] = {log.metadata_file for log in base.metadata_log}\n            current_metadata_files: set[str] = {log.metadata_file for log in metadata.metadata_log}\n            removed_previous_metadata_files.difference_update(current_metadata_files)\n            delete_files(io, removed_previous_metadata_files, METADATA)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Catalog class.\"\"\"\n        return f\"{self.name} ({self.__class__})\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Catalog class.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Catalog class.\"\"\"\n    return f\"{self.name} ({self.__class__})\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog._delete_old_metadata","title":"<code>_delete_old_metadata(io, base, metadata)</code>  <code>staticmethod</code>","text":"<p>Delete oldest metadata if config is set to true.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef _delete_old_metadata(io: FileIO, base: TableMetadata, metadata: TableMetadata) -&gt; None:\n    \"\"\"Delete oldest metadata if config is set to true.\"\"\"\n    delete_after_commit: bool = property_as_bool(\n        metadata.properties,\n        TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED,\n        TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED_DEFAULT,\n    )\n\n    if delete_after_commit:\n        removed_previous_metadata_files: set[str] = {log.metadata_file for log in base.metadata_log}\n        current_metadata_files: set[str] = {log.metadata_file for log in metadata.metadata_log}\n        removed_previous_metadata_files.difference_update(current_metadata_files)\n        delete_files(io, removed_previous_metadata_files, METADATA)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>  <code>abstractmethod</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> <code>CommitStateUnknownException</code> <p>Failed due to an internal exception on the side of the catalog.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NamespaceAlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n\n    Raises:\n        NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_namespace_if_not_exists","title":"<code>create_namespace_if_not_exists(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def create_namespace_if_not_exists(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace if it does not exist.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n    \"\"\"\n    try:\n        self.create_namespace(namespace, properties)\n    except NamespaceAlreadyExistsError:\n        pass\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If a table with the name already exists.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        TableAlreadyExistsError: If a table with the name already exists.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table_if_not_exists","title":"<code>create_table_if_not_exists(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create a table if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance if the table does not exist, else the existing</p> <code>Table</code> <p>table instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def create_table_if_not_exists(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table if it does not exist.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance if the table does not exist, else the existing\n        table instance.\n    \"\"\"\n    try:\n        return self.create_table(identifier, schema, location, partition_spec, sort_order, properties)\n    except TableAlreadyExistsError:\n        return self.load_table(identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table_transaction","title":"<code>create_table_transaction(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a CreateTableTransaction.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>CreateTableTransaction</code> <code>CreateTableTransaction</code> <p>createTableTransaction instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_table_transaction(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; CreateTableTransaction:\n    \"\"\"Create a CreateTableTransaction.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        CreateTableTransaction: createTableTransaction instance.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>  <code>abstractmethod</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_table","title":"<code>drop_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_view","title":"<code>drop_view(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a view.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchViewError</code> <p>If a view with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a view.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Raises:\n        NoSuchViewError: If a view with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.identifier_to_tuple","title":"<code>identifier_to_tuple(identifier)</code>  <code>staticmethod</code>","text":"<p>Parse an identifier to a tuple.</p> <p>If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>an identifier, either a string or tuple of strings.</p> required <p>Returns:</p> Name Type Description <code>Identifier</code> <code>Identifier</code> <p>a tuple of strings.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef identifier_to_tuple(identifier: Union[str, Identifier]) -&gt; Identifier:\n    \"\"\"Parse an identifier to a tuple.\n\n    If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n    Args:\n        identifier (str | Identifier): an identifier, either a string or tuple of strings.\n\n    Returns:\n        Identifier: a tuple of strings.\n    \"\"\"\n    return identifier if isinstance(identifier, tuple) else tuple(str.split(identifier, \".\"))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>  <code>abstractmethod</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> <code>()</code> <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_tables","title":"<code>list_tables(namespace)</code>  <code>abstractmethod</code>","text":"<p>List tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_views","title":"<code>list_views(namespace)</code>  <code>abstractmethod</code>","text":"<p>List views under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List views under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>  <code>abstractmethod</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.load_table","title":"<code>load_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.namespace_from","title":"<code>namespace_from(identifier)</code>  <code>staticmethod</code>","text":"<p>Extract table namespace from a table identifier.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>a table identifier.</p> required <p>Returns:</p> Name Type Description <code>Identifier</code> <code>Identifier</code> <p>Namespace identifier.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef namespace_from(identifier: Union[str, Identifier]) -&gt; Identifier:\n    \"\"\"Extract table namespace from a table identifier.\n\n    Args:\n        identifier (Union[str, Identifier]): a table identifier.\n\n    Returns:\n        Identifier: Namespace identifier.\n    \"\"\"\n    return Catalog.identifier_to_tuple(identifier)[:-1]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.namespace_to_string","title":"<code>namespace_to_string(identifier, err=ValueError)</code>  <code>staticmethod</code>","text":"<p>Transform a namespace identifier into a string.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>a namespace identifier.</p> required <code>err</code> <code>Union[Type[ValueError], Type[NoSuchNamespaceError]]</code> <p>the error type to raise when identifier is empty.</p> <code>ValueError</code> <p>Returns:</p> Name Type Description <code>Identifier</code> <code>str</code> <p>Namespace identifier.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef namespace_to_string(\n    identifier: Union[str, Identifier], err: Union[Type[ValueError], Type[NoSuchNamespaceError]] = ValueError\n) -&gt; str:\n    \"\"\"Transform a namespace identifier into a string.\n\n    Args:\n        identifier (Union[str, Identifier]): a namespace identifier.\n        err (Union[Type[ValueError], Type[NoSuchNamespaceError]]): the error type to raise when identifier is empty.\n\n    Returns:\n        Identifier: Namespace identifier.\n    \"\"\"\n    tuple_identifier = Catalog.identifier_to_tuple(identifier)\n    if len(tuple_identifier) &lt; 1:\n        raise err(\"Empty namespace identifier\")\n\n    # Check if any segment of the tuple is an empty string\n    if any(segment.strip() == \"\" for segment in tuple_identifier):\n        raise err(\"Namespace identifier contains an empty segment or a segment with only whitespace\")\n\n    return \".\".join(segment.strip() for segment in tuple_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.purge_table","title":"<code>purge_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a table and purge all data and metadata files.</p> <p>Note: This method only logs warning rather than raise exception when encountering file deletion failure.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table and purge all data and metadata files.\n\n    Note: This method only logs warning rather than raise exception when encountering file deletion failure.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>  <code>abstractmethod</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>  <code>abstractmethod</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier (str | Identifier): Existing table identifier.\n        to_identifier (str | Identifier): New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.table_exists","title":"<code>table_exists(identifier)</code>  <code>abstractmethod</code>","text":"<p>Check if a table exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n    \"\"\"Check if a table exists.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.table_name_from","title":"<code>table_name_from(identifier)</code>  <code>staticmethod</code>","text":"<p>Extract table name from a table identifier.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>a table identifier.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Table name.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef table_name_from(identifier: Union[str, Identifier]) -&gt; str:\n    \"\"\"Extract table name from a table identifier.\n\n    Args:\n        identifier (str | Identifier: a table identifier.\n\n    Returns:\n        str: Table name.\n    \"\"\"\n    return Catalog.identifier_to_tuple(identifier)[-1]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Remove provided property keys and updates properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Set[str]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef update_namespace_properties(\n    self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and updates properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n        updates (Properties): Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.view_exists","title":"<code>view_exists(identifier)</code>  <code>abstractmethod</code>","text":"<p>Check if a view exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the view exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n    \"\"\"Check if a view exists.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Returns:\n        bool: True if the view exists, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog","title":"<code>MetastoreCatalog</code>","text":"<p>               Bases: <code>Catalog</code>, <code>ABC</code></p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>class MetastoreCatalog(Catalog, ABC):\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n    def create_table_transaction(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        return CreateTableTransaction(\n            self._create_staged_table(identifier, schema, location, partition_spec, sort_order, properties)\n        )\n\n    def table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        try:\n            self.load_table(identifier)\n            return True\n        except NoSuchTableError:\n            return False\n\n    def purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        table = self.load_table(identifier)\n        self.drop_table(identifier)\n        io = load_file_io(self.properties, table.metadata_location)\n        metadata = table.metadata\n        manifest_lists_to_delete = set()\n        manifests_to_delete: List[ManifestFile] = []\n        for snapshot in metadata.snapshots:\n            manifests_to_delete += snapshot.manifests(io)\n            manifest_lists_to_delete.add(snapshot.manifest_list)\n\n        manifest_paths_to_delete = {manifest.manifest_path for manifest in manifests_to_delete}\n        prev_metadata_files = {log.metadata_file for log in metadata.metadata_log}\n\n        delete_data_files(io, manifests_to_delete)\n        delete_files(io, manifest_paths_to_delete, MANIFEST)\n        delete_files(io, manifest_lists_to_delete, MANIFEST_LIST)\n        delete_files(io, prev_metadata_files, PREVIOUS_METADATA)\n        delete_files(io, {table.metadata_location}, METADATA)\n\n    def _create_staged_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; StagedTable:\n        \"\"\"Create a table and return the table instance without committing the changes.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            StagedTable: the created staged table instance.\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        location = self._resolve_table_location(location, database_name, table_name)\n        provider = load_location_provider(location, properties)\n        metadata_location = provider.new_table_metadata_file_location()\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = self._load_file_io(properties=properties, location=metadata_location)\n        return StagedTable(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=io,\n            catalog=self,\n        )\n\n    def _update_and_stage_table(\n        self,\n        current_table: Optional[Table],\n        table_identifier: Identifier,\n        requirements: Tuple[TableRequirement, ...],\n        updates: Tuple[TableUpdate, ...],\n    ) -&gt; StagedTable:\n        for requirement in requirements:\n            requirement.validate(current_table.metadata if current_table else None)\n\n        updated_metadata = update_table_metadata(\n            base_metadata=current_table.metadata if current_table else self._empty_table_metadata(),\n            updates=updates,\n            enforce_validation=current_table is None,\n            metadata_location=current_table.metadata_location if current_table else None,\n        )\n\n        new_metadata_version = self._parse_metadata_version(current_table.metadata_location) + 1 if current_table else 0\n        provider = load_location_provider(updated_metadata.location, updated_metadata.properties)\n        new_metadata_location = provider.new_table_metadata_file_location(new_metadata_version)\n\n        return StagedTable(\n            identifier=table_identifier,\n            metadata=updated_metadata,\n            metadata_location=new_metadata_location,\n            io=self._load_file_io(properties=updated_metadata.properties, location=new_metadata_location),\n            catalog=self,\n        )\n\n    def _get_updated_props_and_update_summary(\n        self, current_properties: Properties, removals: Optional[Set[str]], updates: Properties\n    ) -&gt; Tuple[PropertiesUpdateSummary, Properties]:\n        self._check_for_overlap(updates=updates, removals=removals)\n        updated_properties = dict(current_properties)\n\n        removed: Set[str] = set()\n        updated: Set[str] = set()\n\n        if removals:\n            for key in removals:\n                if key in updated_properties:\n                    updated_properties.pop(key)\n                    removed.add(key)\n        if updates:\n            for key, value in updates.items():\n                updated_properties[key] = value\n                updated.add(key)\n\n        expected_to_change = (removals or set()).difference(removed)\n        properties_update_summary = PropertiesUpdateSummary(\n            removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change)\n        )\n\n        return properties_update_summary, updated_properties\n\n    def _resolve_table_location(self, location: Optional[str], database_name: str, table_name: str) -&gt; str:\n        if not location:\n            return self._get_default_warehouse_location(database_name, table_name)\n        return location.rstrip(\"/\")\n\n    def _get_default_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        database_properties = self.load_namespace_properties(database_name)\n        if database_location := database_properties.get(LOCATION):\n            database_location = database_location.rstrip(\"/\")\n            return f\"{database_location}/{table_name}\"\n\n        if warehouse_path := self.properties.get(WAREHOUSE_LOCATION):\n            warehouse_path = warehouse_path.rstrip(\"/\")\n            return f\"{warehouse_path}/{database_name}.db/{table_name}\"\n\n        raise ValueError(\"No default path is set, please specify a location when creating a table\")\n\n    @staticmethod\n    def _write_metadata(metadata: TableMetadata, io: FileIO, metadata_path: str) -&gt; None:\n        ToOutputFile.table_metadata(metadata, io.new_output(metadata_path))\n\n    @staticmethod\n    def _parse_metadata_version(metadata_location: str) -&gt; int:\n        \"\"\"Parse the version from the metadata location.\n\n        The version is the first part of the file name, before the first dash.\n        For example, the version of the metadata file\n        `s3://bucket/db/tb/metadata/00001-6c97e413-d51b-4538-ac70-12fe2a85cb83.metadata.json`\n        is 1.\n        If the path does not comply with the pattern, the version is defaulted to be -1, ensuring\n        that the next metadata file is treated as having version 0.\n\n        Args:\n            metadata_location (str): The location of the metadata file.\n\n        Returns:\n            int: The version of the metadata file. -1 if the file name does not have valid version string\n        \"\"\"\n        file_name = metadata_location.split(\"/\")[-1]\n        if file_name_match := TABLE_METADATA_FILE_NAME_REGEX.fullmatch(file_name):\n            try:\n                uuid.UUID(file_name_match.group(2))\n            except ValueError:\n                return -1\n            return int(file_name_match.group(1))\n        else:\n            return -1\n\n    @staticmethod\n    def _check_for_overlap(removals: Optional[Set[str]], updates: Properties) -&gt; None:\n        if updates and removals:\n            overlap = set(removals) &amp; set(updates.keys())\n            if overlap:\n                raise ValueError(f\"Updates and deletes have an overlap: {overlap}\")\n\n    @staticmethod\n    def _empty_table_metadata() -&gt; TableMetadata:\n        \"\"\"Return an empty TableMetadata instance.\n\n        It is used to build a TableMetadata from a sequence of initial TableUpdates.\n        It is a V1 TableMetadata because there will be a UpgradeFormatVersionUpdate in\n        initial changes to bump the metadata to the target version.\n\n        Returns:\n            TableMetadata: An empty TableMetadata instance.\n        \"\"\"\n        return TableMetadataV1.model_construct(last_column_id=-1, schema=Schema())\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog._create_staged_table","title":"<code>_create_staged_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create a table and return the table instance without committing the changes.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>StagedTable</code> <code>StagedTable</code> <p>the created staged table instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def _create_staged_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; StagedTable:\n    \"\"\"Create a table and return the table instance without committing the changes.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        StagedTable: the created staged table instance.\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    location = self._resolve_table_location(location, database_name, table_name)\n    provider = load_location_provider(location, properties)\n    metadata_location = provider.new_table_metadata_file_location()\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n    io = self._load_file_io(properties=properties, location=metadata_location)\n    return StagedTable(\n        identifier=(database_name, table_name),\n        metadata=metadata,\n        metadata_location=metadata_location,\n        io=io,\n        catalog=self,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog._empty_table_metadata","title":"<code>_empty_table_metadata()</code>  <code>staticmethod</code>","text":"<p>Return an empty TableMetadata instance.</p> <p>It is used to build a TableMetadata from a sequence of initial TableUpdates. It is a V1 TableMetadata because there will be a UpgradeFormatVersionUpdate in initial changes to bump the metadata to the target version.</p> <p>Returns:</p> Name Type Description <code>TableMetadata</code> <code>TableMetadata</code> <p>An empty TableMetadata instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef _empty_table_metadata() -&gt; TableMetadata:\n    \"\"\"Return an empty TableMetadata instance.\n\n    It is used to build a TableMetadata from a sequence of initial TableUpdates.\n    It is a V1 TableMetadata because there will be a UpgradeFormatVersionUpdate in\n    initial changes to bump the metadata to the target version.\n\n    Returns:\n        TableMetadata: An empty TableMetadata instance.\n    \"\"\"\n    return TableMetadataV1.model_construct(last_column_id=-1, schema=Schema())\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog._parse_metadata_version","title":"<code>_parse_metadata_version(metadata_location)</code>  <code>staticmethod</code>","text":"<p>Parse the version from the metadata location.</p> <p>The version is the first part of the file name, before the first dash. For example, the version of the metadata file <code>s3://bucket/db/tb/metadata/00001-6c97e413-d51b-4538-ac70-12fe2a85cb83.metadata.json</code> is 1. If the path does not comply with the pattern, the version is defaulted to be -1, ensuring that the next metadata file is treated as having version 0.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_location</code> <code>str</code> <p>The location of the metadata file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The version of the metadata file. -1 if the file name does not have valid version string</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef _parse_metadata_version(metadata_location: str) -&gt; int:\n    \"\"\"Parse the version from the metadata location.\n\n    The version is the first part of the file name, before the first dash.\n    For example, the version of the metadata file\n    `s3://bucket/db/tb/metadata/00001-6c97e413-d51b-4538-ac70-12fe2a85cb83.metadata.json`\n    is 1.\n    If the path does not comply with the pattern, the version is defaulted to be -1, ensuring\n    that the next metadata file is treated as having version 0.\n\n    Args:\n        metadata_location (str): The location of the metadata file.\n\n    Returns:\n        int: The version of the metadata file. -1 if the file name does not have valid version string\n    \"\"\"\n    file_name = metadata_location.split(\"/\")[-1]\n    if file_name_match := TABLE_METADATA_FILE_NAME_REGEX.fullmatch(file_name):\n        try:\n            uuid.UUID(file_name_match.group(2))\n        except ValueError:\n            return -1\n        return int(file_name_match.group(1))\n    else:\n        return -1\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.delete_data_files","title":"<code>delete_data_files(io, manifests_to_delete)</code>","text":"<p>Delete data files linked to given manifests.</p> <p>Log warnings if failing to delete any file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO used to delete the object.</p> required <code>manifests_to_delete</code> <code>List[ManifestFile]</code> <p>A list of manifest contains paths of data files to be deleted.</p> required Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def delete_data_files(io: FileIO, manifests_to_delete: List[ManifestFile]) -&gt; None:\n    \"\"\"Delete data files linked to given manifests.\n\n    Log warnings if failing to delete any file.\n\n    Args:\n        io: The FileIO used to delete the object.\n        manifests_to_delete: A list of manifest contains paths of data files to be deleted.\n    \"\"\"\n    deleted_files: dict[str, bool] = {}\n    for manifest_file in manifests_to_delete:\n        for entry in manifest_file.fetch_manifest_entry(io, discard_deleted=False):\n            path = entry.data_file.file_path\n            if not deleted_files.get(path, False):\n                try:\n                    io.delete(path)\n                except OSError as exc:\n                    logger.warning(msg=f\"Failed to delete data file {path}\", exc_info=exc)\n                deleted_files[path] = True\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.delete_files","title":"<code>delete_files(io, files_to_delete, file_type)</code>","text":"<p>Delete files.</p> <p>Log warnings if failing to delete any file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO used to delete the object.</p> required <code>files_to_delete</code> <code>Set[str]</code> <p>A set of file paths to be deleted.</p> required <code>file_type</code> <code>str</code> <p>The type of the file.</p> required Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def delete_files(io: FileIO, files_to_delete: Set[str], file_type: str) -&gt; None:\n    \"\"\"Delete files.\n\n    Log warnings if failing to delete any file.\n\n    Args:\n        io: The FileIO used to delete the object.\n        files_to_delete: A set of file paths to be deleted.\n        file_type: The type of the file.\n    \"\"\"\n    for file in files_to_delete:\n        try:\n            io.delete(file)\n        except OSError as exc:\n            logger.warning(msg=f\"Failed to delete {file_type} file {file}\", exc_info=exc)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.infer_catalog_type","title":"<code>infer_catalog_type(name, catalog_properties)</code>","text":"<p>Try to infer the type based on the dict.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the catalog.</p> required <code>catalog_properties</code> <code>RecursiveDict</code> <p>Catalog properties.</p> required <p>Returns:</p> Type Description <code>Optional[CatalogType]</code> <p>The inferred type based on the provided properties.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises a ValueError in case properties are missing, or the wrong type.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def infer_catalog_type(name: str, catalog_properties: RecursiveDict) -&gt; Optional[CatalogType]:\n    \"\"\"Try to infer the type based on the dict.\n\n    Args:\n        name: Name of the catalog.\n        catalog_properties: Catalog properties.\n\n    Returns:\n        The inferred type based on the provided properties.\n\n    Raises:\n        ValueError: Raises a ValueError in case properties are missing, or the wrong type.\n    \"\"\"\n    if uri := catalog_properties.get(\"uri\"):\n        if isinstance(uri, str):\n            if uri.startswith(\"http\"):\n                return CatalogType.REST\n            elif uri.startswith(\"thrift\"):\n                return CatalogType.HIVE\n            elif uri.startswith((\"sqlite\", \"postgresql\")):\n                return CatalogType.SQL\n            else:\n                raise ValueError(f\"Could not infer the catalog type from the uri: {uri}\")\n        else:\n            raise ValueError(f\"Expects the URI to be a string, got: {type(uri)}\")\n    raise ValueError(\n        f\"URI missing, please provide using --uri, the config or environment variable PYICEBERG_CATALOG__{name.upper()}__URI\"\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.load_catalog","title":"<code>load_catalog(name=None, **properties)</code>","text":"<p>Load the catalog based on the properties.</p> <p>Will look up the properties from the config, based on the name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the catalog.</p> <code>None</code> <code>properties</code> <code>Optional[str]</code> <p>The properties that are used next to the configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Catalog</code> <p>An initialized Catalog.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises a ValueError in case properties are missing or malformed, or if it could not determine the catalog based on the properties.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def load_catalog(name: Optional[str] = None, **properties: Optional[str]) -&gt; Catalog:\n    \"\"\"Load the catalog based on the properties.\n\n    Will look up the properties from the config, based on the name.\n\n    Args:\n        name: The name of the catalog.\n        properties: The properties that are used next to the configuration.\n\n    Returns:\n        An initialized Catalog.\n\n    Raises:\n        ValueError: Raises a ValueError in case properties are missing or malformed,\n            or if it could not determine the catalog based on the properties.\n    \"\"\"\n    if name is None:\n        name = _ENV_CONFIG.get_default_catalog_name()\n\n    env = _ENV_CONFIG.get_catalog_config(name)\n    conf: RecursiveDict = merge_config(env or {}, cast(RecursiveDict, properties))\n\n    catalog_type: Optional[CatalogType]\n    provided_catalog_type = conf.get(TYPE)\n\n    if catalog_impl := properties.get(PY_CATALOG_IMPL):\n        if provided_catalog_type:\n            raise ValueError(\n                \"Must not set both catalog type and py-catalog-impl configurations, \"\n                f\"but found type {provided_catalog_type} and py-catalog-impl {catalog_impl}\"\n            )\n\n        if catalog := _import_catalog(name, catalog_impl, properties):\n            logger.info(\"Loaded Catalog: %s\", catalog_impl)\n            return catalog\n        else:\n            raise ValueError(f\"Could not initialize Catalog: {catalog_impl}\")\n\n    catalog_type = None\n    if provided_catalog_type and isinstance(provided_catalog_type, str):\n        catalog_type = CatalogType(provided_catalog_type.lower())\n    elif not provided_catalog_type:\n        catalog_type = infer_catalog_type(name, conf)\n\n    if catalog_type:\n        return AVAILABLE_CATALOGS[catalog_type](name, cast(Dict[str, str], conf))\n\n    raise ValueError(f\"Could not initialize catalog with the following properties: {properties}\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/","title":"dynamodb","text":""},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog","title":"<code>DynamoDbCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>class DynamoDbCatalog(MetastoreCatalog):\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n        session = boto3.Session(\n            profile_name=properties.get(DYNAMODB_PROFILE_NAME),\n            region_name=get_first_property_value(properties, DYNAMODB_REGION, AWS_REGION),\n            botocore_session=properties.get(BOTOCORE_SESSION),\n            aws_access_key_id=get_first_property_value(properties, DYNAMODB_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            aws_secret_access_key=get_first_property_value(properties, DYNAMODB_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            aws_session_token=get_first_property_value(properties, DYNAMODB_SESSION_TOKEN, AWS_SESSION_TOKEN),\n        )\n        self.dynamodb = session.client(DYNAMODB_CLIENT)\n        self.dynamodb_table_name = self.properties.get(DYNAMODB_TABLE_NAME, DYNAMODB_TABLE_NAME_DEFAULT)\n        self._ensure_catalog_table_exists_or_create()\n\n    def _ensure_catalog_table_exists_or_create(self) -&gt; None:\n        if self._dynamodb_table_exists():\n            return None\n\n        try:\n            self.dynamodb.create_table(\n                TableName=self.dynamodb_table_name,\n                AttributeDefinitions=CREATE_CATALOG_ATTRIBUTE_DEFINITIONS,\n                KeySchema=CREATE_CATALOG_KEY_SCHEMA,\n                GlobalSecondaryIndexes=CREATE_CATALOG_GLOBAL_SECONDARY_INDEXES,\n                BillingMode=DYNAMODB_PAY_PER_REQUEST,\n            )\n        except (\n            self.dynamodb.exceptions.ResourceInUseException,\n            self.dynamodb.exceptions.LimitExceededException,\n            self.dynamodb.exceptions.InternalServerError,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _dynamodb_table_exists(self) -&gt; bool:\n        try:\n            response = self.dynamodb.describe_table(TableName=self.dynamodb_table_name)\n        except self.dynamodb.exceptions.ResourceNotFoundException:\n            return False\n        except self.dynamodb.exceptions.InternalServerError as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        if response[\"Table\"][\"TableStatus\"] != ACTIVE:\n            raise GenericDynamoDbError(f\"DynamoDB table for catalog {self.dynamodb_table_name} is not {ACTIVE}\")\n        else:\n            return True\n\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        location = self._resolve_table_location(location, database_name, table_name)\n        provider = load_location_provider(table_location=location, table_properties=properties)\n        metadata_location = provider.new_table_metadata_file_location()\n\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        self._write_metadata(metadata, io, metadata_location)\n\n        self._ensure_namespace_exists(database_name=database_name)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_create_table_item(\n                    database_name=database_name, table_name=table_name, properties=properties, metadata_location=metadata_location\n                ),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        raise NotImplementedError\n\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        raise NotImplementedError\n\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"\n        Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        dynamo_table_item = self._get_iceberg_table_item(database_name=database_name, table_name=table_name)\n        return self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=dynamo_table_item)\n\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        try:\n            self._delete_dynamo_item(\n                namespace=database_name,\n                identifier=f\"{database_name}.{table_name}\",\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        This method can only rename Iceberg tables in AWS Glue.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchIcebergTableError: When from table is not a valid iceberg table.\n            NoSuchPropertyException: When from table miss some required properties.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n        from_table_item = self._get_iceberg_table_item(database_name=from_database_name, table_name=from_table_name)\n\n        try:\n            # Verify that from_identifier is a valid iceberg table\n            self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=from_table_item)\n        except NoSuchPropertyException as e:\n            raise NoSuchPropertyException(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n            ) from e\n        except NoSuchIcebergTableError as e:\n            raise NoSuchIcebergTableError(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n            ) from e\n\n        self._ensure_namespace_exists(database_name=from_database_name)\n        self._ensure_namespace_exists(database_name=to_database_name)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_rename_table_item(\n                    from_dynamo_table_item=from_table_item, to_database_name=to_database_name, to_table_name=to_table_name\n                ),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise TableAlreadyExistsError(f\"Table {to_database_name}.{to_table_name} already exists\") from e\n\n        try:\n            self.drop_table(from_identifier)\n        except (NoSuchTableError, GenericDynamoDbError) as e:\n            log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n            try:\n                self.drop_table(to_identifier)\n                log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n            except (NoSuchTableError, GenericDynamoDbError):\n                log_message += (\n                    f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n                )\n\n            raise ValueError(log_message) from e\n\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_create_database_item(database_name=database_name, properties=properties),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_NAMESPACE})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        A Glue namespace can only be dropped if it is empty.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        table_identifiers = self.list_tables(namespace=database_name)\n\n        if len(table_identifiers) &gt; 0:\n            raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n        try:\n            self._delete_dynamo_item(\n                namespace=database_name,\n                identifier=DYNAMODB_NAMESPACE,\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n\n        paginator = self.dynamodb.get_paginator(\"query\")\n\n        try:\n            page_iterator = paginator.paginate(\n                TableName=self.dynamodb_table_name,\n                IndexName=DYNAMODB_NAMESPACE_GSI,\n                KeyConditionExpression=f\"{DYNAMODB_COL_NAMESPACE} = :namespace \",\n                ExpressionAttributeValues={\n                    \":namespace\": {\n                        \"S\": database_name,\n                    }\n                },\n            )\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        table_identifiers = []\n        for page in page_iterator:\n            for item in page[\"Items\"]:\n                _dict = _convert_dynamo_item_to_regular_dict(item)\n                identifier_col = _dict[DYNAMODB_COL_IDENTIFIER]\n                if identifier_col == DYNAMODB_NAMESPACE:\n                    continue\n\n                table_identifiers.append(self.identifier_to_tuple(identifier_col))\n\n        return table_identifiers\n\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        \"\"\"List top-level namespaces from the catalog.\n\n        We do not support hierarchical namespace.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        paginator = self.dynamodb.get_paginator(\"query\")\n\n        try:\n            page_iterator = paginator.paginate(\n                TableName=self.dynamodb_table_name,\n                ConsistentRead=True,\n                KeyConditionExpression=f\"{DYNAMODB_COL_IDENTIFIER} = :identifier\",\n                ExpressionAttributeValues={\n                    \":identifier\": {\n                        \"S\": DYNAMODB_NAMESPACE,\n                    }\n                },\n            )\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        database_identifiers = []\n        for page in page_iterator:\n            for item in page[\"Items\"]:\n                _dict = _convert_dynamo_item_to_regular_dict(item)\n                namespace_col = _dict[DYNAMODB_COL_NAMESPACE]\n                database_identifiers.append(self.identifier_to_tuple(namespace_col))\n\n        return database_identifiers\n\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        \"\"\"\n        Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n        namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n        return _get_namespace_properties(namespace_dict=namespace_dict)\n\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"\n        Remove or update provided property keys for a namespace.\n\n        Args:\n            namespace: Namespace identifier\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n        namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n        current_properties = _get_namespace_properties(namespace_dict=namespace_dict)\n\n        properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )\n\n        try:\n            self._put_dynamo_item(\n                item=_get_update_database_item(\n                    namespace_item=namespace_item,\n                    updated_properties=updated_properties,\n                ),\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_NAMESPACE})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n        return properties_update_summary\n\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    def _get_iceberg_table_item(self, database_name: str, table_name: str) -&gt; Dict[str, Any]:\n        try:\n            return self._get_dynamo_item(identifier=f\"{database_name}.{table_name}\", namespace=database_name)\n        except ValueError as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def _get_iceberg_namespace_item(self, database_name: str) -&gt; Dict[str, Any]:\n        try:\n            return self._get_dynamo_item(identifier=DYNAMODB_NAMESPACE, namespace=database_name)\n        except ValueError as e:\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {database_name}\") from e\n\n    def _ensure_namespace_exists(self, database_name: str) -&gt; Dict[str, Any]:\n        return self._get_iceberg_namespace_item(database_name)\n\n    def _get_dynamo_item(self, identifier: str, namespace: str) -&gt; Dict[str, Any]:\n        try:\n            response = self.dynamodb.get_item(\n                TableName=self.dynamodb_table_name,\n                ConsistentRead=True,\n                Key={\n                    DYNAMODB_COL_IDENTIFIER: {\n                        \"S\": identifier,\n                    },\n                    DYNAMODB_COL_NAMESPACE: {\n                        \"S\": namespace,\n                    },\n                },\n            )\n            if ITEM in response:\n                return response[ITEM]\n            else:\n                raise ValueError(f\"Item not found. identifier: {identifier} - namespace: {namespace}\")\n        except self.dynamodb.exceptions.ResourceNotFoundException as e:\n            raise ValueError(f\"Item not found. identifier: {identifier} - namespace: {namespace}\") from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _put_dynamo_item(self, item: Dict[str, Any], condition_expression: str) -&gt; None:\n        try:\n            self.dynamodb.put_item(TableName=self.dynamodb_table_name, Item=item, ConditionExpression=condition_expression)\n        except self.dynamodb.exceptions.ConditionalCheckFailedException as e:\n            raise ConditionalCheckFailedException(f\"Condition expression check failed: {condition_expression} - {item}\") from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n            self.dynamodb.exceptions.ItemCollectionSizeLimitExceededException,\n            self.dynamodb.exceptions.TransactionConflictException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _delete_dynamo_item(self, namespace: str, identifier: str, condition_expression: str) -&gt; None:\n        try:\n            self.dynamodb.delete_item(\n                TableName=self.dynamodb_table_name,\n                Key={\n                    DYNAMODB_COL_IDENTIFIER: {\n                        \"S\": identifier,\n                    },\n                    DYNAMODB_COL_NAMESPACE: {\n                        \"S\": namespace,\n                    },\n                },\n                ConditionExpression=condition_expression,\n            )\n        except self.dynamodb.exceptions.ConditionalCheckFailedException as e:\n            raise ConditionalCheckFailedException(\n                f\"Condition expression check failed: {condition_expression} - {identifier}\"\n            ) from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n            self.dynamodb.exceptions.ItemCollectionSizeLimitExceededException,\n            self.dynamodb.exceptions.TransactionConflictException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _convert_dynamo_table_item_to_iceberg_table(self, dynamo_table_item: Dict[str, Any]) -&gt; Table:\n        table_dict = _convert_dynamo_item_to_regular_dict(dynamo_table_item)\n\n        for prop in [_add_property_prefix(prop) for prop in (TABLE_TYPE, METADATA_LOCATION)] + [\n            DYNAMODB_COL_IDENTIFIER,\n            DYNAMODB_COL_NAMESPACE,\n            DYNAMODB_COL_CREATED_AT,\n        ]:\n            if prop not in table_dict.keys():\n                raise NoSuchPropertyException(f\"Iceberg required property {prop} is missing: {dynamo_table_item}\")\n\n        table_type = table_dict[_add_property_prefix(TABLE_TYPE)]\n        identifier = table_dict[DYNAMODB_COL_IDENTIFIER]\n        metadata_location = table_dict[_add_property_prefix(METADATA_LOCATION)]\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        if table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {table_type}, expected {ICEBERG}: {database_name}.{table_name}\"\n            )\n\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_create_database_item(database_name=database_name, properties=properties),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_NAMESPACE})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>Optional[str]</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def create_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    location = self._resolve_table_location(location, database_name, table_name)\n    provider = load_location_provider(table_location=location, table_properties=properties)\n    metadata_location = provider.new_table_metadata_file_location()\n\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n    io = load_file_io(properties=self.properties, location=metadata_location)\n    self._write_metadata(metadata, io, metadata_location)\n\n    self._ensure_namespace_exists(database_name=database_name)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_create_table_item(\n                database_name=database_name, table_name=table_name, properties=properties, metadata_location=metadata_location\n            ),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>A Glue namespace can only be dropped if it is empty.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    A Glue namespace can only be dropped if it is empty.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    table_identifiers = self.list_tables(namespace=database_name)\n\n    if len(table_identifiers) &gt; 0:\n        raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n    try:\n        self._delete_dynamo_item(\n            namespace=database_name,\n            identifier=DYNAMODB_NAMESPACE,\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    try:\n        self._delete_dynamo_item(\n            namespace=database_name,\n            identifier=f\"{database_name}.{table_name}\",\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List top-level namespaces from the catalog.</p> <p>We do not support hierarchical namespace.</p> <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n    \"\"\"List top-level namespaces from the catalog.\n\n    We do not support hierarchical namespace.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    paginator = self.dynamodb.get_paginator(\"query\")\n\n    try:\n        page_iterator = paginator.paginate(\n            TableName=self.dynamodb_table_name,\n            ConsistentRead=True,\n            KeyConditionExpression=f\"{DYNAMODB_COL_IDENTIFIER} = :identifier\",\n            ExpressionAttributeValues={\n                \":identifier\": {\n                    \"S\": DYNAMODB_NAMESPACE,\n                }\n            },\n        )\n    except (\n        self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n        self.dynamodb.exceptions.RequestLimitExceeded,\n        self.dynamodb.exceptions.InternalServerError,\n        self.dynamodb.exceptions.ResourceNotFoundException,\n    ) as e:\n        raise GenericDynamoDbError(e.message) from e\n\n    database_identifiers = []\n    for page in page_iterator:\n        for item in page[\"Items\"]:\n            _dict = _convert_dynamo_item_to_regular_dict(item)\n            namespace_col = _dict[DYNAMODB_COL_NAMESPACE]\n            database_identifiers.append(self.identifier_to_tuple(namespace_col))\n\n    return database_identifiers\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n\n    paginator = self.dynamodb.get_paginator(\"query\")\n\n    try:\n        page_iterator = paginator.paginate(\n            TableName=self.dynamodb_table_name,\n            IndexName=DYNAMODB_NAMESPACE_GSI,\n            KeyConditionExpression=f\"{DYNAMODB_COL_NAMESPACE} = :namespace \",\n            ExpressionAttributeValues={\n                \":namespace\": {\n                    \"S\": database_name,\n                }\n            },\n        )\n    except (\n        self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n        self.dynamodb.exceptions.RequestLimitExceeded,\n        self.dynamodb.exceptions.InternalServerError,\n        self.dynamodb.exceptions.ResourceNotFoundException,\n    ) as e:\n        raise GenericDynamoDbError(e.message) from e\n\n    table_identifiers = []\n    for page in page_iterator:\n        for item in page[\"Items\"]:\n            _dict = _convert_dynamo_item_to_regular_dict(item)\n            identifier_col = _dict[DYNAMODB_COL_IDENTIFIER]\n            if identifier_col == DYNAMODB_NAMESPACE:\n                continue\n\n            table_identifiers.append(self.identifier_to_tuple(identifier_col))\n\n    return table_identifiers\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n    \"\"\"\n    Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n    namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n    return _get_namespace_properties(namespace_dict=namespace_dict)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"\n    Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    dynamo_table_item = self._get_iceberg_table_item(database_name=database_name, table_name=table_name)\n    return self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=dynamo_table_item)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>This method can only rename Iceberg tables in AWS Glue.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>Union[str, Identifier]</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>Union[str, Identifier]</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchIcebergTableError</code> <p>When from table is not a valid iceberg table.</p> <code>NoSuchPropertyException</code> <p>When from table miss some required properties.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    This method can only rename Iceberg tables in AWS Glue.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchIcebergTableError: When from table is not a valid iceberg table.\n        NoSuchPropertyException: When from table miss some required properties.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n    from_table_item = self._get_iceberg_table_item(database_name=from_database_name, table_name=from_table_name)\n\n    try:\n        # Verify that from_identifier is a valid iceberg table\n        self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=from_table_item)\n    except NoSuchPropertyException as e:\n        raise NoSuchPropertyException(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n        ) from e\n    except NoSuchIcebergTableError as e:\n        raise NoSuchIcebergTableError(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n        ) from e\n\n    self._ensure_namespace_exists(database_name=from_database_name)\n    self._ensure_namespace_exists(database_name=to_database_name)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_rename_table_item(\n                from_dynamo_table_item=from_table_item, to_database_name=to_database_name, to_table_name=to_table_name\n            ),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise TableAlreadyExistsError(f\"Table {to_database_name}.{to_table_name} already exists\") from e\n\n    try:\n        self.drop_table(from_identifier)\n    except (NoSuchTableError, GenericDynamoDbError) as e:\n        log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n        try:\n            self.drop_table(to_identifier)\n            log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n        except (NoSuchTableError, GenericDynamoDbError):\n            log_message += (\n                f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n            )\n\n        raise ValueError(log_message) from e\n\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove or update provided property keys for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier</p> required <code>removals</code> <code>Optional[Set[str]]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist\uff0c or identifier is invalid.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"\n    Remove or update provided property keys for a namespace.\n\n    Args:\n        namespace: Namespace identifier\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n    namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n    current_properties = _get_namespace_properties(namespace_dict=namespace_dict)\n\n    properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )\n\n    try:\n        self._put_dynamo_item(\n            item=_get_update_database_item(\n                namespace_item=namespace_item,\n                updated_properties=updated_properties,\n            ),\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_NAMESPACE})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb._convert_dynamo_item_to_regular_dict","title":"<code>_convert_dynamo_item_to_regular_dict(dynamo_json)</code>","text":"<p>Convert a dynamo json to a regular json.</p> <p>Example of a dynamo json: {     \"AlbumTitle\": {         \"S\": \"Songs About Life\",     },     \"Artist\": {         \"S\": \"Acme Band\",     },     \"SongTitle\": {         \"S\": \"Happy Day\",     } }</p> <p>Converted to regular json: {     \"AlbumTitle\": \"Songs About Life\",     \"Artist\": \"Acme Band\",     \"SongTitle\": \"Happy Day\" }</p> <p>Only \"S\" and \"N\" data types are supported since those are the only ones that Iceberg is utilizing.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def _convert_dynamo_item_to_regular_dict(dynamo_json: Dict[str, Any]) -&gt; Dict[str, str]:\n    \"\"\"Convert a dynamo json to a regular json.\n\n    Example of a dynamo json:\n    {\n        \"AlbumTitle\": {\n            \"S\": \"Songs About Life\",\n        },\n        \"Artist\": {\n            \"S\": \"Acme Band\",\n        },\n        \"SongTitle\": {\n            \"S\": \"Happy Day\",\n        }\n    }\n\n    Converted to regular json:\n    {\n        \"AlbumTitle\": \"Songs About Life\",\n        \"Artist\": \"Acme Band\",\n        \"SongTitle\": \"Happy Day\"\n    }\n\n    Only \"S\" and \"N\" data types are supported since those are the only ones that Iceberg is utilizing.\n    \"\"\"\n    regular_json = {}\n    for column_name, val_dict in dynamo_json.items():\n        keys = list(val_dict.keys())\n\n        if len(keys) != 1:\n            raise ValueError(f\"Expecting only 1 key: {keys}\")\n\n        data_type = keys[0]\n        if data_type not in (\"S\", \"N\"):\n            raise ValueError(\"Only S and N data types are supported.\")\n\n        values = list(val_dict.values())\n        assert len(values) == 1\n        column_value = values[0]\n        regular_json[column_name] = column_value\n\n    return regular_json\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/","title":"glue","text":""},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog","title":"<code>GlueCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>class GlueCatalog(MetastoreCatalog):\n    def __init__(self, name: str, **properties: Any):\n        super().__init__(name, **properties)\n\n        retry_mode_prop_value = get_first_property_value(properties, GLUE_RETRY_MODE)\n\n        session = boto3.Session(\n            profile_name=properties.get(GLUE_PROFILE_NAME),\n            region_name=get_first_property_value(properties, GLUE_REGION, AWS_REGION),\n            botocore_session=properties.get(BOTOCORE_SESSION),\n            aws_access_key_id=get_first_property_value(properties, GLUE_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            aws_secret_access_key=get_first_property_value(properties, GLUE_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            aws_session_token=get_first_property_value(properties, GLUE_SESSION_TOKEN, AWS_SESSION_TOKEN),\n        )\n        self.glue: GlueClient = session.client(\n            \"glue\",\n            endpoint_url=properties.get(GLUE_CATALOG_ENDPOINT),\n            config=Config(\n                retries={\n                    \"max_attempts\": properties.get(GLUE_MAX_RETRIES, MAX_RETRIES),\n                    \"mode\": retry_mode_prop_value if retry_mode_prop_value in EXISTING_RETRY_MODES else STANDARD_RETRY_MODE,\n                }\n            ),\n        )\n\n        if glue_catalog_id := properties.get(GLUE_ID):\n            _register_glue_catalog_id_with_glue_client(self.glue, glue_catalog_id)\n\n    def _convert_glue_to_iceberg(self, glue_table: TableTypeDef) -&gt; Table:\n        properties: Properties = glue_table[\"Parameters\"]\n\n        assert glue_table[\"DatabaseName\"]\n        assert glue_table[\"Parameters\"]\n        database_name = glue_table[\"DatabaseName\"]\n        table_name = glue_table[\"Name\"]\n\n        if TABLE_TYPE not in properties:\n            raise NoSuchPropertyException(\n                f\"Property {TABLE_TYPE} missing, could not determine type: {database_name}.{table_name}\"\n            )\n        glue_table_type = properties[TABLE_TYPE]\n\n        if glue_table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {glue_table_type}, expected {ICEBERG}: {database_name}.{table_name}\"\n            )\n\n        if METADATA_LOCATION not in properties:\n            raise NoSuchPropertyException(\n                f\"Table property {METADATA_LOCATION} is missing, cannot find metadata for: {database_name}.{table_name}\"\n            )\n        metadata_location = properties[METADATA_LOCATION]\n\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _create_glue_table(self, database_name: str, table_name: str, table_input: TableInputTypeDef) -&gt; None:\n        try:\n            self.glue.create_table(DatabaseName=database_name, TableInput=table_input)\n        except self.glue.exceptions.AlreadyExistsException as e:\n            raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n    def _update_glue_table(self, database_name: str, table_name: str, table_input: TableInputTypeDef, version_id: str) -&gt; None:\n        try:\n            self.glue.update_table(\n                DatabaseName=database_name,\n                TableInput=table_input,\n                SkipArchive=property_as_bool(self.properties, GLUE_SKIP_ARCHIVE, GLUE_SKIP_ARCHIVE_DEFAULT),\n                VersionId=version_id,\n            )\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name} (Glue table version {version_id})\") from e\n        except self.glue.exceptions.ConcurrentModificationException as e:\n            raise CommitFailedException(\n                f\"Cannot commit {database_name}.{table_name} because Glue detected concurrent update to table version {version_id}\"\n            ) from e\n\n    def _get_glue_table(self, database_name: str, table_name: str) -&gt; TableTypeDef:\n        try:\n            load_table_response = self.glue.get_table(DatabaseName=database_name, Name=table_name)\n            return load_table_response[\"Table\"]\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        staged_table = self._create_staged_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n        )\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n        table_input = _construct_table_input(table_name, staged_table.metadata_location, properties, staged_table.metadata)\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n        properties = EMPTY_DICT\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        table_input = _construct_table_input(table_name, metadata_location, properties, metadata)\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n        return self.load_table(identifier=identifier)\n\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n\n        current_glue_table: Optional[TableTypeDef]\n        glue_table_version_id: Optional[str]\n        current_table: Optional[Table]\n        try:\n            current_glue_table = self._get_glue_table(database_name=database_name, table_name=table_name)\n            glue_table_version_id = current_glue_table.get(\"VersionId\")\n            current_table = self._convert_glue_to_iceberg(glue_table=current_glue_table)\n        except NoSuchTableError:\n            current_glue_table = None\n            glue_table_version_id = None\n            current_table = None\n\n        updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n        if current_table and updated_staged_table.metadata == current_table.metadata:\n            # no changes, do nothing\n            return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n        self._write_metadata(\n            metadata=updated_staged_table.metadata,\n            io=updated_staged_table.io,\n            metadata_path=updated_staged_table.metadata_location,\n        )\n\n        if current_table:\n            # table exists, update the table\n            if not glue_table_version_id:\n                raise CommitFailedException(\n                    f\"Cannot commit {database_name}.{table_name} because Glue table version id is missing\"\n                )\n\n            # Pass `version_id` to implement optimistic locking: it ensures updates are rejected if concurrent\n            # modifications occur. See more details at https://iceberg.apache.org/docs/latest/aws/#optimistic-locking\n            update_table_input = _construct_table_input(\n                table_name=table_name,\n                metadata_location=updated_staged_table.metadata_location,\n                properties=updated_staged_table.properties,\n                metadata=updated_staged_table.metadata,\n                glue_table=current_glue_table,\n                prev_metadata_location=current_table.metadata_location,\n            )\n            self._update_glue_table(\n                database_name=database_name,\n                table_name=table_name,\n                table_input=update_table_input,\n                version_id=glue_table_version_id,\n            )\n        else:\n            # table does not exist, create the table\n            create_table_input = _construct_table_input(\n                table_name=table_name,\n                metadata_location=updated_staged_table.metadata_location,\n                properties=updated_staged_table.properties,\n                metadata=updated_staged_table.metadata,\n            )\n            self._create_glue_table(database_name=database_name, table_name=table_name, table_input=create_table_input)\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        return self._convert_glue_to_iceberg(self._get_glue_table(database_name=database_name, table_name=table_name))\n\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        try:\n            self.glue.delete_table(DatabaseName=database_name, Name=table_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        This method can only rename Iceberg tables in AWS Glue.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchIcebergTableError: When from table is not a valid iceberg table.\n            NoSuchPropertyException: When from table miss some required properties.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n        try:\n            get_table_response = self.glue.get_table(DatabaseName=from_database_name, Name=from_table_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {from_database_name}.{from_table_name}\") from e\n\n        glue_table = get_table_response[\"Table\"]\n\n        try:\n            # verify that from_identifier is a valid iceberg table\n            self._convert_glue_to_iceberg(glue_table=glue_table)\n        except NoSuchPropertyException as e:\n            raise NoSuchPropertyException(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n            ) from e\n        except NoSuchIcebergTableError as e:\n            raise NoSuchIcebergTableError(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n            ) from e\n\n        rename_table_input = _construct_rename_table_input(to_table_name=to_table_name, glue_table=glue_table)\n        self._create_glue_table(database_name=to_database_name, table_name=to_table_name, table_input=rename_table_input)\n\n        try:\n            self.drop_table(from_identifier)\n        except Exception as e:\n            log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n            try:\n                self.drop_table(to_identifier)\n                log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n            except NoSuchTableError:\n                log_message += (\n                    f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n                )\n\n            raise ValueError(log_message) from e\n\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n        try:\n            self.glue.create_database(DatabaseInput=_construct_database_input(database_name, properties))\n        except self.glue.exceptions.AlreadyExistsException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        A Glue namespace can only be dropped if it is empty.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            table_list = self.list_tables(namespace=database_name)\n        except NoSuchNamespaceError as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n        if len(table_list) &gt; 0:\n            raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n        self.glue.delete_database(Name=database_name)\n\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        table_list: List[TableTypeDef] = []\n        next_token: Optional[str] = None\n        try:\n            while True:\n                table_list_response = (\n                    self.glue.get_tables(DatabaseName=database_name)\n                    if not next_token\n                    else self.glue.get_tables(DatabaseName=database_name, NextToken=next_token)\n                )\n                table_list.extend(table_list_response[\"TableList\"])\n                next_token = table_list_response.get(\"NextToken\")\n                if not next_token:\n                    break\n\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n        return [(database_name, table[\"Name\"]) for table in table_list if self.__is_iceberg_table(table)]\n\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        database_list: List[DatabaseTypeDef] = []\n        next_token: Optional[str] = None\n\n        while True:\n            databases_response = self.glue.get_databases() if not next_token else self.glue.get_databases(NextToken=next_token)\n            database_list.extend(databases_response[\"DatabaseList\"])\n            next_token = databases_response.get(\"NextToken\")\n            if not next_token:\n                break\n\n        return [self.identifier_to_tuple(database[\"Name\"]) for database in database_list]\n\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            database_response = self.glue.get_database(Name=database_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n        except self.glue.exceptions.InvalidInputException as e:\n            raise NoSuchNamespaceError(f\"Invalid input for namespace {database_name}\") from e\n\n        database = database_response[\"Database\"]\n\n        properties = dict(database.get(\"Parameters\", {}))\n        if \"LocationUri\" in database:\n            properties[\"location\"] = database[\"LocationUri\"]\n        if \"Description\" in database:\n            properties[\"Description\"] = database[\"Description\"]\n\n        return properties\n\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and updates properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        current_properties = self.load_namespace_properties(namespace=namespace)\n        properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )\n\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        self.glue.update_database(Name=database_name, DatabaseInput=_construct_database_input(database_name, updated_properties))\n\n        return properties_update_summary\n\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    @staticmethod\n    def __is_iceberg_table(table: TableTypeDef) -&gt; bool:\n        return table.get(\"Parameters\", {}).get(TABLE_TYPE, \"\").lower() == ICEBERG\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n\n    current_glue_table: Optional[TableTypeDef]\n    glue_table_version_id: Optional[str]\n    current_table: Optional[Table]\n    try:\n        current_glue_table = self._get_glue_table(database_name=database_name, table_name=table_name)\n        glue_table_version_id = current_glue_table.get(\"VersionId\")\n        current_table = self._convert_glue_to_iceberg(glue_table=current_glue_table)\n    except NoSuchTableError:\n        current_glue_table = None\n        glue_table_version_id = None\n        current_table = None\n\n    updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n    if current_table and updated_staged_table.metadata == current_table.metadata:\n        # no changes, do nothing\n        return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n    self._write_metadata(\n        metadata=updated_staged_table.metadata,\n        io=updated_staged_table.io,\n        metadata_path=updated_staged_table.metadata_location,\n    )\n\n    if current_table:\n        # table exists, update the table\n        if not glue_table_version_id:\n            raise CommitFailedException(\n                f\"Cannot commit {database_name}.{table_name} because Glue table version id is missing\"\n            )\n\n        # Pass `version_id` to implement optimistic locking: it ensures updates are rejected if concurrent\n        # modifications occur. See more details at https://iceberg.apache.org/docs/latest/aws/#optimistic-locking\n        update_table_input = _construct_table_input(\n            table_name=table_name,\n            metadata_location=updated_staged_table.metadata_location,\n            properties=updated_staged_table.properties,\n            metadata=updated_staged_table.metadata,\n            glue_table=current_glue_table,\n            prev_metadata_location=current_table.metadata_location,\n        )\n        self._update_glue_table(\n            database_name=database_name,\n            table_name=table_name,\n            table_input=update_table_input,\n            version_id=glue_table_version_id,\n        )\n    else:\n        # table does not exist, create the table\n        create_table_input = _construct_table_input(\n            table_name=table_name,\n            metadata_location=updated_staged_table.metadata_location,\n            properties=updated_staged_table.properties,\n            metadata=updated_staged_table.metadata,\n        )\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=create_table_input)\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n    try:\n        self.glue.create_database(DatabaseInput=_construct_database_input(database_name, properties))\n    except self.glue.exceptions.AlreadyExistsException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>Optional[str]</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def create_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    staged_table = self._create_staged_table(\n        identifier=identifier,\n        schema=schema,\n        location=location,\n        partition_spec=partition_spec,\n        sort_order=sort_order,\n        properties=properties,\n    )\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n    table_input = _construct_table_input(table_name, staged_table.metadata_location, properties, staged_table.metadata)\n    self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>A Glue namespace can only be dropped if it is empty.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    A Glue namespace can only be dropped if it is empty.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        table_list = self.list_tables(namespace=database_name)\n    except NoSuchNamespaceError as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n    if len(table_list) &gt; 0:\n        raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n    self.glue.delete_database(Name=database_name)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    try:\n        self.glue.delete_table(DatabaseName=database_name, Name=table_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    database_list: List[DatabaseTypeDef] = []\n    next_token: Optional[str] = None\n\n    while True:\n        databases_response = self.glue.get_databases() if not next_token else self.glue.get_databases(NextToken=next_token)\n        database_list.extend(databases_response[\"DatabaseList\"])\n        next_token = databases_response.get(\"NextToken\")\n        if not next_token:\n            break\n\n    return [self.identifier_to_tuple(database[\"Name\"]) for database in database_list]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    table_list: List[TableTypeDef] = []\n    next_token: Optional[str] = None\n    try:\n        while True:\n            table_list_response = (\n                self.glue.get_tables(DatabaseName=database_name)\n                if not next_token\n                else self.glue.get_tables(DatabaseName=database_name, NextToken=next_token)\n            )\n            table_list.extend(table_list_response[\"TableList\"])\n            next_token = table_list_response.get(\"NextToken\")\n            if not next_token:\n                break\n\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n    return [(database_name, table[\"Name\"]) for table in table_list if self.__is_iceberg_table(table)]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        database_response = self.glue.get_database(Name=database_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n    except self.glue.exceptions.InvalidInputException as e:\n        raise NoSuchNamespaceError(f\"Invalid input for namespace {database_name}\") from e\n\n    database = database_response[\"Database\"]\n\n    properties = dict(database.get(\"Parameters\", {}))\n    if \"LocationUri\" in database:\n        properties[\"location\"] = database[\"LocationUri\"]\n    if \"Description\" in database:\n        properties[\"Description\"] = database[\"Description\"]\n\n    return properties\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    return self._convert_glue_to_iceberg(self._get_glue_table(database_name=database_name, table_name=table_name))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n    properties = EMPTY_DICT\n    io = self._load_file_io(location=metadata_location)\n    file = io.new_input(metadata_location)\n    metadata = FromInputFile.table_metadata(file)\n    table_input = _construct_table_input(table_name, metadata_location, properties, metadata)\n    self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>This method can only rename Iceberg tables in AWS Glue.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>Union[str, Identifier]</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>Union[str, Identifier]</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchIcebergTableError</code> <p>When from table is not a valid iceberg table.</p> <code>NoSuchPropertyException</code> <p>When from table miss some required properties.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    This method can only rename Iceberg tables in AWS Glue.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchIcebergTableError: When from table is not a valid iceberg table.\n        NoSuchPropertyException: When from table miss some required properties.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n    try:\n        get_table_response = self.glue.get_table(DatabaseName=from_database_name, Name=from_table_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchTableError(f\"Table does not exist: {from_database_name}.{from_table_name}\") from e\n\n    glue_table = get_table_response[\"Table\"]\n\n    try:\n        # verify that from_identifier is a valid iceberg table\n        self._convert_glue_to_iceberg(glue_table=glue_table)\n    except NoSuchPropertyException as e:\n        raise NoSuchPropertyException(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n        ) from e\n    except NoSuchIcebergTableError as e:\n        raise NoSuchIcebergTableError(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n        ) from e\n\n    rename_table_input = _construct_rename_table_input(to_table_name=to_table_name, glue_table=glue_table)\n    self._create_glue_table(database_name=to_database_name, table_name=to_table_name, table_input=rename_table_input)\n\n    try:\n        self.drop_table(from_identifier)\n    except Exception as e:\n        log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n        try:\n            self.drop_table(to_identifier)\n            log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n        except NoSuchTableError:\n            log_message += (\n                f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n            )\n\n        raise ValueError(log_message) from e\n\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and updates properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Optional[Set[str]]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist\uff0c or identifier is invalid.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and updates properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    current_properties = self.load_namespace_properties(namespace=namespace)\n    properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )\n\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    self.glue.update_database(Name=database_name, DatabaseInput=_construct_database_input(database_name, updated_properties))\n\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue._register_glue_catalog_id_with_glue_client","title":"<code>_register_glue_catalog_id_with_glue_client(glue, glue_catalog_id)</code>","text":"<p>Register the Glue Catalog ID (AWS Account ID) as a parameter on all Glue client methods.</p> <p>It's more ergonomic to do this than to pass the CatalogId as a parameter to every client call since it's an optional parameter and boto3 does not support 'None' values for missing parameters.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def _register_glue_catalog_id_with_glue_client(glue: GlueClient, glue_catalog_id: str) -&gt; None:\n    \"\"\"\n    Register the Glue Catalog ID (AWS Account ID) as a parameter on all Glue client methods.\n\n    It's more ergonomic to do this than to pass the CatalogId as a parameter to every client call since it's an optional\n    parameter and boto3 does not support 'None' values for missing parameters.\n    \"\"\"\n    event_system = glue.meta.events\n\n    def add_glue_catalog_id(params: Dict[str, str], **kwargs: Any) -&gt; None:\n        if \"CatalogId\" not in params:\n            params[\"CatalogId\"] = glue_catalog_id\n\n    event_system.register(\"provide-client-params.glue\", add_glue_catalog_id)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/","title":"hive","text":""},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog","title":"<code>HiveCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>class HiveCatalog(MetastoreCatalog):\n    _client: _HiveClient\n\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n        self._client = self._create_hive_client(properties)\n\n        self._lock_check_min_wait_time = property_as_float(properties, LOCK_CHECK_MIN_WAIT_TIME, DEFAULT_LOCK_CHECK_MIN_WAIT_TIME)\n        self._lock_check_max_wait_time = property_as_float(properties, LOCK_CHECK_MAX_WAIT_TIME, DEFAULT_LOCK_CHECK_MAX_WAIT_TIME)\n        self._lock_check_retries = property_as_float(\n            properties,\n            LOCK_CHECK_RETRIES,\n            DEFAULT_LOCK_CHECK_RETRIES,\n        )\n\n    @staticmethod\n    def _create_hive_client(properties: Dict[str, str]) -&gt; _HiveClient:\n        last_exception = None\n        for uri in properties[\"uri\"].split(\",\"):\n            try:\n                return _HiveClient(\n                    uri,\n                    properties.get(\"ugi\"),\n                    property_as_bool(properties, HIVE_KERBEROS_AUTH, HIVE_KERBEROS_AUTH_DEFAULT),\n                )\n            except BaseException as e:\n                last_exception = e\n        if last_exception is not None:\n            raise last_exception\n        else:\n            raise ValueError(f\"Unable to connect to hive using uri: {properties['uri']}\")\n\n    def _convert_hive_into_iceberg(self, table: HiveTable) -&gt; Table:\n        properties: Dict[str, str] = table.parameters\n        if TABLE_TYPE not in properties:\n            raise NoSuchPropertyException(\n                f\"Property table_type missing, could not determine type: {table.dbName}.{table.tableName}\"\n            )\n\n        table_type = properties[TABLE_TYPE]\n        if table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {table_type}, expected {ICEBERG}: {table.dbName}.{table.tableName}\"\n            )\n\n        if prop_metadata_location := properties.get(METADATA_LOCATION):\n            metadata_location = prop_metadata_location\n        else:\n            raise NoSuchPropertyException(f\"Table property {METADATA_LOCATION} is missing\")\n\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(table.dbName, table.tableName),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _convert_iceberg_into_hive(self, table: Table) -&gt; HiveTable:\n        identifier_tuple = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(identifier_tuple, NoSuchTableError)\n        current_time_millis = int(time.time() * 1000)\n\n        return HiveTable(\n            dbName=database_name,\n            tableName=table_name,\n            owner=table.properties[OWNER] if table.properties and OWNER in table.properties else getpass.getuser(),\n            createTime=current_time_millis // 1000,\n            lastAccessTime=current_time_millis // 1000,\n            sd=_construct_hive_storage_descriptor(\n                table.schema(),\n                table.location(),\n                property_as_bool(self.properties, HIVE2_COMPATIBLE, HIVE2_COMPATIBLE_DEFAULT),\n            ),\n            tableType=EXTERNAL_TABLE,\n            parameters=_construct_parameters(table.metadata_location),\n        )\n\n    def _create_hive_table(self, open_client: Client, hive_table: HiveTable) -&gt; None:\n        try:\n            open_client.create_table(hive_table)\n        except AlreadyExistsException as e:\n            raise TableAlreadyExistsError(f\"Table {hive_table.dbName}.{hive_table.tableName} already exists\") from e\n\n    def _get_hive_table(self, open_client: Client, database_name: str, table_name: str) -&gt; HiveTable:\n        try:\n            return open_client.get_table(dbname=database_name, tbl_name=table_name)\n        except NoSuchObjectException as e:\n            raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid.\n        \"\"\"\n        properties = {**DEFAULT_PROPERTIES, **properties}\n        staged_table = self._create_staged_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n        )\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n        tbl = self._convert_iceberg_into_hive(staged_table)\n\n        with self._client as open_client:\n            self._create_hive_table(open_client, tbl)\n            hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n        io = self._load_file_io(location=metadata_location)\n        metadata_file = io.new_input(metadata_location)\n        staged_table = StagedTable(\n            identifier=(database_name, table_name),\n            metadata=FromInputFile.table_metadata(metadata_file),\n            metadata_location=metadata_location,\n            io=io,\n            catalog=self,\n        )\n        tbl = self._convert_iceberg_into_hive(staged_table)\n        with self._client as open_client:\n            self._create_hive_table(open_client, tbl)\n            hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    def _create_lock_request(self, database_name: str, table_name: str) -&gt; LockRequest:\n        lock_component: LockComponent = LockComponent(\n            level=LockLevel.TABLE, type=LockType.EXCLUSIVE, dbname=database_name, tablename=table_name, isTransactional=True\n        )\n\n        lock_request: LockRequest = LockRequest(component=[lock_component], user=getpass.getuser(), hostname=socket.gethostname())\n\n        return lock_request\n\n    def _wait_for_lock(self, database_name: str, table_name: str, lockid: int, open_client: Client) -&gt; LockResponse:\n        @retry(\n            retry=retry_if_exception_type(WaitingForLockException),\n            wait=wait_exponential(multiplier=2, min=self._lock_check_min_wait_time, max=self._lock_check_max_wait_time),\n            stop=stop_after_attempt(self._lock_check_retries),\n            reraise=True,\n        )\n        def _do_wait_for_lock() -&gt; LockResponse:\n            response: LockResponse = open_client.check_lock(CheckLockRequest(lockid=lockid))\n            if response.state == LockState.ACQUIRED:\n                return response\n            elif response.state == LockState.WAITING:\n                msg = f\"Wait on lock for {database_name}.{table_name}\"\n                logger.warning(msg)\n                raise WaitingForLockException(msg)\n            else:\n                raise CommitFailedException(f\"Failed to check lock for {database_name}.{table_name}, state: {response.state}\")\n\n        return _do_wait_for_lock()\n\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n        # commit to hive\n        # https://github.com/apache/hive/blob/master/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1232\n        with self._client as open_client:\n            lock: LockResponse = open_client.lock(self._create_lock_request(database_name, table_name))\n\n            try:\n                if lock.state != LockState.ACQUIRED:\n                    if lock.state == LockState.WAITING:\n                        self._wait_for_lock(database_name, table_name, lock.lockid, open_client)\n                    else:\n                        raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\")\n\n                hive_table: Optional[HiveTable]\n                current_table: Optional[Table]\n                try:\n                    hive_table = self._get_hive_table(open_client, database_name, table_name)\n                    current_table = self._convert_hive_into_iceberg(hive_table)\n                except NoSuchTableError:\n                    hive_table = None\n                    current_table = None\n\n                updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n                if current_table and updated_staged_table.metadata == current_table.metadata:\n                    # no changes, do nothing\n                    return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n                self._write_metadata(\n                    metadata=updated_staged_table.metadata,\n                    io=updated_staged_table.io,\n                    metadata_path=updated_staged_table.metadata_location,\n                )\n\n                if hive_table and current_table:\n                    # Table exists, update it.\n                    hive_table.parameters = _construct_parameters(\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=current_table.metadata_location,\n                    )\n                    open_client.alter_table(dbname=database_name, tbl_name=table_name, new_tbl=hive_table)\n                else:\n                    # Table does not exist, create it.\n                    hive_table = self._convert_iceberg_into_hive(\n                        StagedTable(\n                            identifier=(database_name, table_name),\n                            metadata=updated_staged_table.metadata,\n                            metadata_location=updated_staged_table.metadata_location,\n                            io=updated_staged_table.io,\n                            catalog=self,\n                        )\n                    )\n                    self._create_hive_table(open_client, hive_table)\n            except WaitingForLockException as e:\n                raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\") from e\n            finally:\n                open_client.unlock(UnlockRequest(lockid=lock.lockid))\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Load the table's metadata and return the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        with self._client as open_client:\n            hive_table = self._get_hive_table(open_client, database_name, table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        try:\n            with self._client as open_client:\n                open_client.drop_table(dbname=database_name, name=table_name, deleteData=False)\n        except NoSuchObjectException as e:\n            # When the namespace doesn't exist, it throws the same error\n            raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n\n    def purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        # This requires to traverse the reachability set, and drop all the data files.\n        raise NotImplementedError(\"Not yet implemented\")\n\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n        try:\n            with self._client as open_client:\n                tbl = open_client.get_table(dbname=from_database_name, tbl_name=from_table_name)\n                tbl.dbName = to_database_name\n                tbl.tableName = to_table_name\n                open_client.alter_table(dbname=from_database_name, tbl_name=from_table_name, new_tbl=tbl)\n        except NoSuchObjectException as e:\n            raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n        except InvalidOperationException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {to_database_name}\") from e\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n        hive_database = HiveDatabase(name=database_name, parameters=properties)\n\n        try:\n            with self._client as open_client:\n                open_client.create_database(_annotate_namespace(hive_database, properties))\n        except AlreadyExistsException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            with self._client as open_client:\n                open_client.drop_database(database_name, deleteData=False, cascade=False)\n        except InvalidOperationException as e:\n            raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\") from e\n        except MetaException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        When the database doesn't exist, it will just return an empty list.\n\n        Args:\n            namespace: Database to list.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        with self._client as open_client:\n            return [\n                (database_name, table.tableName)\n                for table in open_client.get_table_objects_by_name(\n                    dbname=database_name, tbl_names=open_client.get_all_tables(db_name=database_name)\n                )\n                if table.parameters.get(TABLE_TYPE, \"\").lower() == ICEBERG\n            ]\n\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        with self._client as open_client:\n            return list(map(self.identifier_to_tuple, open_client.get_all_databases()))\n\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            with self._client as open_client:\n                database = open_client.get_database(name=database_name)\n                properties = database.parameters\n                properties[LOCATION] = database.locationUri\n                if comment := database.description:\n                    properties[COMMENT] = comment\n                return properties\n        except NoSuchObjectException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and update properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        self._check_for_overlap(updates=updates, removals=removals)\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        with self._client as open_client:\n            try:\n                database = open_client.get_database(database_name)\n                parameters = database.parameters\n            except NoSuchObjectException as e:\n                raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n            removed: Set[str] = set()\n            updated: Set[str] = set()\n\n            if removals:\n                for key in removals:\n                    if key in parameters:\n                        parameters[key] = None\n                        removed.add(key)\n            if updates:\n                for key, value in updates.items():\n                    parameters[key] = value\n                    updated.add(key)\n\n            open_client.alter_database(database_name, _annotate_namespace(database, parameters))\n\n        expected_to_change = (removals or set()).difference(removed)\n\n        return PropertiesUpdateSummary(removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change))\n\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n    # commit to hive\n    # https://github.com/apache/hive/blob/master/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1232\n    with self._client as open_client:\n        lock: LockResponse = open_client.lock(self._create_lock_request(database_name, table_name))\n\n        try:\n            if lock.state != LockState.ACQUIRED:\n                if lock.state == LockState.WAITING:\n                    self._wait_for_lock(database_name, table_name, lock.lockid, open_client)\n                else:\n                    raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\")\n\n            hive_table: Optional[HiveTable]\n            current_table: Optional[Table]\n            try:\n                hive_table = self._get_hive_table(open_client, database_name, table_name)\n                current_table = self._convert_hive_into_iceberg(hive_table)\n            except NoSuchTableError:\n                hive_table = None\n                current_table = None\n\n            updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n            if current_table and updated_staged_table.metadata == current_table.metadata:\n                # no changes, do nothing\n                return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n            self._write_metadata(\n                metadata=updated_staged_table.metadata,\n                io=updated_staged_table.io,\n                metadata_path=updated_staged_table.metadata_location,\n            )\n\n            if hive_table and current_table:\n                # Table exists, update it.\n                hive_table.parameters = _construct_parameters(\n                    metadata_location=updated_staged_table.metadata_location,\n                    previous_metadata_location=current_table.metadata_location,\n                )\n                open_client.alter_table(dbname=database_name, tbl_name=table_name, new_tbl=hive_table)\n            else:\n                # Table does not exist, create it.\n                hive_table = self._convert_iceberg_into_hive(\n                    StagedTable(\n                        identifier=(database_name, table_name),\n                        metadata=updated_staged_table.metadata,\n                        metadata_location=updated_staged_table.metadata_location,\n                        io=updated_staged_table.io,\n                        catalog=self,\n                    )\n                )\n                self._create_hive_table(open_client, hive_table)\n        except WaitingForLockException as e:\n            raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\") from e\n        finally:\n            open_client.unlock(UnlockRequest(lockid=lock.lockid))\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n    hive_database = HiveDatabase(name=database_name, parameters=properties)\n\n    try:\n        with self._client as open_client:\n            open_client.create_database(_annotate_namespace(hive_database, properties))\n    except AlreadyExistsException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>Optional[str]</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def create_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid.\n    \"\"\"\n    properties = {**DEFAULT_PROPERTIES, **properties}\n    staged_table = self._create_staged_table(\n        identifier=identifier,\n        schema=schema,\n        location=location,\n        partition_spec=partition_spec,\n        sort_order=sort_order,\n        properties=properties,\n    )\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n    tbl = self._convert_iceberg_into_hive(staged_table)\n\n    with self._client as open_client:\n        self._create_hive_table(open_client, tbl)\n        hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        with self._client as open_client:\n            open_client.drop_database(database_name, deleteData=False, cascade=False)\n    except InvalidOperationException as e:\n        raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\") from e\n    except MetaException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    try:\n        with self._client as open_client:\n            open_client.drop_table(dbname=database_name, name=table_name, deleteData=False)\n    except NoSuchObjectException as e:\n        # When the namespace doesn't exist, it throws the same error\n        raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    with self._client as open_client:\n        return list(map(self.identifier_to_tuple, open_client.get_all_databases()))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>When the database doesn't exist, it will just return an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Database to list.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    When the database doesn't exist, it will just return an empty list.\n\n    Args:\n        namespace: Database to list.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    with self._client as open_client:\n        return [\n            (database_name, table.tableName)\n            for table in open_client.get_table_objects_by_name(\n                dbname=database_name, tbl_names=open_client.get_all_tables(db_name=database_name)\n            )\n            if table.parameters.get(TABLE_TYPE, \"\").lower() == ICEBERG\n        ]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        with self._client as open_client:\n            database = open_client.get_database(name=database_name)\n            properties = database.parameters\n            properties[LOCATION] = database.locationUri\n            if comment := database.description:\n                properties[COMMENT] = comment\n            return properties\n    except NoSuchObjectException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and return the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Load the table's metadata and return the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    with self._client as open_client:\n        hive_table = self._get_hive_table(open_client, database_name, table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n    io = self._load_file_io(location=metadata_location)\n    metadata_file = io.new_input(metadata_location)\n    staged_table = StagedTable(\n        identifier=(database_name, table_name),\n        metadata=FromInputFile.table_metadata(metadata_file),\n        metadata_location=metadata_location,\n        io=io,\n        catalog=self,\n    )\n    tbl = self._convert_iceberg_into_hive(staged_table)\n    with self._client as open_client:\n        self._create_hive_table(open_client, tbl)\n        hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>Union[str, Identifier]</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>Union[str, Identifier]</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n    try:\n        with self._client as open_client:\n            tbl = open_client.get_table(dbname=from_database_name, tbl_name=from_table_name)\n            tbl.dbName = to_database_name\n            tbl.tableName = to_table_name\n            open_client.alter_table(dbname=from_database_name, tbl_name=from_table_name, new_tbl=tbl)\n    except NoSuchObjectException as e:\n        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n    except InvalidOperationException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {to_database_name}\") from e\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and update properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Union[str, Identifier]</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Optional[Set[str]]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and update properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    self._check_for_overlap(updates=updates, removals=removals)\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    with self._client as open_client:\n        try:\n            database = open_client.get_database(database_name)\n            parameters = database.parameters\n        except NoSuchObjectException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n        removed: Set[str] = set()\n        updated: Set[str] = set()\n\n        if removals:\n            for key in removals:\n                if key in parameters:\n                    parameters[key] = None\n                    removed.add(key)\n        if updates:\n            for key, value in updates.items():\n                parameters[key] = value\n                updated.add(key)\n\n        open_client.alter_database(database_name, _annotate_namespace(database, parameters))\n\n    expected_to_change = (removals or set()).difference(removed)\n\n    return PropertiesUpdateSummary(removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive._HiveClient","title":"<code>_HiveClient</code>","text":"<p>Helper class to nicely open and close the transport.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>class _HiveClient:\n    \"\"\"Helper class to nicely open and close the transport.\"\"\"\n\n    _transport: TTransport\n    _client: Client\n    _ugi: Optional[List[str]]\n\n    def __init__(self, uri: str, ugi: Optional[str] = None, kerberos_auth: Optional[bool] = HIVE_KERBEROS_AUTH_DEFAULT):\n        self._uri = uri\n        self._kerberos_auth = kerberos_auth\n        self._ugi = ugi.split(\":\") if ugi else None\n\n        self._init_thrift_client()\n\n    def _init_thrift_client(self) -&gt; None:\n        url_parts = urlparse(self._uri)\n\n        socket = TSocket.TSocket(url_parts.hostname, url_parts.port)\n\n        if not self._kerberos_auth:\n            self._transport = TTransport.TBufferedTransport(socket)\n        else:\n            self._transport = TTransport.TSaslClientTransport(socket, host=url_parts.hostname, service=\"hive\")\n\n        protocol = TBinaryProtocol.TBinaryProtocol(self._transport)\n\n        self._client = Client(protocol)\n\n    def __enter__(self) -&gt; Client:\n        self._transport.open()\n        if self._ugi:\n            self._client.set_ugi(*self._ugi)\n        return self._client\n\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        self._transport.close()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/memory/","title":"memory","text":""},{"location":"reference/pyiceberg/catalog/memory/#pyiceberg.catalog.memory.InMemoryCatalog","title":"<code>InMemoryCatalog</code>","text":"<p>               Bases: <code>SqlCatalog</code></p> <p>An in-memory catalog implementation that uses SqlCatalog with SQLite in-memory database.</p> <p>This is useful for test, demo, and playground but not in production as it does not support concurrent access.</p> Source code in <code>pyiceberg/catalog/memory.py</code> <pre><code>class InMemoryCatalog(SqlCatalog):\n    \"\"\"\n    An in-memory catalog implementation that uses SqlCatalog with SQLite in-memory database.\n\n    This is useful for test, demo, and playground but not in production as it does not support concurrent access.\n    \"\"\"\n\n    def __init__(self, name: str, warehouse: str = \"file:///tmp/iceberg/warehouse\", **kwargs: str) -&gt; None:\n        self._warehouse_location = warehouse\n        if \"uri\" not in kwargs:\n            kwargs[\"uri\"] = \"sqlite:///:memory:\"\n        super().__init__(name=name, warehouse=warehouse, **kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/noop/","title":"noop","text":""},{"location":"reference/pyiceberg/catalog/noop/#pyiceberg.catalog.noop.NoopCatalog","title":"<code>NoopCatalog</code>","text":"<p>               Bases: <code>Catalog</code></p> Source code in <code>pyiceberg/catalog/noop.py</code> <pre><code>class NoopCatalog(Catalog):\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        raise NotImplementedError\n\n    def create_table_transaction(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        raise NotImplementedError\n\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        raise NotImplementedError\n\n    def table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n\n    def purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        raise NotImplementedError\n\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        raise NotImplementedError\n\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        raise NotImplementedError\n\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        raise NotImplementedError\n\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        raise NotImplementedError\n\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/noop/#pyiceberg.catalog.noop.NoopCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/noop.py</code> <pre><code>def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/","title":"rest","text":""},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog","title":"<code>RestCatalog</code>","text":"<p>               Bases: <code>Catalog</code></p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>class RestCatalog(Catalog):\n    uri: str\n    _session: Session\n\n    def __init__(self, name: str, **properties: str):\n        \"\"\"Rest Catalog.\n\n        You either need to provide a client_id and client_secret, or an already valid token.\n\n        Args:\n            name: Name to identify the catalog.\n            properties: Properties that are passed along to the configuration.\n        \"\"\"\n        super().__init__(name, **properties)\n        self.uri = properties[URI]\n        self._fetch_config()\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; Session:\n        \"\"\"Create a request session with provided catalog configuration.\"\"\"\n        session = Session()\n\n        # Sets the client side and server side SSL cert verification, if provided as properties.\n        if ssl_config := self.properties.get(SSL):\n            if ssl_ca_bundle := ssl_config.get(CA_BUNDLE):\n                session.verify = ssl_ca_bundle\n            if ssl_client := ssl_config.get(CLIENT):\n                if all(k in ssl_client for k in (CERT, KEY)):\n                    session.cert = (ssl_client[CERT], ssl_client[KEY])\n                elif ssl_client_cert := ssl_client.get(CERT):\n                    session.cert = ssl_client_cert\n\n        self._refresh_token(session, self.properties.get(TOKEN))\n\n        # Set HTTP headers\n        self._config_headers(session)\n\n        # Configure SigV4 Request Signing\n        if property_as_bool(self.properties, SIGV4, False):\n            self._init_sigv4(session)\n\n        return session\n\n    def _check_valid_namespace_identifier(self, identifier: Union[str, Identifier]) -&gt; Identifier:\n        \"\"\"Check if the identifier has at least one element.\"\"\"\n        identifier_tuple = Catalog.identifier_to_tuple(identifier)\n        if len(identifier_tuple) &lt; 1:\n            raise NoSuchNamespaceError(f\"Empty namespace identifier: {identifier}\")\n        return identifier_tuple\n\n    def url(self, endpoint: str, prefixed: bool = True, **kwargs: Any) -&gt; str:\n        \"\"\"Construct the endpoint.\n\n        Args:\n            endpoint: Resource identifier that points to the REST catalog.\n            prefixed: If the prefix return by the config needs to be appended.\n\n        Returns:\n            The base url of the rest catalog.\n        \"\"\"\n        url = self.uri\n        url = url + \"v1/\" if url.endswith(\"/\") else url + \"/v1/\"\n\n        if prefixed:\n            url += self.properties.get(PREFIX, \"\")\n            url = url if url.endswith(\"/\") else url + \"/\"\n\n        return url + endpoint.format(**kwargs)\n\n    @property\n    def auth_url(self) -&gt; str:\n        self._warn_oauth_tokens_deprecation()\n\n        if url := self.properties.get(OAUTH2_SERVER_URI):\n            return url\n        else:\n            return self.url(Endpoints.get_token, prefixed=False)\n\n    def _warn_oauth_tokens_deprecation(self) -&gt; None:\n        has_oauth_server_uri = OAUTH2_SERVER_URI in self.properties\n        has_credential = CREDENTIAL in self.properties\n        has_init_token = TOKEN in self.properties\n        has_sigv4_enabled = property_as_bool(self.properties, SIGV4, False)\n\n        if not has_oauth_server_uri and (has_init_token or has_credential) and not has_sigv4_enabled:\n            deprecation_message(\n                deprecated_in=\"0.8.0\",\n                removed_in=\"1.0.0\",\n                help_message=\"Iceberg REST client is missing the OAuth2 server URI \"\n                f\"configuration and defaults to {self.uri}{Endpoints.get_token}. \"\n                \"This automatic fallback will be removed in a future Iceberg release.\"\n                f\"It is recommended to configure the OAuth2 endpoint using the '{OAUTH2_SERVER_URI}'\"\n                \"property to be prepared. This warning will disappear if the OAuth2\"\n                \"endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\",\n            )\n\n    def _extract_optional_oauth_params(self) -&gt; Dict[str, str]:\n        optional_oauth_param = {SCOPE: self.properties.get(SCOPE) or CATALOG_SCOPE}\n        set_of_optional_params = {AUDIENCE, RESOURCE}\n        for param in set_of_optional_params:\n            if param_value := self.properties.get(param):\n                optional_oauth_param[param] = param_value\n\n        return optional_oauth_param\n\n    def _fetch_access_token(self, session: Session, credential: str) -&gt; str:\n        if SEMICOLON in credential:\n            client_id, client_secret = credential.split(SEMICOLON)\n        else:\n            client_id, client_secret = None, credential\n\n        data = {GRANT_TYPE: CLIENT_CREDENTIALS, CLIENT_ID: client_id, CLIENT_SECRET: client_secret}\n\n        optional_oauth_params = self._extract_optional_oauth_params()\n        data.update(optional_oauth_params)\n\n        response = session.post(\n            url=self.auth_url, data=data, headers={**session.headers, \"Content-type\": \"application/x-www-form-urlencoded\"}\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {400: OAuthError, 401: OAuthError})\n\n        return TokenResponse(**response.json()).access_token\n\n    def _fetch_config(self) -&gt; None:\n        params = {}\n        if warehouse_location := self.properties.get(WAREHOUSE_LOCATION):\n            params[WAREHOUSE_LOCATION] = warehouse_location\n\n        with self._create_session() as session:\n            response = session.get(self.url(Endpoints.get_config, prefixed=False), params=params)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {})\n        config_response = ConfigResponse(**response.json())\n\n        config = config_response.defaults\n        config.update(self.properties)\n        config.update(config_response.overrides)\n        self.properties = config\n\n        # Update URI based on overrides\n        self.uri = config[URI]\n\n    def _identifier_to_validated_tuple(self, identifier: Union[str, Identifier]) -&gt; Identifier:\n        identifier_tuple = self.identifier_to_tuple(identifier)\n        if len(identifier_tuple) &lt;= 1:\n            raise NoSuchIdentifierError(f\"Missing namespace or invalid identifier: {'.'.join(identifier_tuple)}\")\n        return identifier_tuple\n\n    def _split_identifier_for_path(\n        self, identifier: Union[str, Identifier, TableIdentifier], kind: IdentifierKind = IdentifierKind.TABLE\n    ) -&gt; Properties:\n        if isinstance(identifier, TableIdentifier):\n            return {\"namespace\": NAMESPACE_SEPARATOR.join(identifier.namespace.root), kind.value: identifier.name}\n        identifier_tuple = self._identifier_to_validated_tuple(identifier)\n\n        return {\"namespace\": NAMESPACE_SEPARATOR.join(identifier_tuple[:-1]), kind.value: identifier_tuple[-1]}\n\n    def _split_identifier_for_json(self, identifier: Union[str, Identifier]) -&gt; Dict[str, Union[Identifier, str]]:\n        identifier_tuple = self._identifier_to_validated_tuple(identifier)\n        return {\"namespace\": identifier_tuple[:-1], \"name\": identifier_tuple[-1]}\n\n    def _handle_non_200_response(self, exc: HTTPError, error_handler: Dict[int, Type[Exception]]) -&gt; None:\n        exception: Type[Exception]\n\n        if exc.response is None:\n            raise ValueError(\"Did not receive a response\")\n\n        code = exc.response.status_code\n        if code in error_handler:\n            exception = error_handler[code]\n        elif code == 400:\n            exception = BadRequestError\n        elif code == 401:\n            exception = UnauthorizedError\n        elif code == 403:\n            exception = ForbiddenError\n        elif code == 422:\n            exception = RESTError\n        elif code == 419:\n            exception = AuthorizationExpiredError\n        elif code == 501:\n            exception = NotImplementedError\n        elif code == 503:\n            exception = ServiceUnavailableError\n        elif 500 &lt;= code &lt; 600:\n            exception = ServerError\n        else:\n            exception = RESTError\n\n        try:\n            if exception == OAuthError:\n                # The OAuthErrorResponse has a different format\n                error = OAuthErrorResponse(**exc.response.json())\n                response = str(error.error)\n                if description := error.error_description:\n                    response += f\": {description}\"\n                if uri := error.error_uri:\n                    response += f\" ({uri})\"\n            else:\n                error = ErrorResponse(**exc.response.json()).error\n                response = f\"{error.type}: {error.message}\"\n        except JSONDecodeError:\n            # In the case we don't have a proper response\n            response = f\"RESTError {exc.response.status_code}: Could not decode json payload: {exc.response.text}\"\n        except ValidationError as e:\n            # In the case we don't have a proper response\n            errs = \", \".join(err[\"msg\"] for err in e.errors())\n            response = (\n                f\"RESTError {exc.response.status_code}: Received unexpected JSON Payload: {exc.response.text}, errors: {errs}\"\n            )\n\n        raise exception(response) from exc\n\n    def _init_sigv4(self, session: Session) -&gt; None:\n        from urllib import parse\n\n        import boto3\n        from botocore.auth import SigV4Auth\n        from botocore.awsrequest import AWSRequest\n        from requests import PreparedRequest\n        from requests.adapters import HTTPAdapter\n\n        class SigV4Adapter(HTTPAdapter):\n            def __init__(self, **properties: str):\n                super().__init__()\n                self._properties = properties\n\n            def add_headers(self, request: PreparedRequest, **kwargs: Any) -&gt; None:  # pylint: disable=W0613\n                boto_session = boto3.Session()\n                credentials = boto_session.get_credentials().get_frozen_credentials()\n                region = self._properties.get(SIGV4_REGION, boto_session.region_name)\n                service = self._properties.get(SIGV4_SERVICE, \"execute-api\")\n\n                url = str(request.url).split(\"?\")[0]\n                query = str(parse.urlsplit(request.url).query)\n                params = dict(parse.parse_qsl(query))\n\n                # remove the connection header as it will be updated after signing\n                del request.headers[\"connection\"]\n\n                aws_request = AWSRequest(\n                    method=request.method, url=url, params=params, data=request.body, headers=dict(request.headers)\n                )\n\n                SigV4Auth(credentials, service, region).add_auth(aws_request)\n                original_header = request.headers\n                signed_headers = aws_request.headers\n                relocated_headers = {}\n\n                # relocate headers if there is a conflict with signed headers\n                for header, value in original_header.items():\n                    if header in signed_headers and signed_headers[header] != value:\n                        relocated_headers[f\"Original-{header}\"] = value\n\n                request.headers.update(relocated_headers)\n                request.headers.update(signed_headers)\n\n        session.mount(self.uri, SigV4Adapter(**self.properties))\n\n    def _response_to_table(self, identifier_tuple: Tuple[str, ...], table_response: TableResponse) -&gt; Table:\n        return Table(\n            identifier=identifier_tuple,\n            metadata_location=table_response.metadata_location,  # type: ignore\n            metadata=table_response.metadata,\n            io=self._load_file_io(\n                {**table_response.metadata.properties, **table_response.config}, table_response.metadata_location\n            ),\n            catalog=self,\n            config=table_response.config,\n        )\n\n    def _response_to_staged_table(self, identifier_tuple: Tuple[str, ...], table_response: TableResponse) -&gt; StagedTable:\n        return StagedTable(\n            identifier=identifier_tuple,\n            metadata_location=table_response.metadata_location,  # type: ignore\n            metadata=table_response.metadata,\n            io=self._load_file_io(\n                {**table_response.metadata.properties, **table_response.config}, table_response.metadata_location\n            ),\n            catalog=self,\n        )\n\n    def _refresh_token(self, session: Optional[Session] = None, initial_token: Optional[str] = None) -&gt; None:\n        session = session or self._session\n        if initial_token is not None:\n            self.properties[TOKEN] = initial_token\n        elif CREDENTIAL in self.properties:\n            self.properties[TOKEN] = self._fetch_access_token(session, self.properties[CREDENTIAL])\n\n        # Set Auth token for subsequent calls in the session\n        if token := self.properties.get(TOKEN):\n            session.headers[AUTHORIZATION_HEADER] = f\"{BEARER_PREFIX} {token}\"\n\n    def _config_headers(self, session: Session) -&gt; None:\n        header_properties = get_header_properties(self.properties)\n        session.headers.update(header_properties)\n        session.headers[\"Content-type\"] = \"application/json\"\n        session.headers[\"X-Client-Version\"] = ICEBERG_REST_SPEC_VERSION\n        session.headers[\"User-Agent\"] = f\"PyIceberg/{__version__}\"\n        session.headers.setdefault(\"X-Iceberg-Access-Delegation\", ACCESS_DELEGATION_DEFAULT)\n\n    def _create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n        stage_create: bool = False,\n    ) -&gt; TableResponse:\n        iceberg_schema = self._convert_schema_if_needed(schema)\n        fresh_schema = assign_fresh_schema_ids(iceberg_schema)\n        fresh_partition_spec = assign_fresh_partition_spec_ids(partition_spec, iceberg_schema, fresh_schema)\n        fresh_sort_order = assign_fresh_sort_order_ids(sort_order, iceberg_schema, fresh_schema)\n\n        namespace_and_table = self._split_identifier_for_path(identifier)\n        if location:\n            location = location.rstrip(\"/\")\n        request = CreateTableRequest(\n            name=namespace_and_table[\"table\"],\n            location=location,\n            table_schema=fresh_schema,\n            partition_spec=fresh_partition_spec,\n            write_order=fresh_sort_order,\n            stage_create=stage_create,\n            properties=properties,\n        )\n        serialized_json = request.model_dump_json().encode(UTF8)\n        response = self._session.post(\n            self.url(Endpoints.create_table, namespace=namespace_and_table[\"namespace\"]),\n            data=serialized_json,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {409: TableAlreadyExistsError})\n        return TableResponse(**response.json())\n\n    @retry(**_RETRY_ARGS)\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        table_response = self._create_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n            stage_create=False,\n        )\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def create_table_transaction(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        table_response = self._create_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n            stage_create=True,\n        )\n        staged_table = self._response_to_staged_table(self.identifier_to_tuple(identifier), table_response)\n        return CreateTableTransaction(staged_table)\n\n    @retry(**_RETRY_ARGS)\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        namespace_and_table = self._split_identifier_for_path(identifier)\n        request = RegisterTableRequest(\n            name=namespace_and_table[\"table\"],\n            metadata_location=metadata_location,\n        )\n        serialized_json = request.model_dump_json().encode(UTF8)\n        response = self._session.post(\n            self.url(Endpoints.register_table, namespace=namespace_and_table[\"namespace\"]),\n            data=serialized_json,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {409: TableAlreadyExistsError})\n\n        table_response = TableResponse(**response.json())\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace_concat = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.list_tables, namespace=namespace_concat))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        return [(*table.namespace, table.name) for table in ListTablesResponse(**response.json()).identifiers]\n\n    @retry(**_RETRY_ARGS)\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        response = self._session.get(self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier)))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchTableError})\n\n        table_response = TableResponse(**response.json())\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def drop_table(self, identifier: Union[str, Identifier], purge_requested: bool = False) -&gt; None:\n        response = self._session.delete(\n            self.url(Endpoints.drop_table, prefixed=True, purge=purge_requested, **self._split_identifier_for_path(identifier)),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchTableError})\n\n    @retry(**_RETRY_ARGS)\n    def purge_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        self.drop_table(identifier=identifier, purge_requested=True)\n\n    @retry(**_RETRY_ARGS)\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        payload = {\n            \"source\": self._split_identifier_for_json(from_identifier),\n            \"destination\": self._split_identifier_for_json(to_identifier),\n        }\n        response = self._session.post(self.url(Endpoints.rename_table), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchTableError, 409: TableAlreadyExistsError})\n\n        return self.load_table(to_identifier)\n\n    def _remove_catalog_name_from_table_request_identifier(self, table_request: CommitTableRequest) -&gt; CommitTableRequest:\n        if table_request.identifier.namespace.root[0] == self.name:\n            return table_request.model_copy(\n                update={\n                    \"identifier\": TableIdentifier(\n                        namespace=table_request.identifier.namespace.root[1:], name=table_request.identifier.name\n                    )\n                }\n            )\n        return table_request\n\n    @retry(**_RETRY_ARGS)\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace_concat = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.list_views, namespace=namespace_concat))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        return [(*view.namespace, view.name) for view in ListViewsResponse(**response.json()).identifiers]\n\n    @retry(**_RETRY_ARGS)\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n            CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n        \"\"\"\n        identifier = table.name()\n        table_identifier = TableIdentifier(namespace=identifier[:-1], name=identifier[-1])\n        table_request = CommitTableRequest(identifier=table_identifier, requirements=requirements, updates=updates)\n\n        headers = self._session.headers\n        if table_token := table.config.get(TOKEN):\n            headers[AUTHORIZATION_HEADER] = f\"{BEARER_PREFIX} {table_token}\"\n\n        response = self._session.post(\n            self.url(Endpoints.update_table, prefixed=True, **self._split_identifier_for_path(table_request.identifier)),\n            data=table_request.model_dump_json().encode(UTF8),\n            headers=headers,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(\n                exc,\n                {\n                    409: CommitFailedException,\n                    500: CommitStateUnknownException,\n                    502: CommitStateUnknownException,\n                    504: CommitStateUnknownException,\n                },\n            )\n        return CommitTableResponse(**response.json())\n\n    @retry(**_RETRY_ARGS)\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        payload = {\"namespace\": namespace_tuple, \"properties\": properties}\n        response = self._session.post(self.url(Endpoints.create_namespace), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {409: NamespaceAlreadyExistsError})\n\n    @retry(**_RETRY_ARGS)\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        response = self._session.delete(self.url(Endpoints.drop_namespace, namespace=namespace))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchNamespaceError, 409: NamespaceNotEmptyError})\n\n    @retry(**_RETRY_ARGS)\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        namespace_tuple = self.identifier_to_tuple(namespace)\n        response = self._session.get(\n            self.url(\n                f\"{Endpoints.list_namespaces}?parent={NAMESPACE_SEPARATOR.join(namespace_tuple)}\"\n                if namespace_tuple\n                else Endpoints.list_namespaces\n            ),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {})\n\n        return ListNamespaceResponse(**response.json()).namespaces\n\n    @retry(**_RETRY_ARGS)\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.load_namespace_metadata, namespace=namespace))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchNamespaceError})\n\n        return NamespaceResponse(**response.json()).properties\n\n    @retry(**_RETRY_ARGS)\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        payload = {\"removals\": list(removals or []), \"updates\": updates}\n        response = self._session.post(self.url(Endpoints.update_namespace_properties, namespace=namespace), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        parsed_response = UpdateNamespacePropertiesResponse(**response.json())\n        return PropertiesUpdateSummary(\n            removed=parsed_response.removed,\n            updated=parsed_response.updated,\n            missing=parsed_response.missing,\n        )\n\n    @retry(**_RETRY_ARGS)\n    def namespace_exists(self, namespace: Union[str, Identifier]) -&gt; bool:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = NAMESPACE_SEPARATOR.join(namespace_tuple)\n        response = self._session.head(self.url(Endpoints.namespace_exists, namespace=namespace))\n\n        if response.status_code == 404:\n            return False\n        elif response.status_code in (200, 204):\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        \"\"\"Check if a table exists.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n        response = self._session.head(\n            self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier))\n        )\n\n        if response.status_code == 404:\n            return False\n        elif response.status_code in (200, 204):\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        \"\"\"Check if a view exists.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Returns:\n            bool: True if the view exists, False otherwise.\n        \"\"\"\n        response = self._session.head(\n            self.url(Endpoints.view_exists, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n        )\n        if response.status_code == 404:\n            return False\n        elif response.status_code in [200, 204]:\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def drop_view(self, identifier: Union[str]) -&gt; None:\n        response = self._session.delete(\n            self.url(Endpoints.drop_view, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            self._handle_non_200_response(exc, {404: NoSuchViewError})\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.__init__","title":"<code>__init__(name, **properties)</code>","text":"<p>Rest Catalog.</p> <p>You either need to provide a client_id and client_secret, or an already valid token.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the catalog.</p> required <code>properties</code> <code>str</code> <p>Properties that are passed along to the configuration.</p> <code>{}</code> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>def __init__(self, name: str, **properties: str):\n    \"\"\"Rest Catalog.\n\n    You either need to provide a client_id and client_secret, or an already valid token.\n\n    Args:\n        name: Name to identify the catalog.\n        properties: Properties that are passed along to the configuration.\n    \"\"\"\n    super().__init__(name, **properties)\n    self.uri = properties[URI]\n    self._fetch_config()\n    self._session = self._create_session()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog._check_valid_namespace_identifier","title":"<code>_check_valid_namespace_identifier(identifier)</code>","text":"<p>Check if the identifier has at least one element.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>def _check_valid_namespace_identifier(self, identifier: Union[str, Identifier]) -&gt; Identifier:\n    \"\"\"Check if the identifier has at least one element.\"\"\"\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n    if len(identifier_tuple) &lt; 1:\n        raise NoSuchNamespaceError(f\"Empty namespace identifier: {identifier}\")\n    return identifier_tuple\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog._create_session","title":"<code>_create_session()</code>","text":"<p>Create a request session with provided catalog configuration.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>def _create_session(self) -&gt; Session:\n    \"\"\"Create a request session with provided catalog configuration.\"\"\"\n    session = Session()\n\n    # Sets the client side and server side SSL cert verification, if provided as properties.\n    if ssl_config := self.properties.get(SSL):\n        if ssl_ca_bundle := ssl_config.get(CA_BUNDLE):\n            session.verify = ssl_ca_bundle\n        if ssl_client := ssl_config.get(CLIENT):\n            if all(k in ssl_client for k in (CERT, KEY)):\n                session.cert = (ssl_client[CERT], ssl_client[KEY])\n            elif ssl_client_cert := ssl_client.get(CERT):\n                session.cert = ssl_client_cert\n\n    self._refresh_token(session, self.properties.get(TOKEN))\n\n    # Set HTTP headers\n    self._config_headers(session)\n\n    # Configure SigV4 Request Signing\n    if property_as_bool(self.properties, SIGV4, False):\n        self._init_sigv4(session)\n\n    return session\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> <code>CommitStateUnknownException</code> <p>Failed due to an internal exception on the side of the catalog.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n    \"\"\"\n    identifier = table.name()\n    table_identifier = TableIdentifier(namespace=identifier[:-1], name=identifier[-1])\n    table_request = CommitTableRequest(identifier=table_identifier, requirements=requirements, updates=updates)\n\n    headers = self._session.headers\n    if table_token := table.config.get(TOKEN):\n        headers[AUTHORIZATION_HEADER] = f\"{BEARER_PREFIX} {table_token}\"\n\n    response = self._session.post(\n        self.url(Endpoints.update_table, prefixed=True, **self._split_identifier_for_path(table_request.identifier)),\n        data=table_request.model_dump_json().encode(UTF8),\n        headers=headers,\n    )\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        self._handle_non_200_response(\n            exc,\n            {\n                409: CommitFailedException,\n                500: CommitStateUnknownException,\n                502: CommitStateUnknownException,\n                504: CommitStateUnknownException,\n            },\n        )\n    return CommitTableResponse(**response.json())\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    namespace_and_table = self._split_identifier_for_path(identifier)\n    request = RegisterTableRequest(\n        name=namespace_and_table[\"table\"],\n        metadata_location=metadata_location,\n    )\n    serialized_json = request.model_dump_json().encode(UTF8)\n    response = self._session.post(\n        self.url(Endpoints.register_table, namespace=namespace_and_table[\"namespace\"]),\n        data=serialized_json,\n    )\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        self._handle_non_200_response(exc, {409: TableAlreadyExistsError})\n\n    table_response = TableResponse(**response.json())\n    return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.table_exists","title":"<code>table_exists(identifier)</code>","text":"<p>Check if a table exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef table_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n    \"\"\"Check if a table exists.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n    response = self._session.head(\n        self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier))\n    )\n\n    if response.status_code == 404:\n        return False\n    elif response.status_code in (200, 204):\n        return True\n\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        self._handle_non_200_response(exc, {})\n\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.url","title":"<code>url(endpoint, prefixed=True, **kwargs)</code>","text":"<p>Construct the endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>Resource identifier that points to the REST catalog.</p> required <code>prefixed</code> <code>bool</code> <p>If the prefix return by the config needs to be appended.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The base url of the rest catalog.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>def url(self, endpoint: str, prefixed: bool = True, **kwargs: Any) -&gt; str:\n    \"\"\"Construct the endpoint.\n\n    Args:\n        endpoint: Resource identifier that points to the REST catalog.\n        prefixed: If the prefix return by the config needs to be appended.\n\n    Returns:\n        The base url of the rest catalog.\n    \"\"\"\n    url = self.uri\n    url = url + \"v1/\" if url.endswith(\"/\") else url + \"/v1/\"\n\n    if prefixed:\n        url += self.properties.get(PREFIX, \"\")\n        url = url if url.endswith(\"/\") else url + \"/\"\n\n    return url + endpoint.format(**kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.view_exists","title":"<code>view_exists(identifier)</code>","text":"<p>Check if a view exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the view exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/rest.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n    \"\"\"Check if a view exists.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Returns:\n        bool: True if the view exists, False otherwise.\n    \"\"\"\n    response = self._session.head(\n        self.url(Endpoints.view_exists, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n    )\n    if response.status_code == 404:\n        return False\n    elif response.status_code in [200, 204]:\n        return True\n\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        self._handle_non_200_response(exc, {})\n\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/","title":"sql","text":""},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog","title":"<code>SqlCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> <p>Implementation of a SQL based catalog.</p> <p>In the <code>JDBCCatalog</code> implementation, a <code>Namespace</code> is composed of a list of strings separated by dots: <code>'ns1.ns2.ns3'</code>. And you can have as many levels as you want, but you need at least one.  The <code>SqlCatalog</code> honors the same convention.</p> <p>In the <code>JDBCCatalog</code> implementation, a <code>TableIdentifier</code> is composed of an optional <code>Namespace</code> and a table name. When a <code>Namespace</code> is present, the full name will be <code>'ns1.ns2.ns3.table'</code>.  A valid <code>TableIdentifier</code> could be <code>'name'</code> (no namespace). The <code>SqlCatalog</code> has a different convention where a <code>TableIdentifier</code> requires a <code>Namespace</code>.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>class SqlCatalog(MetastoreCatalog):\n    \"\"\"Implementation of a SQL based catalog.\n\n    In the `JDBCCatalog` implementation, a `Namespace` is composed of a list of strings separated by dots: `'ns1.ns2.ns3'`.\n    And you can have as many levels as you want, but you need at least one.  The `SqlCatalog` honors the same convention.\n\n    In the `JDBCCatalog` implementation, a `TableIdentifier` is composed of an optional `Namespace` and a table name.\n    When a `Namespace` is present, the full name will be `'ns1.ns2.ns3.table'`.  A valid `TableIdentifier` could be `'name'` (no namespace).\n    The `SqlCatalog` has a different convention where a `TableIdentifier` requires a `Namespace`.\n    \"\"\"\n\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n        if not (uri_prop := self.properties.get(\"uri\")):\n            raise NoSuchPropertyException(\"SQL connection URI is required\")\n\n        echo_str = str(self.properties.get(\"echo\", DEFAULT_ECHO_VALUE)).lower()\n        echo = strtobool(echo_str) if echo_str != \"debug\" else \"debug\"\n        pool_pre_ping = strtobool(self.properties.get(\"pool_pre_ping\", DEFAULT_POOL_PRE_PING_VALUE))\n        init_catalog_tables = strtobool(self.properties.get(\"init_catalog_tables\", DEFAULT_INIT_CATALOG_TABLES))\n\n        self.engine = create_engine(uri_prop, echo=echo, pool_pre_ping=pool_pre_ping)\n\n        if init_catalog_tables:\n            self._ensure_tables_exist()\n\n    def _ensure_tables_exist(self) -&gt; None:\n        with Session(self.engine) as session:\n            for table in [IcebergTables, IcebergNamespaceProperties]:\n                stmt = select(1).select_from(table)\n                try:\n                    session.scalar(stmt)\n                except (\n                    OperationalError,\n                    ProgrammingError,\n                ):  # sqlalchemy returns OperationalError in case of sqlite and ProgrammingError with postgres.\n                    self.create_tables()\n                    return\n\n    def create_tables(self) -&gt; None:\n        SqlCatalogBaseTable.metadata.create_all(self.engine)\n\n    def destroy_tables(self) -&gt; None:\n        SqlCatalogBaseTable.metadata.drop_all(self.engine)\n\n    def _convert_orm_to_iceberg(self, orm_table: IcebergTables) -&gt; Table:\n        # Check for expected properties.\n        if not (metadata_location := orm_table.metadata_location):\n            raise NoSuchTableError(f\"Table property {METADATA_LOCATION} is missing\")\n        if not (table_namespace := orm_table.table_namespace):\n            raise NoSuchTableError(f\"Table property {IcebergTables.table_namespace} is missing\")\n        if not (table_name := orm_table.table_name):\n            raise NoSuchTableError(f\"Table property {IcebergTables.table_name} is missing\")\n\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=Catalog.identifier_to_tuple(table_namespace) + (table_name,),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def create_table(\n        self,\n        identifier: Union[str, Identifier],\n        schema: Union[Schema, \"pa.Schema\"],\n        location: Optional[str] = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n        namespace_identifier = Catalog.namespace_from(identifier)\n        table_name = Catalog.table_name_from(identifier)\n        if not self._namespace_exists(namespace_identifier):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace_identifier}\")\n\n        namespace = Catalog.namespace_to_string(namespace_identifier)\n        location = self._resolve_table_location(location, namespace, table_name)\n        location_provider = load_location_provider(table_location=location, table_properties=properties)\n        metadata_location = location_provider.new_table_metadata_file_location()\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        self._write_metadata(metadata, io, metadata_location)\n\n        with Session(self.engine) as session:\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier Union[str, Identifier]: Table identifier for the table\n            metadata_location str: The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n            NoSuchNamespaceError: If namespace does not exist\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        if not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        with Session(self.engine) as session:\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Load the table's metadata and return the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        with Session(self.engine) as session:\n            stmt = select(IcebergTables).where(\n                IcebergTables.catalog_name == self.name,\n                IcebergTables.table_namespace == namespace,\n                IcebergTables.table_name == table_name,\n            )\n            result = session.scalar(stmt)\n        if result:\n            return self._convert_orm_to_iceberg(result)\n        raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n\n    def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        with Session(self.engine) as session:\n            if self.engine.dialect.supports_sane_rowcount:\n                res = session.execute(\n                    delete(IcebergTables).where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                    )\n                )\n                if res.rowcount &lt; 1:\n                    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                        )\n                        .one()\n                    )\n                    session.delete(tbl)\n                except NoResultFound as e:\n                    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\") from e\n            session.commit()\n\n    def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier (str | Identifier): Existing table identifier.\n            to_identifier (str | Identifier): New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n            TableAlreadyExistsError: If a table with the new name already exist.\n            NoSuchNamespaceError: If the target namespace does not exist.\n        \"\"\"\n        from_namespace_tuple = Catalog.namespace_from(from_identifier)\n        from_namespace = Catalog.namespace_to_string(from_namespace_tuple)\n        from_table_name = Catalog.table_name_from(from_identifier)\n        to_namespace_tuple = Catalog.namespace_from(to_identifier)\n        to_namespace = Catalog.namespace_to_string(to_namespace_tuple)\n        to_table_name = Catalog.table_name_from(to_identifier)\n        if not self._namespace_exists(to_namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {to_namespace}\")\n        with Session(self.engine) as session:\n            try:\n                if self.engine.dialect.supports_sane_rowcount:\n                    stmt = (\n                        update(IcebergTables)\n                        .where(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == from_namespace,\n                            IcebergTables.table_name == from_table_name,\n                        )\n                        .values(table_namespace=to_namespace, table_name=to_table_name)\n                    )\n                    result = session.execute(stmt)\n                    if result.rowcount &lt; 1:\n                        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\")\n                else:\n                    try:\n                        tbl = (\n                            session.query(IcebergTables)\n                            .with_for_update(of=IcebergTables)\n                            .filter(\n                                IcebergTables.catalog_name == self.name,\n                                IcebergTables.table_namespace == from_namespace,\n                                IcebergTables.table_name == from_table_name,\n                            )\n                            .one()\n                        )\n                        tbl.table_namespace = to_namespace\n                        tbl.table_name = to_table_name\n                    except NoResultFound as e:\n                        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {to_namespace}.{to_table_name} already exists\") from e\n        return self.load_table(to_identifier)\n\n    def commit_table(\n        self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        namespace_tuple = Catalog.namespace_from(table_identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(table_identifier)\n\n        current_table: Optional[Table]\n        try:\n            current_table = self.load_table(table_identifier)\n        except NoSuchTableError:\n            current_table = None\n\n        updated_staged_table = self._update_and_stage_table(current_table, table.name(), requirements, updates)\n        if current_table and updated_staged_table.metadata == current_table.metadata:\n            # no changes, do nothing\n            return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n        self._write_metadata(\n            metadata=updated_staged_table.metadata,\n            io=updated_staged_table.io,\n            metadata_path=updated_staged_table.metadata_location,\n        )\n\n        with Session(self.engine) as session:\n            if current_table:\n                # table exists, update it\n                if self.engine.dialect.supports_sane_rowcount:\n                    stmt = (\n                        update(IcebergTables)\n                        .where(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                            IcebergTables.metadata_location == current_table.metadata_location,\n                        )\n                        .values(\n                            metadata_location=updated_staged_table.metadata_location,\n                            previous_metadata_location=current_table.metadata_location,\n                        )\n                    )\n                    result = session.execute(stmt)\n                    if result.rowcount &lt; 1:\n                        raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\")\n                else:\n                    try:\n                        tbl = (\n                            session.query(IcebergTables)\n                            .with_for_update(of=IcebergTables)\n                            .filter(\n                                IcebergTables.catalog_name == self.name,\n                                IcebergTables.table_namespace == namespace,\n                                IcebergTables.table_name == table_name,\n                                IcebergTables.metadata_location == current_table.metadata_location,\n                            )\n                            .one()\n                        )\n                        tbl.metadata_location = updated_staged_table.metadata_location\n                        tbl.previous_metadata_location = current_table.metadata_location\n                    except NoResultFound as e:\n                        raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\") from e\n                session.commit()\n            else:\n                # table does not exist, create it\n                try:\n                    session.add(\n                        IcebergTables(\n                            catalog_name=self.name,\n                            table_namespace=namespace,\n                            table_name=table_name,\n                            metadata_location=updated_staged_table.metadata_location,\n                            previous_metadata_location=None,\n                        )\n                    )\n                    session.commit()\n                except IntegrityError as e:\n                    raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def _namespace_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        namespace_tuple = Catalog.identifier_to_tuple(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple, NoSuchNamespaceError)\n        namespace_starts_with = namespace.replace(\"!\", \"!!\").replace(\"_\", \"!_\").replace(\"%\", \"!%\") + \".%\"\n\n        with Session(self.engine) as session:\n            stmt = (\n                select(IcebergTables)\n                .where(\n                    IcebergTables.catalog_name == self.name,\n                    (IcebergTables.table_namespace == namespace)\n                    | (IcebergTables.table_namespace.like(namespace_starts_with, escape=\"!\")),\n                )\n                .limit(1)\n            )\n            result = session.execute(stmt).all()\n            if result:\n                return True\n            stmt = (\n                select(IcebergNamespaceProperties)\n                .where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    (IcebergNamespaceProperties.namespace == namespace)\n                    | (IcebergNamespaceProperties.namespace.like(namespace_starts_with, escape=\"!\")),\n                )\n                .limit(1)\n            )\n            result = session.execute(stmt).all()\n            if result:\n                return True\n        return False\n\n    def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n\n        Raises:\n            NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        if self._namespace_exists(namespace):\n            raise NamespaceAlreadyExistsError(f\"Namespace {namespace} already exists\")\n\n        if not properties:\n            properties = IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n        create_properties = properties if properties else IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n        with Session(self.engine) as session:\n            for key, value in create_properties.items():\n                session.add(\n                    IcebergNamespaceProperties(\n                        catalog_name=self.name,\n                        namespace=Catalog.namespace_to_string(namespace, NoSuchNamespaceError),\n                        property_key=key,\n                        property_value=value,\n                    )\n                )\n            session.commit()\n\n    def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        if not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if tables := self.list_tables(namespace):\n            raise NamespaceNotEmptyError(f\"Namespace {namespace_str} is not empty. {len(tables)} tables exist.\")\n\n        with Session(self.engine) as session:\n            session.execute(\n                delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                )\n            )\n            session.commit()\n\n    def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        \"\"\"List tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        if namespace and not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        namespace = Catalog.namespace_to_string(namespace)\n        stmt = select(IcebergTables).where(IcebergTables.catalog_name == self.name, IcebergTables.table_namespace == namespace)\n        with Session(self.engine) as session:\n            result = session.scalars(stmt)\n            return [(Catalog.identifier_to_tuple(table.table_namespace) + (table.table_name,)) for table in result]\n\n    def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        if namespace and not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        table_stmt = select(IcebergTables.table_namespace).where(IcebergTables.catalog_name == self.name)\n        namespace_stmt = select(IcebergNamespaceProperties.namespace).where(IcebergNamespaceProperties.catalog_name == self.name)\n        if namespace:\n            namespace_str = Catalog.namespace_to_string(namespace, NoSuchNamespaceError)\n            table_stmt = table_stmt.where(IcebergTables.table_namespace.like(namespace_str))\n            namespace_stmt = namespace_stmt.where(IcebergNamespaceProperties.namespace.like(namespace_str))\n        stmt = union(\n            table_stmt,\n            namespace_stmt,\n        )\n        with Session(self.engine) as session:\n            return [Catalog.identifier_to_tuple(namespace_col) for namespace_col in session.execute(stmt).scalars()]\n\n    def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n        stmt = select(IcebergNamespaceProperties).where(\n            IcebergNamespaceProperties.catalog_name == self.name, IcebergNamespaceProperties.namespace == namespace_str\n        )\n        with Session(self.engine) as session:\n            result = session.scalars(stmt)\n            return {props.property_key: props.property_value for props in result}\n\n    def update_namespace_properties(\n        self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and update properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n            updates (Properties): Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if not self._namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n        current_properties = self.load_namespace_properties(namespace=namespace)\n        properties_update_summary = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )[0]\n\n        with Session(self.engine) as session:\n            if removals:\n                delete_stmt = delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                    IcebergNamespaceProperties.property_key.in_(removals),\n                )\n                session.execute(delete_stmt)\n\n            if updates:\n                # SQLAlchemy does not (yet) support engine agnostic UPSERT\n                # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-upsert-statements\n                # This is not a problem since it runs in a single transaction\n                delete_stmt = delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                    IcebergNamespaceProperties.property_key.in_(set(updates.keys())),\n                )\n                session.execute(delete_stmt)\n                insert_stmt_values = [\n                    {\n                        IcebergNamespaceProperties.catalog_name: self.name,\n                        IcebergNamespaceProperties.namespace: namespace_str,\n                        IcebergNamespaceProperties.property_key: property_key,\n                        IcebergNamespaceProperties.property_value: property_value,\n                    }\n                    for property_key, property_value in updates.items()\n                ]\n                insert_stmt = insert(IcebergNamespaceProperties).values(insert_stmt_values)\n                session.execute(insert_stmt)\n            session.commit()\n        return properties_update_summary\n\n    def list_views(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: Union[str, Identifier]) -&gt; bool:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: Union[str, Identifier]) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>Tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: Tuple[TableRequirement, ...], updates: Tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    namespace_tuple = Catalog.namespace_from(table_identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(table_identifier)\n\n    current_table: Optional[Table]\n    try:\n        current_table = self.load_table(table_identifier)\n    except NoSuchTableError:\n        current_table = None\n\n    updated_staged_table = self._update_and_stage_table(current_table, table.name(), requirements, updates)\n    if current_table and updated_staged_table.metadata == current_table.metadata:\n        # no changes, do nothing\n        return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n    self._write_metadata(\n        metadata=updated_staged_table.metadata,\n        io=updated_staged_table.io,\n        metadata_path=updated_staged_table.metadata_location,\n    )\n\n    with Session(self.engine) as session:\n        if current_table:\n            # table exists, update it\n            if self.engine.dialect.supports_sane_rowcount:\n                stmt = (\n                    update(IcebergTables)\n                    .where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                        IcebergTables.metadata_location == current_table.metadata_location,\n                    )\n                    .values(\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=current_table.metadata_location,\n                    )\n                )\n                result = session.execute(stmt)\n                if result.rowcount &lt; 1:\n                    raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                            IcebergTables.metadata_location == current_table.metadata_location,\n                        )\n                        .one()\n                    )\n                    tbl.metadata_location = updated_staged_table.metadata_location\n                    tbl.previous_metadata_location = current_table.metadata_location\n                except NoResultFound as e:\n                    raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\") from e\n            session.commit()\n        else:\n            # table does not exist, create it\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NamespaceAlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def create_namespace(self, namespace: Union[str, Identifier], properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n\n    Raises:\n        NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    if self._namespace_exists(namespace):\n        raise NamespaceAlreadyExistsError(f\"Namespace {namespace} already exists\")\n\n    if not properties:\n        properties = IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n    create_properties = properties if properties else IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n    with Session(self.engine) as session:\n        for key, value in create_properties.items():\n            session.add(\n                IcebergNamespaceProperties(\n                    catalog_name=self.name,\n                    namespace=Catalog.namespace_to_string(namespace, NoSuchNamespaceError),\n                    property_key=key,\n                    property_value=value,\n                )\n            )\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>Optional[str]</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def create_table(\n    self,\n    identifier: Union[str, Identifier],\n    schema: Union[Schema, \"pa.Schema\"],\n    location: Optional[str] = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n    namespace_identifier = Catalog.namespace_from(identifier)\n    table_name = Catalog.table_name_from(identifier)\n    if not self._namespace_exists(namespace_identifier):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace_identifier}\")\n\n    namespace = Catalog.namespace_to_string(namespace_identifier)\n    location = self._resolve_table_location(location, namespace, table_name)\n    location_provider = load_location_provider(table_location=location, table_properties=properties)\n    metadata_location = location_provider.new_table_metadata_file_location()\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n    io = load_file_io(properties=self.properties, location=metadata_location)\n    self._write_metadata(metadata, io, metadata_location)\n\n    with Session(self.engine) as session:\n        try:\n            session.add(\n                IcebergTables(\n                    catalog_name=self.name,\n                    table_namespace=namespace,\n                    table_name=table_name,\n                    metadata_location=metadata_location,\n                    previous_metadata_location=None,\n                )\n            )\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def drop_namespace(self, namespace: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    if not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if tables := self.list_tables(namespace):\n        raise NamespaceNotEmptyError(f\"Namespace {namespace_str} is not empty. {len(tables)} tables exist.\")\n\n    with Session(self.engine) as session:\n        session.execute(\n            delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n            )\n        )\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def drop_table(self, identifier: Union[str, Identifier]) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    with Session(self.engine) as session:\n        if self.engine.dialect.supports_sane_rowcount:\n            res = session.execute(\n                delete(IcebergTables).where(\n                    IcebergTables.catalog_name == self.name,\n                    IcebergTables.table_namespace == namespace,\n                    IcebergTables.table_name == table_name,\n                )\n            )\n            if res.rowcount &lt; 1:\n                raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n        else:\n            try:\n                tbl = (\n                    session.query(IcebergTables)\n                    .with_for_update(of=IcebergTables)\n                    .filter(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                    )\n                    .one()\n                )\n                session.delete(tbl)\n            except NoResultFound as e:\n                raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\") from e\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> <code>()</code> <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def list_namespaces(self, namespace: Union[str, Identifier] = ()) -&gt; List[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    if namespace and not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    table_stmt = select(IcebergTables.table_namespace).where(IcebergTables.catalog_name == self.name)\n    namespace_stmt = select(IcebergNamespaceProperties.namespace).where(IcebergNamespaceProperties.catalog_name == self.name)\n    if namespace:\n        namespace_str = Catalog.namespace_to_string(namespace, NoSuchNamespaceError)\n        table_stmt = table_stmt.where(IcebergTables.table_namespace.like(namespace_str))\n        namespace_stmt = namespace_stmt.where(IcebergNamespaceProperties.namespace.like(namespace_str))\n    stmt = union(\n        table_stmt,\n        namespace_stmt,\n    )\n    with Session(self.engine) as session:\n        return [Catalog.identifier_to_tuple(namespace_col) for namespace_col in session.execute(stmt).scalars()]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>List[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def list_tables(self, namespace: Union[str, Identifier]) -&gt; List[Identifier]:\n    \"\"\"List tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    if namespace and not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    namespace = Catalog.namespace_to_string(namespace)\n    stmt = select(IcebergTables).where(IcebergTables.catalog_name == self.name, IcebergTables.table_namespace == namespace)\n    with Session(self.engine) as session:\n        result = session.scalars(stmt)\n        return [(Catalog.identifier_to_tuple(table.table_namespace) + (table.table_name,)) for table in result]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def load_namespace_properties(self, namespace: Union[str, Identifier]) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n    stmt = select(IcebergNamespaceProperties).where(\n        IcebergNamespaceProperties.catalog_name == self.name, IcebergNamespaceProperties.namespace == namespace_str\n    )\n    with Session(self.engine) as session:\n        result = session.scalars(stmt)\n        return {props.property_key: props.property_value for props in result}\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and return the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def load_table(self, identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Load the table's metadata and return the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    with Session(self.engine) as session:\n        stmt = select(IcebergTables).where(\n            IcebergTables.catalog_name == self.name,\n            IcebergTables.table_namespace == namespace,\n            IcebergTables.table_name == table_name,\n        )\n        result = session.scalar(stmt)\n    if result:\n        return self._convert_orm_to_iceberg(result)\n    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> <code>NoSuchNamespaceError</code> <p>If namespace does not exist</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def register_table(self, identifier: Union[str, Identifier], metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier Union[str, Identifier]: Table identifier for the table\n        metadata_location str: The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n        NoSuchNamespaceError: If namespace does not exist\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    if not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    with Session(self.engine) as session:\n        try:\n            session.add(\n                IcebergTables(\n                    catalog_name=self.name,\n                    table_namespace=namespace,\n                    table_name=table_name,\n                    metadata_location=metadata_location,\n                    previous_metadata_location=None,\n                )\n            )\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> <code>TableAlreadyExistsError</code> <p>If a table with the new name already exist.</p> <code>NoSuchNamespaceError</code> <p>If the target namespace does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def rename_table(self, from_identifier: Union[str, Identifier], to_identifier: Union[str, Identifier]) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier (str | Identifier): Existing table identifier.\n        to_identifier (str | Identifier): New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n        TableAlreadyExistsError: If a table with the new name already exist.\n        NoSuchNamespaceError: If the target namespace does not exist.\n    \"\"\"\n    from_namespace_tuple = Catalog.namespace_from(from_identifier)\n    from_namespace = Catalog.namespace_to_string(from_namespace_tuple)\n    from_table_name = Catalog.table_name_from(from_identifier)\n    to_namespace_tuple = Catalog.namespace_from(to_identifier)\n    to_namespace = Catalog.namespace_to_string(to_namespace_tuple)\n    to_table_name = Catalog.table_name_from(to_identifier)\n    if not self._namespace_exists(to_namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {to_namespace}\")\n    with Session(self.engine) as session:\n        try:\n            if self.engine.dialect.supports_sane_rowcount:\n                stmt = (\n                    update(IcebergTables)\n                    .where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == from_namespace,\n                        IcebergTables.table_name == from_table_name,\n                    )\n                    .values(table_namespace=to_namespace, table_name=to_table_name)\n                )\n                result = session.execute(stmt)\n                if result.rowcount &lt; 1:\n                    raise NoSuchTableError(f\"Table does not exist: {from_table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == from_namespace,\n                            IcebergTables.table_name == from_table_name,\n                        )\n                        .one()\n                    )\n                    tbl.table_namespace = to_namespace\n                    tbl.table_name = to_table_name\n                except NoResultFound as e:\n                    raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {to_namespace}.{to_table_name} already exists\") from e\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and update properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Set[str]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: Union[str, Identifier], removals: Optional[Set[str]] = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and update properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n        updates (Properties): Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if not self._namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n    current_properties = self.load_namespace_properties(namespace=namespace)\n    properties_update_summary = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )[0]\n\n    with Session(self.engine) as session:\n        if removals:\n            delete_stmt = delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n                IcebergNamespaceProperties.property_key.in_(removals),\n            )\n            session.execute(delete_stmt)\n\n        if updates:\n            # SQLAlchemy does not (yet) support engine agnostic UPSERT\n            # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-upsert-statements\n            # This is not a problem since it runs in a single transaction\n            delete_stmt = delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n                IcebergNamespaceProperties.property_key.in_(set(updates.keys())),\n            )\n            session.execute(delete_stmt)\n            insert_stmt_values = [\n                {\n                    IcebergNamespaceProperties.catalog_name: self.name,\n                    IcebergNamespaceProperties.namespace: namespace_str,\n                    IcebergNamespaceProperties.property_key: property_key,\n                    IcebergNamespaceProperties.property_value: property_value,\n                }\n                for property_key, property_value in updates.items()\n            ]\n            insert_stmt = insert(IcebergNamespaceProperties).values(insert_stmt_values)\n            session.execute(insert_stmt)\n        session.commit()\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/cli/","title":"cli","text":""},{"location":"reference/pyiceberg/cli/console/","title":"console","text":""},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console._catalog_and_output","title":"<code>_catalog_and_output(ctx)</code>","text":"<p>Small helper to set the types.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>def _catalog_and_output(ctx: Context) -&gt; Tuple[Catalog, Output]:\n    \"\"\"Small helper to set the types.\"\"\"\n    return ctx.obj[\"catalog\"], ctx.obj[\"output\"]\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.create","title":"<code>create()</code>","text":"<p>Operation to create a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef create() -&gt; None:\n    \"\"\"Operation to create a namespace.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.describe","title":"<code>describe(ctx, entity, identifier)</code>","text":"<p>Describe a namespace or a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.option(\"--entity\", type=click.Choice([\"any\", \"namespace\", \"table\"]), default=\"any\")\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef describe(ctx: Context, entity: Literal[\"name\", \"namespace\", \"table\"], identifier: str) -&gt; None:\n    \"\"\"Describe a namespace or a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    is_namespace = False\n    if entity in {\"namespace\", \"any\"} and len(identifier_tuple) &gt; 0:\n        try:\n            namespace_properties = catalog.load_namespace_properties(identifier_tuple)\n            output.describe_properties(namespace_properties)\n            is_namespace = True\n        except NoSuchNamespaceError as exc:\n            if entity != \"any\" or len(identifier_tuple) == 1:  # type: ignore\n                raise exc\n\n    is_table = False\n    if entity in {\"table\", \"any\"} and len(identifier_tuple) &gt; 1:\n        try:\n            catalog_table = catalog.load_table(identifier)\n            output.describe_table(catalog_table)\n            is_table = True\n        except NoSuchTableError as exc:\n            if entity != \"any\":\n                raise exc\n\n    if is_namespace is False and is_table is False:\n        raise NoSuchTableError(f\"Table or namespace does not exist: {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.drop","title":"<code>drop()</code>","text":"<p>Operations to drop a namespace or table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef drop() -&gt; None:\n    \"\"\"Operations to drop a namespace or table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.files","title":"<code>files(ctx, identifier, history)</code>","text":"<p>List all the files of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.option(\"--history\", is_flag=True)\n@click.pass_context\n@catch_exception()\ndef files(ctx: Context, identifier: str, history: bool) -&gt; None:\n    \"\"\"List all the files of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    catalog_table = catalog.load_table(identifier)\n    output.files(catalog_table, history)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get","title":"<code>get()</code>","text":"<p>Fetch properties on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef get() -&gt; None:\n    \"\"\"Fetch properties on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get_namespace","title":"<code>get_namespace(ctx, identifier, property_name)</code>","text":"<p>Fetch properties on a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@get.command(\"namespace\")\n@click.argument(\"identifier\")\n@click.argument(\"property_name\", required=False)\n@click.pass_context\n@catch_exception()\ndef get_namespace(ctx: Context, identifier: str, property_name: str) -&gt; None:\n    \"\"\"Fetch properties on a namespace.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    namespace_properties = catalog.load_namespace_properties(identifier_tuple)\n    assert namespace_properties\n\n    if property_name:\n        if property_value := namespace_properties.get(property_name):\n            output.text(property_value)\n        else:\n            raise NoSuchPropertyException(f\"Could not find property {property_name} on namespace {identifier}\")\n    else:\n        output.describe_properties(namespace_properties)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get_table","title":"<code>get_table(ctx, identifier, property_name)</code>","text":"<p>Fetch properties on a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@get.command(\"table\")\n@click.argument(\"identifier\")\n@click.argument(\"property_name\", required=False)\n@click.pass_context\n@catch_exception()\ndef get_table(ctx: Context, identifier: str, property_name: str) -&gt; None:\n    \"\"\"Fetch properties on a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    metadata = catalog.load_table(identifier_tuple).metadata\n    assert metadata\n\n    if property_name:\n        if property_value := metadata.properties.get(property_name):\n            output.text(property_value)\n        else:\n            raise NoSuchPropertyException(f\"Could not find property {property_name} on table {identifier}\")\n    else:\n        output.describe_properties(metadata.properties)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.list","title":"<code>list(ctx, parent)</code>","text":"<p>List tables or namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.pass_context\n@click.argument(\"parent\", required=False)\n@catch_exception()\ndef list(ctx: Context, parent: Optional[str]) -&gt; None:  # pylint: disable=redefined-builtin\n    \"\"\"List tables or namespaces.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    identifiers = []\n    if parent:\n        # Do we have tables under parent namespace?\n        identifiers = catalog.list_tables(parent)\n    if not identifiers:\n        # List hierarchical namespaces if parent, root namespaces otherwise.\n        identifiers = catalog.list_namespaces(parent or ())\n    output.identifiers(identifiers)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.list_refs","title":"<code>list_refs(ctx, identifier, type, verbose)</code>","text":"<p>List all the refs in the provided table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.option(\"--type\", required=False)\n@click.option(\"--verbose\", type=click.BOOL)\n@click.pass_context\n@catch_exception()\ndef list_refs(ctx: Context, identifier: str, type: str, verbose: bool) -&gt; None:\n    \"\"\"List all the refs in the provided table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    refs = table.refs()\n    if type:\n        type = type.lower()\n        if type not in {\"branch\", \"tag\"}:\n            raise ValueError(f\"Type must be either branch or tag, got: {type}\")\n\n    relevant_refs = [\n        (ref_name, ref.snapshot_ref_type, _retention_properties(ref, table.properties))\n        for (ref_name, ref) in refs.items()\n        if not type or ref.snapshot_ref_type == type\n    ]\n\n    output.describe_refs(relevant_refs)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.location","title":"<code>location(ctx, identifier)</code>","text":"<p>Return the location of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef location(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the location of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.text(table.location())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.namespace","title":"<code>namespace(ctx, identifier, property_name)</code>","text":"<p>Remove a property from a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@remove.command()  # type: ignore\n@click.argument(\"identifier\")\n@click.argument(\"property_name\")\n@click.pass_context\n@catch_exception()\ndef namespace(ctx: Context, identifier: str, property_name: str) -&gt; None:  # noqa: F811\n    \"\"\"Remove a property from a namespace.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    result = catalog.update_namespace_properties(identifier, removals={property_name})\n\n    if result.removed == [property_name]:\n        output.text(f\"Property {property_name} removed from {identifier}\")\n    else:\n        raise NoSuchPropertyException(f\"Property {property_name} does not exist on {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.properties","title":"<code>properties()</code>","text":"<p>Properties on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef properties() -&gt; None:\n    \"\"\"Properties on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.remove","title":"<code>remove()</code>","text":"<p>Remove a property from tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef remove() -&gt; None:\n    \"\"\"Remove a property from tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.rename","title":"<code>rename(ctx, from_identifier, to_identifier)</code>","text":"<p>Rename a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"from_identifier\")\n@click.argument(\"to_identifier\")\n@click.pass_context\n@catch_exception()\ndef rename(ctx: Context, from_identifier: str, to_identifier: str) -&gt; None:\n    \"\"\"Rename a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    catalog.rename_table(from_identifier, to_identifier)\n    output.text(f\"Renamed table from {from_identifier} to {to_identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.schema","title":"<code>schema(ctx, identifier)</code>","text":"<p>Get the schema of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef schema(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Get the schema of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.schema(table.schema())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.set","title":"<code>set()</code>","text":"<p>Set a property on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef set() -&gt; None:\n    \"\"\"Set a property on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.spec","title":"<code>spec(ctx, identifier)</code>","text":"<p>Return the partition spec of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef spec(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the partition spec of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.spec(table.spec())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.table","title":"<code>table(ctx, identifier, property_name)</code>","text":"<p>Remove a property from a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@remove.command()  # type: ignore\n@click.argument(\"identifier\")\n@click.argument(\"property_name\")\n@click.pass_context\n@catch_exception()\ndef table(ctx: Context, identifier: str, property_name: str) -&gt; None:  # noqa: F811\n    \"\"\"Remove a property from a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    if property_name in table.metadata.properties:\n        output.exception(NotImplementedError(\"Writing is WIP\"))\n        ctx.exit(1)\n    else:\n        raise NoSuchPropertyException(f\"Property {property_name} does not exist on {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.uuid","title":"<code>uuid(ctx, identifier)</code>","text":"<p>Return the UUID of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef uuid(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the UUID of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    metadata = catalog.load_table(identifier).metadata\n    output.uuid(metadata.table_uuid)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.version","title":"<code>version(ctx)</code>","text":"<p>Print pyiceberg version.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.pass_context\n@catch_exception()\ndef version(ctx: Context) -&gt; None:\n    \"\"\"Print pyiceberg version.\"\"\"\n    ctx.obj[\"output\"].version(__version__)\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/","title":"output","text":""},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.ConsoleOutput","title":"<code>ConsoleOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>Writes to the console.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class ConsoleOutput(Output):\n    \"\"\"Writes to the console.\"\"\"\n\n    verbose: bool\n\n    def __init__(self, **properties: Any) -&gt; None:\n        self.verbose = properties.get(\"verbose\", False)\n\n    @property\n    def _table(self) -&gt; RichTable:\n        return RichTable.grid(padding=(0, 2))\n\n    def exception(self, ex: Exception) -&gt; None:\n        if self.verbose:\n            Console(stderr=True).print_exception()\n        else:\n            Console(stderr=True).print(ex)\n\n    def identifiers(self, identifiers: List[Identifier]) -&gt; None:\n        table = self._table\n        for identifier in identifiers:\n            table.add_row(\".\".join(identifier))\n\n        Console().print(table)\n\n    def describe_table(self, table: Table) -&gt; None:\n        metadata = table.metadata\n        table_properties = self._table\n\n        for key, value in metadata.properties.items():\n            table_properties.add_row(key, value)\n\n        schema_tree = Tree(f\"Schema, id={table.metadata.current_schema_id}\")\n        for field in table.schema().fields:\n            schema_tree.add(str(field))\n\n        snapshot_tree = Tree(\"Snapshots\")\n        for snapshot in metadata.snapshots:\n            snapshot_tree.add(f\"Snapshot {snapshot.snapshot_id}, schema {snapshot.schema_id}: {snapshot.manifest_list}\")\n\n        output_table = self._table\n        output_table.add_row(\"Table format version\", str(metadata.format_version))\n        output_table.add_row(\"Metadata location\", table.metadata_location)\n        output_table.add_row(\"Table UUID\", str(table.metadata.table_uuid))\n        output_table.add_row(\"Last Updated\", str(metadata.last_updated_ms))\n        output_table.add_row(\"Partition spec\", str(table.spec()))\n        output_table.add_row(\"Sort order\", str(table.sort_order()))\n        output_table.add_row(\"Current schema\", schema_tree)\n        output_table.add_row(\"Current snapshot\", str(table.current_snapshot()))\n        output_table.add_row(\"Snapshots\", snapshot_tree)\n        output_table.add_row(\"Properties\", table_properties)\n        Console().print(output_table)\n\n    def files(self, table: Table, history: bool) -&gt; None:\n        if history:\n            snapshots = table.metadata.snapshots\n        else:\n            if snapshot := table.current_snapshot():\n                snapshots = [snapshot]\n            else:\n                snapshots = []\n\n        snapshot_tree = Tree(f\"Snapshots: {'.'.join(table.name())}\")\n        io = table.io\n\n        for snapshot in snapshots:\n            list_tree = snapshot_tree.add(\n                f\"Snapshot {snapshot.snapshot_id}, schema {snapshot.schema_id}: {snapshot.manifest_list}\"\n            )\n\n            manifest_list = snapshot.manifests(io)\n            for manifest in manifest_list:\n                manifest_tree = list_tree.add(f\"Manifest: {manifest.manifest_path}\")\n                for manifest_entry in manifest.fetch_manifest_entry(io, discard_deleted=False):\n                    manifest_tree.add(f\"Datafile: {manifest_entry.data_file.file_path}\")\n        Console().print(snapshot_tree)\n\n    def describe_properties(self, properties: Properties) -&gt; None:\n        output_table = self._table\n        for k, v in properties.items():\n            output_table.add_row(k, v)\n        Console().print(output_table)\n\n    def text(self, response: str) -&gt; None:\n        Console(soft_wrap=True).print(response)\n\n    def schema(self, schema: Schema) -&gt; None:\n        output_table = self._table\n        for field in schema.fields:\n            output_table.add_row(field.name, str(field.field_type), field.doc or \"\")\n        Console().print(output_table)\n\n    def spec(self, spec: PartitionSpec) -&gt; None:\n        Console().print(str(spec))\n\n    def uuid(self, uuid: Optional[UUID]) -&gt; None:\n        Console().print(str(uuid) if uuid else \"missing\")\n\n    def version(self, version: str) -&gt; None:\n        Console().print(version)\n\n    def describe_refs(self, ref_details: List[Tuple[str, SnapshotRefType, Dict[str, str]]]) -&gt; None:\n        refs_table = RichTable(title=\"Snapshot Refs\")\n        refs_table.add_column(\"Ref\")\n        refs_table.add_column(\"Type\")\n        refs_table.add_column(\"Max ref age ms\")\n        refs_table.add_column(\"Min snapshots to keep\")\n        refs_table.add_column(\"Max snapshot age ms\")\n        for name, type, ref_detail in ref_details:\n            refs_table.add_row(\n                name, type, ref_detail[\"max_ref_age_ms\"], ref_detail[\"min_snapshots_to_keep\"], ref_detail[\"max_snapshot_age_ms\"]\n            )\n        Console().print(refs_table)\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.JsonOutput","title":"<code>JsonOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>Writes json to stdout.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class JsonOutput(Output):\n    \"\"\"Writes json to stdout.\"\"\"\n\n    verbose: bool\n\n    def __init__(self, **properties: Any) -&gt; None:\n        self.verbose = properties.get(\"verbose\", False)\n\n    def _out(self, d: Any) -&gt; None:\n        print(json.dumps(d))\n\n    def exception(self, ex: Exception) -&gt; None:\n        self._out({\"type\": ex.__class__.__name__, \"message\": str(ex)})\n\n    def identifiers(self, identifiers: List[Identifier]) -&gt; None:\n        self._out([\".\".join(identifier) for identifier in identifiers])\n\n    def describe_table(self, table: Table) -&gt; None:\n        class FauxTable(IcebergBaseModel):\n            \"\"\"Just to encode it using Pydantic.\"\"\"\n\n            identifier: Identifier\n            metadata_location: str\n            metadata: TableMetadata\n\n        print(\n            FauxTable(\n                identifier=table.name(), metadata=table.metadata, metadata_location=table.metadata_location\n            ).model_dump_json()\n        )\n\n    def describe_properties(self, properties: Properties) -&gt; None:\n        self._out(properties)\n\n    def text(self, response: str) -&gt; None:\n        print(json.dumps(response))\n\n    def schema(self, schema: Schema) -&gt; None:\n        print(schema.model_dump_json())\n\n    def files(self, table: Table, history: bool) -&gt; None:\n        pass\n\n    def spec(self, spec: PartitionSpec) -&gt; None:\n        print(spec.model_dump_json())\n\n    def uuid(self, uuid: Optional[UUID]) -&gt; None:\n        self._out({\"uuid\": str(uuid) if uuid else \"missing\"})\n\n    def version(self, version: str) -&gt; None:\n        self._out({\"version\": version})\n\n    def describe_refs(self, refs: List[Tuple[str, SnapshotRefType, Dict[str, str]]]) -&gt; None:\n        self._out(\n            [\n                {\"name\": name, \"type\": type, detail_key: detail_val}\n                for name, type, detail in refs\n                for detail_key, detail_val in detail.items()\n            ]\n        )\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Output interface for exporting.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class Output(ABC):\n    \"\"\"Output interface for exporting.\"\"\"\n\n    @abstractmethod\n    def exception(self, ex: Exception) -&gt; None: ...\n\n    @abstractmethod\n    def identifiers(self, identifiers: List[Identifier]) -&gt; None: ...\n\n    @abstractmethod\n    def describe_table(self, table: Table) -&gt; None: ...\n\n    @abstractmethod\n    def files(self, table: Table, history: bool) -&gt; None: ...\n\n    @abstractmethod\n    def describe_properties(self, properties: Properties) -&gt; None: ...\n\n    @abstractmethod\n    def text(self, response: str) -&gt; None: ...\n\n    @abstractmethod\n    def schema(self, schema: Schema) -&gt; None: ...\n\n    @abstractmethod\n    def spec(self, spec: PartitionSpec) -&gt; None: ...\n\n    @abstractmethod\n    def uuid(self, uuid: Optional[UUID]) -&gt; None: ...\n\n    @abstractmethod\n    def version(self, version: str) -&gt; None: ...\n\n    @abstractmethod\n    def describe_refs(self, refs: List[Tuple[str, SnapshotRefType, Dict[str, str]]]) -&gt; None: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/","title":"expressions","text":""},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse","title":"<code>AlwaysFalse</code>","text":"<p>               Bases: <code>BooleanExpression</code>, <code>Singleton</code></p> <p>FALSE expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class AlwaysFalse(BooleanExpression, Singleton):\n    \"\"\"FALSE expression.\"\"\"\n\n    def __invert__(self) -&gt; AlwaysTrue:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return AlwaysTrue()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n        return \"AlwaysFalse()\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n        return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; AlwaysTrue:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return AlwaysTrue()\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AlwaysFalse class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n    return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AlwaysFalse class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n    return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue","title":"<code>AlwaysTrue</code>","text":"<p>               Bases: <code>BooleanExpression</code>, <code>Singleton</code></p> <p>TRUE expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class AlwaysTrue(BooleanExpression, Singleton):\n    \"\"\"TRUE expression.\"\"\"\n\n    def __invert__(self) -&gt; AlwaysFalse:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return AlwaysFalse()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n        return \"AlwaysTrue()\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n        return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; AlwaysFalse:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return AlwaysFalse()\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AlwaysTrue class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n    return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AlwaysTrue class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n    return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And","title":"<code>And</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>AND operation expression - logical conjunction.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class And(BooleanExpression):\n    \"\"\"AND operation expression - logical conjunction.\"\"\"\n\n    left: BooleanExpression\n    right: BooleanExpression\n\n    def __new__(cls, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression) -&gt; BooleanExpression:  # type: ignore\n        if rest:\n            return reduce(And, (left, right, *rest))\n        if left is AlwaysFalse() or right is AlwaysFalse():\n            return AlwaysFalse()\n        elif left is AlwaysTrue():\n            return right\n        elif right is AlwaysTrue():\n            return left\n        else:\n            obj = super().__new__(cls)\n            obj.left = left\n            obj.right = right\n            return obj\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the And class.\"\"\"\n        return self.left == other.left and self.right == other.right if isinstance(other, And) else False\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the And class.\"\"\"\n        return f\"And(left={str(self.left)}, right={str(self.right)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the And class.\"\"\"\n        return f\"And(left={repr(self.left)}, right={repr(self.right)})\"\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        # De Morgan's law: not (A and B) = (not A) or (not B)\n        return Or(~self.left, ~self.right)\n\n    def __getnewargs__(self) -&gt; Tuple[BooleanExpression, BooleanExpression]:\n        \"\"\"Pickle the And class.\"\"\"\n        return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the And class.\"\"\"\n    return self.left == other.left and self.right == other.right if isinstance(other, And) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[BooleanExpression, BooleanExpression]:\n    \"\"\"Pickle the And class.\"\"\"\n    return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    # De Morgan's law: not (A and B) = (not A) or (not B)\n    return Or(~self.left, ~self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the And class.\"\"\"\n    return f\"And(left={repr(self.left)}, right={repr(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the And class.\"\"\"\n    return f\"And(left={str(self.left)}, right={str(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression","title":"<code>BooleanExpression</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An expression that evaluates to a boolean.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BooleanExpression(ABC):\n    \"\"\"An expression that evaluates to a boolean.\"\"\"\n\n    @abstractmethod\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n\n    def __and__(self, other: BooleanExpression) -&gt; BooleanExpression:\n        \"\"\"Perform and operation on another expression.\"\"\"\n        if not isinstance(other, BooleanExpression):\n            raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n        return And(self, other)\n\n    def __or__(self, other: BooleanExpression) -&gt; BooleanExpression:\n        \"\"\"Perform or operation on another expression.\"\"\"\n        if not isinstance(other, BooleanExpression):\n            raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n        return Or(self, other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform and operation on another expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __and__(self, other: BooleanExpression) -&gt; BooleanExpression:\n    \"\"\"Perform and operation on another expression.\"\"\"\n    if not isinstance(other, BooleanExpression):\n        raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n    return And(self, other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__invert__","title":"<code>__invert__()</code>  <code>abstractmethod</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform or operation on another expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __or__(self, other: BooleanExpression) -&gt; BooleanExpression:\n    \"\"\"Perform or operation on another expression.\"\"\"\n    if not isinstance(other, BooleanExpression):\n        raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n    return Or(self, other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Bound","title":"<code>Bound</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Represents a bound value expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Bound(ABC):\n    \"\"\"Represents a bound value expression.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundEqualTo","title":"<code>BoundEqualTo</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundEqualTo(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundNotEqualTo[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotEqualTo[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[EqualTo[L]]:\n        return EqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotEqualTo[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotEqualTo[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThan","title":"<code>BoundGreaterThan</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundGreaterThan(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundLessThanOrEqual[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundLessThanOrEqual(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[GreaterThan[L]]:\n        return GreaterThan[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundLessThanOrEqual[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundLessThanOrEqual(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThanOrEqual","title":"<code>BoundGreaterThanOrEqual</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundGreaterThanOrEqual(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundLessThan[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundLessThan[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[GreaterThanOrEqual[L]]:\n        return GreaterThanOrEqual[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundLessThan[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundLessThan[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn","title":"<code>BoundIn</code>","text":"<p>               Bases: <code>BoundSetPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIn(BoundSetPredicate[L]):\n    def __new__(cls, term: BoundTerm[L], literals: Set[Literal[L]]) -&gt; BooleanExpression:  # type: ignore  # pylint: disable=W0221\n        count = len(literals)\n        if count == 0:\n            return AlwaysFalse()\n        elif count == 1:\n            return BoundEqualTo(term, next(iter(literals)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundNotIn[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotIn(self.term, self.literals)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundIn class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    @property\n    def as_unbound(self) -&gt; Type[In[L]]:\n        return In\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundIn class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundIn class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotIn[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotIn(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNaN","title":"<code>BoundIsNaN</code>","text":"<p>               Bases: <code>BoundUnaryPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIsNaN(BoundUnaryPredicate[L]):\n    def __new__(cls, term: BoundTerm[L]) -&gt; BooleanExpression:  # type: ignore  # pylint: disable=W0221\n        bound_type = term.ref().field.field_type\n        if isinstance(bound_type, (FloatType, DoubleType)):\n            return super().__new__(cls)\n        return AlwaysFalse()\n\n    def __invert__(self) -&gt; BoundNotNaN[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotNaN(self.term)\n\n    @property\n    def as_unbound(self) -&gt; Type[IsNaN]:\n        return IsNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotNaN[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNull","title":"<code>BoundIsNull</code>","text":"<p>               Bases: <code>BoundUnaryPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIsNull(BoundUnaryPredicate[L]):\n    def __new__(cls, term: BoundTerm[L]) -&gt; BooleanExpression:  # type: ignore  # pylint: disable=W0221\n        if term.ref().field.required:\n            return AlwaysFalse()\n        return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundNotNull[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotNull(self.term)\n\n    @property\n    def as_unbound(self) -&gt; Type[IsNull]:\n        return IsNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotNull[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThan","title":"<code>BoundLessThan</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLessThan(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundGreaterThanOrEqual[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundGreaterThanOrEqual[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[LessThan[L]]:\n        return LessThan[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundGreaterThanOrEqual[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundGreaterThanOrEqual[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThanOrEqual","title":"<code>BoundLessThanOrEqual</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLessThanOrEqual(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundGreaterThan[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundGreaterThan[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[LessThanOrEqual[L]]:\n        return LessThanOrEqual[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundGreaterThan[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundGreaterThan[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate","title":"<code>BoundLiteralPredicate</code>","text":"<p>               Bases: <code>BoundPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLiteralPredicate(BoundPredicate[L], ABC):\n    literal: Literal[L]\n\n    def __init__(self, term: BoundTerm[L], literal: Literal[L]):  # pylint: disable=W0621\n        # Since we don't know the type of BoundPredicate[L], we have to ignore this one\n        super().__init__(term)  # type: ignore\n        self.literal = literal  # pylint: disable=W0621\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundLiteralPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term and self.literal == other.literal\n        return False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; Type[LiteralPredicate[L]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundLiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundLiteralPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term and self.literal == other.literal\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundLiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotEqualTo","title":"<code>BoundNotEqualTo</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotEqualTo(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundEqualTo[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundEqualTo[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[NotEqualTo[L]]:\n        return NotEqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundEqualTo[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundEqualTo[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotIn","title":"<code>BoundNotIn</code>","text":"<p>               Bases: <code>BoundSetPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotIn(BoundSetPredicate[L]):\n    def __new__(  # type: ignore  # pylint: disable=W0221\n        cls,\n        term: BoundTerm[L],\n        literals: Set[Literal[L]],\n    ) -&gt; BooleanExpression:\n        count = len(literals)\n        if count == 0:\n            return AlwaysTrue()\n        elif count == 1:\n            return BoundNotEqualTo(term, next(iter(literals)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundIn[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIn(self.term, self.literals)\n\n    @property\n    def as_unbound(self) -&gt; Type[NotIn[L]]:\n        return NotIn\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIn[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIn(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNaN","title":"<code>BoundNotNaN</code>","text":"<p>               Bases: <code>BoundUnaryPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotNaN(BoundUnaryPredicate[L]):\n    def __new__(cls, term: BoundTerm[L]) -&gt; BooleanExpression:  # type: ignore  # pylint: disable=W0221\n        bound_type = term.ref().field.field_type\n        if isinstance(bound_type, (FloatType, DoubleType)):\n            return super().__new__(cls)\n        return AlwaysTrue()\n\n    def __invert__(self) -&gt; BoundIsNaN[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIsNaN(self.term)\n\n    @property\n    def as_unbound(self) -&gt; Type[NotNaN]:\n        return NotNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIsNaN[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIsNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNull","title":"<code>BoundNotNull</code>","text":"<p>               Bases: <code>BoundUnaryPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotNull(BoundUnaryPredicate[L]):\n    def __new__(cls, term: BoundTerm[L]):  # type: ignore  # pylint: disable=W0221\n        if term.ref().field.required:\n            return AlwaysTrue()\n        return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundIsNull[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIsNull(self.term)\n\n    @property\n    def as_unbound(self) -&gt; Type[NotNull]:\n        return NotNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIsNull[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIsNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotStartsWith","title":"<code>BoundNotStartsWith</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotStartsWith(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundStartsWith[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundStartsWith[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[NotStartsWith[L]]:\n        return NotStartsWith[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundStartsWith[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundStartsWith[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundPredicate","title":"<code>BoundPredicate</code>","text":"<p>               Bases: <code>Generic[L]</code>, <code>Bound</code>, <code>BooleanExpression</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundPredicate(Generic[L], Bound, BooleanExpression, ABC):\n    term: BoundTerm[L]\n\n    def __init__(self, term: BoundTerm[L]):\n        self.term = term\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term\n        return False\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; Type[UnboundPredicate[Any]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference","title":"<code>BoundReference</code>","text":"<p>               Bases: <code>BoundTerm[L]</code></p> <p>A reference bound to a field in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>NestedField</code> <p>A referenced field in an Iceberg schema.</p> required <code>accessor</code> <code>Accessor</code> <p>An Accessor object to access the value at the field's position.</p> required Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundReference(BoundTerm[L]):\n    \"\"\"A reference bound to a field in a schema.\n\n    Args:\n        field (NestedField): A referenced field in an Iceberg schema.\n        accessor (Accessor): An Accessor object to access the value at the field's position.\n    \"\"\"\n\n    field: NestedField\n    accessor: Accessor\n\n    def __init__(self, field: NestedField, accessor: Accessor):\n        self.field = field\n        self.accessor = accessor\n\n    def eval(self, struct: StructProtocol) -&gt; L:\n        \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\n\n        Args:\n            struct (StructProtocol): A row object that abides by the StructProtocol and returns values given a position.\n        Returns:\n            Any: The value at the referenced field's position in `struct`.\n        \"\"\"\n        return self.accessor.get(struct)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundReference class.\"\"\"\n        return self.field == other.field if isinstance(other, BoundReference) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundReference class.\"\"\"\n        return f\"BoundReference(field={repr(self.field)}, accessor={repr(self.accessor)})\"\n\n    def ref(self) -&gt; BoundReference[L]:\n        return self\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return hash value of the BoundReference class.\"\"\"\n        return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundReference class.\"\"\"\n    return self.field == other.field if isinstance(other, BoundReference) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__hash__","title":"<code>__hash__()</code>","text":"<p>Return hash value of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash value of the BoundReference class.\"\"\"\n    return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundReference class.\"\"\"\n    return f\"BoundReference(field={repr(self.field)}, accessor={repr(self.accessor)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.eval","title":"<code>eval(struct)</code>","text":"<p>Return the value at the referenced field's position in an object that abides by the StructProtocol.</p> <p>Parameters:</p> Name Type Description Default <code>struct</code> <code>StructProtocol</code> <p>A row object that abides by the StructProtocol and returns values given a position.</p> required <p>Returns:     Any: The value at the referenced field's position in <code>struct</code>.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def eval(self, struct: StructProtocol) -&gt; L:\n    \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\n\n    Args:\n        struct (StructProtocol): A row object that abides by the StructProtocol and returns values given a position.\n    Returns:\n        Any: The value at the referenced field's position in `struct`.\n    \"\"\"\n    return self.accessor.get(struct)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate","title":"<code>BoundSetPredicate</code>","text":"<p>               Bases: <code>BoundPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundSetPredicate(BoundPredicate[L], ABC):\n    literals: Set[Literal[L]]\n\n    def __init__(self, term: BoundTerm[L], literals: Set[Literal[L]]):\n        # Since we don't know the type of BoundPredicate[L], we have to ignore this one\n        super().__init__(term)  # type: ignore\n        self.literals = _to_literal_set(literals)  # pylint: disable=W0621\n\n    @cached_property\n    def value_set(self) -&gt; Set[L]:\n        return {lit.value for lit in self.literals}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{', '.join(sorted([str(literal) for literal in self.literals]))}}})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{', '.join(sorted([repr(literal) for literal in self.literals]))}}})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundSetPredicate class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    def __getnewargs__(self) -&gt; Tuple[BoundTerm[L], Set[Literal[L]]]:\n        \"\"\"Pickle the BoundSetPredicate class.\"\"\"\n        return (self.term, self.literals)\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; Type[SetPredicate[L]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundSetPredicate class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[BoundTerm[L], Set[Literal[L]]]:\n    \"\"\"Pickle the BoundSetPredicate class.\"\"\"\n    return (self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{', '.join(sorted([repr(literal) for literal in self.literals]))}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{', '.join(sorted([str(literal) for literal in self.literals]))}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundStartsWith","title":"<code>BoundStartsWith</code>","text":"<p>               Bases: <code>BoundLiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundStartsWith(BoundLiteralPredicate[L]):\n    def __invert__(self) -&gt; BoundNotStartsWith[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotStartsWith[L](self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; Type[StartsWith[L]]:\n        return StartsWith[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotStartsWith[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotStartsWith[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm","title":"<code>BoundTerm</code>","text":"<p>               Bases: <code>Term[L]</code>, <code>Bound</code>, <code>ABC</code></p> <p>Represents a bound term.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundTerm(Term[L], Bound, ABC):\n    \"\"\"Represents a bound term.\"\"\"\n\n    @abstractmethod\n    def ref(self) -&gt; BoundReference[L]:\n        \"\"\"Return the bound reference.\"\"\"\n\n    @abstractmethod\n    def eval(self, struct: StructProtocol) -&gt; L:  # pylint: disable=W0613\n        \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm.eval","title":"<code>eval(struct)</code>  <code>abstractmethod</code>","text":"<p>Return the value at the referenced field's position in an object that abides by the StructProtocol.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef eval(self, struct: StructProtocol) -&gt; L:  # pylint: disable=W0613\n    \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm.ref","title":"<code>ref()</code>  <code>abstractmethod</code>","text":"<p>Return the bound reference.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef ref(self) -&gt; BoundReference[L]:\n    \"\"\"Return the bound reference.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate","title":"<code>BoundUnaryPredicate</code>","text":"<p>               Bases: <code>BoundPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundUnaryPredicate(BoundPredicate[L], ABC):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundUnaryPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; Type[UnaryPredicate]: ...\n\n    def __getnewargs__(self) -&gt; Tuple[BoundTerm[L]]:\n        \"\"\"Pickle the BoundUnaryPredicate class.\"\"\"\n        return (self.term,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the BoundUnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[BoundTerm[L]]:\n    \"\"\"Pickle the BoundUnaryPredicate class.\"\"\"\n    return (self.term,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundUnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundUnaryPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.EqualTo","title":"<code>EqualTo</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class EqualTo(LiteralPredicate[L]):\n    def __invert__(self) -&gt; NotEqualTo[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotEqualTo[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundEqualTo[L]]:\n        return BoundEqualTo[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.EqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotEqualTo[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotEqualTo[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThan","title":"<code>GreaterThan</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class GreaterThan(LiteralPredicate[L]):\n    def __invert__(self) -&gt; LessThanOrEqual[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return LessThanOrEqual[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundGreaterThan[L]]:\n        return BoundGreaterThan[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; LessThanOrEqual[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return LessThanOrEqual[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThanOrEqual","title":"<code>GreaterThanOrEqual</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class GreaterThanOrEqual(LiteralPredicate[L]):\n    def __invert__(self) -&gt; LessThan[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return LessThan[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundGreaterThanOrEqual[L]]:\n        return BoundGreaterThanOrEqual[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; LessThan[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return LessThan[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.In","title":"<code>In</code>","text":"<p>               Bases: <code>SetPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class In(SetPredicate[L]):\n    def __new__(  # type: ignore  # pylint: disable=W0221\n        cls, term: Union[str, UnboundTerm[Any]], literals: Union[Iterable[L], Iterable[Literal[L]]]\n    ) -&gt; BooleanExpression:\n        literals_set: Set[Literal[L]] = _to_literal_set(literals)\n        count = len(literals_set)\n        if count == 0:\n            return AlwaysFalse()\n        elif count == 1:\n            return EqualTo(term, next(iter(literals)))  # type: ignore\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; NotIn[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotIn[L](self.term, self.literals)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundIn[L]]:\n        return BoundIn[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.In.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotIn[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotIn[L](self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNaN","title":"<code>IsNaN</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class IsNaN(UnaryPredicate):\n    def __invert__(self) -&gt; NotNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotNaN(self.term)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundIsNaN[L]]:\n        return BoundIsNaN[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNull","title":"<code>IsNull</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class IsNull(UnaryPredicate):\n    def __invert__(self) -&gt; NotNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotNull(self.term)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundIsNull[L]]:\n        return BoundIsNull[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThan","title":"<code>LessThan</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LessThan(LiteralPredicate[L]):\n    def __invert__(self) -&gt; GreaterThanOrEqual[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return GreaterThanOrEqual[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundLessThan[L]]:\n        return BoundLessThan[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; GreaterThanOrEqual[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return GreaterThanOrEqual[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThanOrEqual","title":"<code>LessThanOrEqual</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LessThanOrEqual(LiteralPredicate[L]):\n    def __invert__(self) -&gt; GreaterThan[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return GreaterThan[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundLessThanOrEqual[L]]:\n        return BoundLessThanOrEqual[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; GreaterThan[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return GreaterThan[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate","title":"<code>LiteralPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LiteralPredicate(UnboundPredicate[L], ABC):\n    literal: Literal[L]\n\n    def __init__(self, term: Union[str, UnboundTerm[Any]], literal: Union[L, Literal[L]]):  # pylint: disable=W0621\n        super().__init__(term)\n        self.literal = _to_literal(literal)  # pylint: disable=W0621\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundLiteralPredicate[L]:\n        bound_term = self.term.bind(schema, case_sensitive)\n        lit = self.literal.to(bound_term.ref().field.field_type)\n\n        if isinstance(lit, AboveMax):\n            if isinstance(self, (LessThan, LessThanOrEqual, NotEqualTo)):\n                return AlwaysTrue()  # type: ignore\n            elif isinstance(self, (GreaterThan, GreaterThanOrEqual, EqualTo)):\n                return AlwaysFalse()  # type: ignore\n        elif isinstance(lit, BelowMin):\n            if isinstance(self, (GreaterThan, GreaterThanOrEqual, NotEqualTo)):\n                return AlwaysTrue()  # type: ignore\n            elif isinstance(self, (LessThan, LessThanOrEqual, EqualTo)):\n                return AlwaysFalse()  # type: ignore\n\n        return self.as_bound(bound_term, lit)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the LiteralPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term and self.literal == other.literal\n        return False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; Type[BoundLiteralPredicate[L]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the LiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the LiteralPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term and self.literal == other.literal\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the LiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not","title":"<code>Not</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>NOT operation expression - logical negation.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Not(BooleanExpression):\n    \"\"\"NOT operation expression - logical negation.\"\"\"\n\n    child: BooleanExpression\n\n    def __new__(cls, child: BooleanExpression) -&gt; BooleanExpression:  # type: ignore\n        if child is AlwaysTrue():\n            return AlwaysFalse()\n        elif child is AlwaysFalse():\n            return AlwaysTrue()\n        elif isinstance(child, Not):\n            return child.child\n        obj = super().__new__(cls)\n        obj.child = child\n        return obj\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Not class.\"\"\"\n        return f\"Not(child={repr(self.child)})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Not class.\"\"\"\n        return self.child == other.child if isinstance(other, Not) else False\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return self.child\n\n    def __getnewargs__(self) -&gt; Tuple[BooleanExpression]:\n        \"\"\"Pickle the Not class.\"\"\"\n        return (self.child,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Not class.\"\"\"\n    return self.child == other.child if isinstance(other, Not) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[BooleanExpression]:\n    \"\"\"Pickle the Not class.\"\"\"\n    return (self.child,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return self.child\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Not class.\"\"\"\n    return f\"Not(child={repr(self.child)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotEqualTo","title":"<code>NotEqualTo</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotEqualTo(LiteralPredicate[L]):\n    def __invert__(self) -&gt; EqualTo[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return EqualTo[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundNotEqualTo[L]]:\n        return BoundNotEqualTo[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; EqualTo[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return EqualTo[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotIn","title":"<code>NotIn</code>","text":"<p>               Bases: <code>SetPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotIn(SetPredicate[L], ABC):\n    def __new__(  # type: ignore  # pylint: disable=W0221\n        cls, term: Union[str, UnboundTerm[Any]], literals: Union[Iterable[L], Iterable[Literal[L]]]\n    ) -&gt; BooleanExpression:\n        literals_set: Set[Literal[L]] = _to_literal_set(literals)\n        count = len(literals_set)\n        if count == 0:\n            return AlwaysTrue()\n        elif count == 1:\n            return NotEqualTo(term, next(iter(literals_set)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; In[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return In[L](self.term, self.literals)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundNotIn[L]]:\n        return BoundNotIn[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; In[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return In[L](self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNaN","title":"<code>NotNaN</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotNaN(UnaryPredicate):\n    def __invert__(self) -&gt; IsNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return IsNaN(self.term)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundNotNaN[L]]:\n        return BoundNotNaN[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; IsNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return IsNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNull","title":"<code>NotNull</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotNull(UnaryPredicate):\n    def __invert__(self) -&gt; IsNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return IsNull(self.term)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundNotNull[L]]:\n        return BoundNotNull[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; IsNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return IsNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotStartsWith","title":"<code>NotStartsWith</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotStartsWith(LiteralPredicate[L]):\n    def __invert__(self) -&gt; StartsWith[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return StartsWith[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundNotStartsWith[L]]:\n        return BoundNotStartsWith[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; StartsWith[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return StartsWith[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or","title":"<code>Or</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>OR operation expression - logical disjunction.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Or(BooleanExpression):\n    \"\"\"OR operation expression - logical disjunction.\"\"\"\n\n    left: BooleanExpression\n    right: BooleanExpression\n\n    def __new__(cls, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression) -&gt; BooleanExpression:  # type: ignore\n        if rest:\n            return reduce(Or, (left, right, *rest))\n        if left is AlwaysTrue() or right is AlwaysTrue():\n            return AlwaysTrue()\n        elif left is AlwaysFalse():\n            return right\n        elif right is AlwaysFalse():\n            return left\n        else:\n            obj = super().__new__(cls)\n            obj.left = left\n            obj.right = right\n            return obj\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Or class.\"\"\"\n        return self.left == other.left and self.right == other.right if isinstance(other, Or) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Or class.\"\"\"\n        return f\"Or(left={repr(self.left)}, right={repr(self.right)})\"\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        # De Morgan's law: not (A or B) = (not A) and (not B)\n        return And(~self.left, ~self.right)\n\n    def __getnewargs__(self) -&gt; Tuple[BooleanExpression, BooleanExpression]:\n        \"\"\"Pickle the Or class.\"\"\"\n        return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Or class.\"\"\"\n    return self.left == other.left and self.right == other.right if isinstance(other, Or) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[BooleanExpression, BooleanExpression]:\n    \"\"\"Pickle the Or class.\"\"\"\n    return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    # De Morgan's law: not (A or B) = (not A) and (not B)\n    return And(~self.left, ~self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Or class.\"\"\"\n    return f\"Or(left={repr(self.left)}, right={repr(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference","title":"<code>Reference</code>","text":"<p>               Bases: <code>UnboundTerm[Any]</code></p> <p>A reference not yet bound to a field in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required Note <p>An unbound reference is sometimes referred to as a \"named\" reference.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Reference(UnboundTerm[Any]):\n    \"\"\"A reference not yet bound to a field in a schema.\n\n    Args:\n        name (str): The name of the field.\n\n    Note:\n        An unbound reference is sometimes referred to as a \"named\" reference.\n    \"\"\"\n\n    name: str\n\n    def __init__(self, name: str) -&gt; None:\n        self.name = name\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Reference class.\"\"\"\n        return f\"Reference(name={repr(self.name)})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Reference class.\"\"\"\n        return self.name == other.name if isinstance(other, Reference) else False\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundReference[L]:\n        \"\"\"Bind the reference to an Iceberg schema.\n\n        Args:\n            schema (Schema): An Iceberg schema.\n            case_sensitive (bool): Whether to consider case when binding the reference to the field.\n\n        Raises:\n            ValueError: If an empty name is provided.\n\n        Returns:\n            BoundReference: A reference bound to the specific field in the Iceberg schema.\n        \"\"\"\n        field = schema.find_field(name_or_id=self.name, case_sensitive=case_sensitive)\n        accessor = schema.accessor_for_field(field.field_id)\n        return self.as_bound(field=field, accessor=accessor)  # type: ignore\n\n    @property\n    def as_bound(self) -&gt; Type[BoundReference[L]]:\n        return BoundReference[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Reference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Reference class.\"\"\"\n    return self.name == other.name if isinstance(other, Reference) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Reference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Reference class.\"\"\"\n    return f\"Reference(name={repr(self.name)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.bind","title":"<code>bind(schema, case_sensitive=True)</code>","text":"<p>Bind the reference to an Iceberg schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>An Iceberg schema.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding the reference to the field.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an empty name is provided.</p> <p>Returns:</p> Name Type Description <code>BoundReference</code> <code>BoundReference[L]</code> <p>A reference bound to the specific field in the Iceberg schema.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundReference[L]:\n    \"\"\"Bind the reference to an Iceberg schema.\n\n    Args:\n        schema (Schema): An Iceberg schema.\n        case_sensitive (bool): Whether to consider case when binding the reference to the field.\n\n    Raises:\n        ValueError: If an empty name is provided.\n\n    Returns:\n        BoundReference: A reference bound to the specific field in the Iceberg schema.\n    \"\"\"\n    field = schema.find_field(name_or_id=self.name, case_sensitive=case_sensitive)\n    accessor = schema.accessor_for_field(field.field_id)\n    return self.as_bound(field=field, accessor=accessor)  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate","title":"<code>SetPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate[L]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class SetPredicate(UnboundPredicate[L], ABC):\n    literals: Set[Literal[L]]\n\n    def __init__(self, term: Union[str, UnboundTerm[Any]], literals: Union[Iterable[L], Iterable[Literal[L]]]):\n        super().__init__(term)\n        self.literals = _to_literal_set(literals)\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundSetPredicate[L]:\n        bound_term = self.term.bind(schema, case_sensitive)\n        return self.as_bound(bound_term, {lit.to(bound_term.ref().field.field_type) for lit in self.literals})\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{', '.join(sorted([str(literal) for literal in self.literals]))}}})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{', '.join(sorted([repr(literal) for literal in self.literals]))}}})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the SetPredicate class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    def __getnewargs__(self) -&gt; Tuple[UnboundTerm[L], Set[Literal[L]]]:\n        \"\"\"Pickle the SetPredicate class.\"\"\"\n        return (self.term, self.literals)\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; Type[BoundSetPredicate[L]]:\n        return BoundSetPredicate[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the SetPredicate class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; Tuple[UnboundTerm[L], Set[Literal[L]]]:\n    \"\"\"Pickle the SetPredicate class.\"\"\"\n    return (self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{', '.join(sorted([repr(literal) for literal in self.literals]))}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{', '.join(sorted([str(literal) for literal in self.literals]))}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.StartsWith","title":"<code>StartsWith</code>","text":"<p>               Bases: <code>LiteralPredicate[L]</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class StartsWith(LiteralPredicate[L]):\n    def __invert__(self) -&gt; NotStartsWith[L]:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotStartsWith[L](self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; Type[BoundStartsWith[L]]:\n        return BoundStartsWith[L]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.StartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotStartsWith[L]:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotStartsWith[L](self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Term","title":"<code>Term</code>","text":"<p>               Bases: <code>Generic[L]</code>, <code>ABC</code></p> <p>A simple expression that evaluates to a value.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Term(Generic[L], ABC):\n    \"\"\"A simple expression that evaluates to a value.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnaryPredicate","title":"<code>UnaryPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate[Any]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnaryPredicate(UnboundPredicate[Any], ABC):\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundUnaryPredicate[Any]:\n        bound_term = self.term.bind(schema, case_sensitive)\n        return self.as_bound(bound_term)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; Type[BoundUnaryPredicate[Any]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnaryPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the UnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Unbound","title":"<code>Unbound</code>","text":"<p>               Bases: <code>Generic[B]</code>, <code>ABC</code></p> <p>Represents an unbound value expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Unbound(Generic[B], ABC):\n    \"\"\"Represents an unbound value expression.\"\"\"\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; B: ...\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; Type[Bound]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundPredicate","title":"<code>UnboundPredicate</code>","text":"<p>               Bases: <code>Generic[L]</code>, <code>Unbound[BooleanExpression]</code>, <code>BooleanExpression</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnboundPredicate(Generic[L], Unbound[BooleanExpression], BooleanExpression, ABC):\n    term: UnboundTerm[Any]\n\n    def __init__(self, term: Union[str, UnboundTerm[Any]]):\n        self.term = _to_unbound_term(term)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the UnboundPredicate class.\"\"\"\n        return self.term == other.term if isinstance(other, self.__class__) else False\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BooleanExpression: ...\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; Type[BoundPredicate[L]]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the UnboundPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the UnboundPredicate class.\"\"\"\n    return self.term == other.term if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundTerm","title":"<code>UnboundTerm</code>","text":"<p>               Bases: <code>Term[Any]</code>, <code>Unbound[BoundTerm[L]]</code>, <code>ABC</code></p> <p>Represents an unbound term.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnboundTerm(Term[Any], Unbound[BoundTerm[L]], ABC):\n    \"\"\"Represents an unbound term.\"\"\"\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundTerm[L]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/","title":"literals","text":""},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax","title":"<code>AboveMax</code>","text":"<p>               Bases: <code>Literal[L]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class AboveMax(Literal[L]):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AboveMax class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AboveMax class.\"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AboveMax class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AboveMax class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AboveMax class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AboveMax class.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin","title":"<code>BelowMin</code>","text":"<p>               Bases: <code>Literal[L]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class BelowMin(Literal[L]):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BelowMin class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BelowMin class.\"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BelowMin class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BelowMin class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BelowMin class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BelowMin class.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral","title":"<code>FloatLiteral</code>","text":"<p>               Bases: <code>Literal[float]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class FloatLiteral(Literal[float]):\n    def __init__(self, value: float) -&gt; None:\n        super().__init__(value, float)\n        self._value32 = struct.unpack(\"&lt;f\", struct.pack(\"&lt;f\", value))[0]\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the FloatLiteral class.\"\"\"\n        return self._value32 == other\n\n    def __lt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is less than another instance.\"\"\"\n        return self._value32 &lt; other\n\n    def __gt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is greater than another instance.\"\"\"\n        return self._value32 &gt; other\n\n    def __le__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is less than or equal to another instance.\"\"\"\n        return self._value32 &lt;= other\n\n    def __ge__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is greater than or equal to another instance.\"\"\"\n        return self._value32 &gt;= other\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the FloatLiteral class.\"\"\"\n        return hash(self._value32)\n\n    @singledispatchmethod\n    def to(self, type_var: IcebergType) -&gt; Literal:  # type: ignore\n        raise TypeError(f\"Cannot convert FloatLiteral into {type_var}\")\n\n    @to.register(FloatType)\n    def _(self, _: FloatType) -&gt; Literal[float]:\n        return self\n\n    @to.register(DoubleType)\n    def _(self, _: DoubleType) -&gt; Literal[float]:\n        return DoubleLiteral(self.value)\n\n    @to.register(DecimalType)\n    def _(self, type_var: DecimalType) -&gt; Literal[Decimal]:\n        return DecimalLiteral(Decimal(self.value).quantize(Decimal((0, (1,), -type_var.scale)), rounding=ROUND_HALF_UP))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the FloatLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the FloatLiteral class.\"\"\"\n    return self._value32 == other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__ge__","title":"<code>__ge__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is greater than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is greater than or equal to another instance.\"\"\"\n    return self._value32 &gt;= other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is greater than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is greater than another instance.\"\"\"\n    return self._value32 &gt; other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the FloatLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the FloatLiteral class.\"\"\"\n    return hash(self._value32)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__le__","title":"<code>__le__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is less than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is less than or equal to another instance.\"\"\"\n    return self._value32 &lt;= other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is less than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is less than another instance.\"\"\"\n    return self._value32 &lt; other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal","title":"<code>Literal</code>","text":"<p>               Bases: <code>Generic[L]</code>, <code>ABC</code></p> <p>Literal which has a value and can be converted between types.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class Literal(Generic[L], ABC):\n    \"\"\"Literal which has a value and can be converted between types.\"\"\"\n\n    _value: L\n\n    def __init__(self, value: L, value_type: Type[L]):\n        if value is None or not isinstance(value, value_type):\n            raise TypeError(f\"Invalid literal value: {value!r} (not a {value_type})\")\n        if isinstance(value, float) and isnan(value):\n            raise ValueError(\"Cannot create expression literal from NaN.\")\n        self._value = value\n\n    @property\n    def value(self) -&gt; L:\n        return self._value\n\n    @singledispatchmethod\n    @abstractmethod\n    def to(self, type_var: IcebergType) -&gt; Literal[L]: ...  # pragma: no cover\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Literal class.\"\"\"\n        return f\"{type(self).__name__}({self.value!r})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Literal class.\"\"\"\n        return str(self.value)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the Literal class.\"\"\"\n        return hash(self.value)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Literal class.\"\"\"\n        if not isinstance(other, Literal):\n            return False\n        return self.value == other.value\n\n    def __ne__(self, other: Any) -&gt; bool:\n        \"\"\"Return the inequality of two instances of the Literal class.\"\"\"\n        return not self.__eq__(other)\n\n    def __lt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is less than another instance.\"\"\"\n        return self.value &lt; other.value\n\n    def __gt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is greater than another instance.\"\"\"\n        return self.value &gt; other.value\n\n    def __le__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is less than or equal to another instance.\"\"\"\n        return self.value &lt;= other.value\n\n    def __ge__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is greater than or equal to another instance.\"\"\"\n        return self.value &gt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Literal class.\"\"\"\n    if not isinstance(other, Literal):\n        return False\n    return self.value == other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__ge__","title":"<code>__ge__(other)</code>","text":"<p>Return if one instance of the Literal class is greater than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is greater than or equal to another instance.\"\"\"\n    return self.value &gt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Return if one instance of the Literal class is greater than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is greater than another instance.\"\"\"\n    return self.value &gt; other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the Literal class.\"\"\"\n    return hash(self.value)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__le__","title":"<code>__le__(other)</code>","text":"<p>Return if one instance of the Literal class is less than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is less than or equal to another instance.\"\"\"\n    return self.value &lt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Return if one instance of the Literal class is less than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is less than another instance.\"\"\"\n    return self.value &lt; other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__ne__","title":"<code>__ne__(other)</code>","text":"<p>Return the inequality of two instances of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ne__(self, other: Any) -&gt; bool:\n    \"\"\"Return the inequality of two instances of the Literal class.\"\"\"\n    return not self.__eq__(other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Literal class.\"\"\"\n    return f\"{type(self).__name__}({self.value!r})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Literal class.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.StringLiteral","title":"<code>StringLiteral</code>","text":"<p>               Bases: <code>Literal[str]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class StringLiteral(Literal[str]):\n    def __init__(self, value: str) -&gt; None:\n        super().__init__(value, str)\n\n    @singledispatchmethod\n    def to(self, type_var: IcebergType) -&gt; Literal:  # type: ignore\n        raise TypeError(f\"Cannot convert StringLiteral into {type_var}\")\n\n    @to.register(StringType)\n    def _(self, _: StringType) -&gt; Literal[str]:\n        return self\n\n    @to.register(IntegerType)\n    def _(self, type_var: IntegerType) -&gt; Literal[int]:\n        try:\n            number = int(float(self.value))\n\n            if IntegerType.max &lt; number:\n                return IntAboveMax()\n            elif IntegerType.min &gt; number:\n                return IntBelowMin()\n            return LongLiteral(number)\n        except ValueError as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(LongType)\n    def _(self, type_var: LongType) -&gt; Literal[int]:\n        try:\n            long_value = int(float(self.value))\n            if LongType.max &lt; long_value:\n                return LongAboveMax()\n            elif LongType.min &gt; long_value:\n                return LongBelowMin()\n            else:\n                return LongLiteral(long_value)\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(DateType)\n    def _(self, type_var: DateType) -&gt; Literal[int]:\n        try:\n            return DateLiteral(date_str_to_days(self.value))\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(TimeType)\n    def _(self, type_var: TimeType) -&gt; Literal[int]:\n        try:\n            return TimeLiteral(time_str_to_micros(self.value))\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(TimestampType)\n    def _(self, _: TimestampType) -&gt; Literal[int]:\n        return TimestampLiteral(timestamp_to_micros(self.value))\n\n    @to.register(TimestamptzType)\n    def _(self, _: TimestamptzType) -&gt; Literal[int]:\n        return TimestampLiteral(timestamptz_to_micros(self.value))\n\n    @to.register(UUIDType)\n    def _(self, _: UUIDType) -&gt; Literal[bytes]:\n        return UUIDLiteral(UUID(self.value).bytes)\n\n    @to.register(DecimalType)\n    def _(self, type_var: DecimalType) -&gt; Literal[Decimal]:\n        dec = Decimal(self.value)\n        scale = abs(int(dec.as_tuple().exponent))\n        if type_var.scale == scale:\n            return DecimalLiteral(dec)\n        else:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}, scales differ {type_var.scale} &lt;&gt; {scale}\")\n\n    @to.register(BooleanType)\n    def _(self, type_var: BooleanType) -&gt; Literal[bool]:\n        value_upper = self.value.upper()\n        if value_upper in [\"TRUE\", \"FALSE\"]:\n            return BooleanLiteral(value_upper == \"TRUE\")\n        else:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StringLiteral class.\"\"\"\n        return f\"literal({repr(self.value)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.StringLiteral.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StringLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StringLiteral class.\"\"\"\n    return f\"literal({repr(self.value)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.literal","title":"<code>literal(value)</code>","text":"<p>Construct an Iceberg Literal based on Python primitive data type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Python primitive type</code> <p>the value to be associated with literal.</p> required Example <p>from pyiceberg.expressions.literals import literal.</p> <p>literal(123) LongLiteral(123)</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def literal(value: L) -&gt; Literal[L]:\n    \"\"\"\n    Construct an Iceberg Literal based on Python primitive data type.\n\n    Args:\n        value (Python primitive type): the value to be associated with literal.\n\n    Example:\n        from pyiceberg.expressions.literals import literal.\n        &gt;&gt;&gt; literal(123)\n        LongLiteral(123)\n    \"\"\"\n    if isinstance(value, float):\n        return DoubleLiteral(value)  # type: ignore\n    elif isinstance(value, bool):\n        return BooleanLiteral(value)\n    elif isinstance(value, int):\n        return LongLiteral(value)\n    elif isinstance(value, str):\n        return StringLiteral(value)\n    elif isinstance(value, UUID):\n        return UUIDLiteral(value.bytes)  # type: ignore\n    elif isinstance(value, bytes):\n        return BinaryLiteral(value)\n    elif isinstance(value, Decimal):\n        return DecimalLiteral(value)\n    elif isinstance(value, datetime):\n        return TimestampLiteral(datetime_to_micros(value))  # type: ignore\n    elif isinstance(value, date):\n        return DateLiteral(date_to_days(value))  # type: ignore\n    else:\n        raise TypeError(f\"Invalid literal value: {repr(value)}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/parser/","title":"parser","text":""},{"location":"reference/pyiceberg/expressions/parser/#pyiceberg.expressions.parser.parse","title":"<code>parse(expr)</code>","text":"<p>Parse a boolean expression.</p> Source code in <code>pyiceberg/expressions/parser.py</code> <pre><code>def parse(expr: str) -&gt; BooleanExpression:\n    \"\"\"Parse a boolean expression.\"\"\"\n    return boolean_expression.parse_string(expr, parse_all=True)[0]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/","title":"visitors","text":""},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BindVisitor","title":"<code>BindVisitor</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[BooleanExpression]</code></p> <p>Rewrites a boolean expression by replacing unbound references with references to fields in a struct schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>A schema to use when binding the expression.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding a reference to a field in a schema, defaults to True.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case a predicate is already bound.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BindVisitor(BooleanExpressionVisitor[BooleanExpression]):\n    \"\"\"Rewrites a boolean expression by replacing unbound references with references to fields in a struct schema.\n\n    Args:\n      schema (Schema): A schema to use when binding the expression.\n      case_sensitive (bool): Whether to consider case when binding a reference to a field in a schema, defaults to True.\n\n    Raises:\n        TypeError: In the case a predicate is already bound.\n    \"\"\"\n\n    schema: Schema\n    case_sensitive: bool\n\n    def __init__(self, schema: Schema, case_sensitive: bool) -&gt; None:\n        self.schema = schema\n        self.case_sensitive = case_sensitive\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return Not(child=child_result)\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left=left_result, right=right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left=left_result, right=right_result)\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; BooleanExpression:\n        return predicate.bind(self.schema, case_sensitive=self.case_sensitive)\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; BooleanExpression:\n        raise TypeError(f\"Found already bound predicate: {predicate}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor","title":"<code>BooleanExpressionVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BooleanExpressionVisitor(Generic[T], ABC):\n    @abstractmethod\n    def visit_true(self) -&gt; T:\n        \"\"\"Visit method for an AlwaysTrue boolean expression.\n\n        Note: This visit method has no arguments since AlwaysTrue instances have no context.\n        \"\"\"\n\n    @abstractmethod\n    def visit_false(self) -&gt; T:\n        \"\"\"Visit method for an AlwaysFalse boolean expression.\n\n        Note: This visit method has no arguments since AlwaysFalse instances have no context.\n        \"\"\"\n\n    @abstractmethod\n    def visit_not(self, child_result: T) -&gt; T:\n        \"\"\"Visit method for a Not boolean expression.\n\n        Args:\n            child_result (T): The result of visiting the child of the Not boolean expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_and(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit method for an And boolean expression.\n\n        Args:\n            left_result (T): The result of visiting the left side of the expression.\n            right_result (T): The result of visiting the right side of the expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_or(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit method for an Or boolean expression.\n\n        Args:\n            left_result (T): The result of visiting the left side of the expression.\n            right_result (T): The result of visiting the right side of the expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; T:\n        \"\"\"Visit method for an unbound predicate in an expression tree.\n\n        Args:\n            predicate (UnboundPredicate[L): An instance of an UnboundPredicate.\n        \"\"\"\n\n    @abstractmethod\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; T:\n        \"\"\"Visit method for a bound predicate in an expression tree.\n\n        Args:\n            predicate (BoundPredicate[L]): An instance of a BoundPredicate.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_and","title":"<code>visit_and(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an And boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>left_result</code> <code>T</code> <p>The result of visiting the left side of the expression.</p> required <code>right_result</code> <code>T</code> <p>The result of visiting the right side of the expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_and(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit method for an And boolean expression.\n\n    Args:\n        left_result (T): The result of visiting the left side of the expression.\n        right_result (T): The result of visiting the right side of the expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>  <code>abstractmethod</code>","text":"<p>Visit method for a bound predicate in an expression tree.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>BoundPredicate[L]</code> <p>An instance of a BoundPredicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; T:\n    \"\"\"Visit method for a bound predicate in an expression tree.\n\n    Args:\n        predicate (BoundPredicate[L]): An instance of a BoundPredicate.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_false","title":"<code>visit_false()</code>  <code>abstractmethod</code>","text":"<p>Visit method for an AlwaysFalse boolean expression.</p> <p>Note: This visit method has no arguments since AlwaysFalse instances have no context.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_false(self) -&gt; T:\n    \"\"\"Visit method for an AlwaysFalse boolean expression.\n\n    Note: This visit method has no arguments since AlwaysFalse instances have no context.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_not","title":"<code>visit_not(child_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for a Not boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>child_result</code> <code>T</code> <p>The result of visiting the child of the Not boolean expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not(self, child_result: T) -&gt; T:\n    \"\"\"Visit method for a Not boolean expression.\n\n    Args:\n        child_result (T): The result of visiting the child of the Not boolean expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_or","title":"<code>visit_or(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an Or boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>left_result</code> <code>T</code> <p>The result of visiting the left side of the expression.</p> required <code>right_result</code> <code>T</code> <p>The result of visiting the right side of the expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_or(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit method for an Or boolean expression.\n\n    Args:\n        left_result (T): The result of visiting the left side of the expression.\n        right_result (T): The result of visiting the right side of the expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_true","title":"<code>visit_true()</code>  <code>abstractmethod</code>","text":"<p>Visit method for an AlwaysTrue boolean expression.</p> <p>Note: This visit method has no arguments since AlwaysTrue instances have no context.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_true(self) -&gt; T:\n    \"\"\"Visit method for an AlwaysTrue boolean expression.\n\n    Note: This visit method has no arguments since AlwaysTrue instances have no context.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_unbound_predicate","title":"<code>visit_unbound_predicate(predicate)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an unbound predicate in an expression tree.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>UnboundPredicate[L</code> <p>An instance of an UnboundPredicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; T:\n    \"\"\"Visit method for an unbound predicate in an expression tree.\n\n    Args:\n        predicate (UnboundPredicate[L): An instance of an UnboundPredicate.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor","title":"<code>BoundBooleanExpressionVisitor</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BoundBooleanExpressionVisitor(BooleanExpressionVisitor[T], ABC):\n    @abstractmethod\n    def visit_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; T:\n        \"\"\"Visit a bound In predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; T:\n        \"\"\"Visit a bound NotIn predicate.\"\"\"\n\n    @abstractmethod\n    def visit_is_nan(self, term: BoundTerm[L]) -&gt; T:\n        \"\"\"Visit a bound IsNan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_nan(self, term: BoundTerm[L]) -&gt; T:\n        \"\"\"Visit a bound NotNan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_is_null(self, term: BoundTerm[L]) -&gt; T:\n        \"\"\"Visit a bound IsNull predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_null(self, term: BoundTerm[L]) -&gt; T:\n        \"\"\"Visit a bound NotNull predicate.\"\"\"\n\n    @abstractmethod\n    def visit_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound Equal predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound NotEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_greater_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound GreaterThanOrEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_greater_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound GreaterThan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_less_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound LessThan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_less_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit a bound LessThanOrEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_true(self) -&gt; T:\n        \"\"\"Visit a bound True predicate.\"\"\"\n\n    @abstractmethod\n    def visit_false(self) -&gt; T:\n        \"\"\"Visit a bound False predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not(self, child_result: T) -&gt; T:\n        \"\"\"Visit a bound Not predicate.\"\"\"\n\n    @abstractmethod\n    def visit_and(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit a bound And predicate.\"\"\"\n\n    @abstractmethod\n    def visit_or(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit a bound Or predicate.\"\"\"\n\n    @abstractmethod\n    def visit_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit bound StartsWith predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n        \"\"\"Visit bound NotStartsWith predicate.\"\"\"\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; T:\n        \"\"\"Visit an unbound predicate.\n\n        Args:\n            predicate (UnboundPredicate[L]): An unbound predicate.\n        Raises:\n            TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.\n        \"\"\"\n        raise TypeError(f\"Not a bound predicate: {predicate}\")\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; T:\n        \"\"\"Visit a bound predicate.\n\n        Args:\n            predicate (BoundPredicate[L]): A bound predicate.\n        \"\"\"\n        return visit_bound_predicate(predicate, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_and","title":"<code>visit_and(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound And predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_and(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit a bound And predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>","text":"<p>Visit a bound predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>BoundPredicate[L]</code> <p>A bound predicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; T:\n    \"\"\"Visit a bound predicate.\n\n    Args:\n        predicate (BoundPredicate[L]): A bound predicate.\n    \"\"\"\n    return visit_bound_predicate(predicate, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_equal","title":"<code>visit_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Equal predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound Equal predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_false","title":"<code>visit_false()</code>  <code>abstractmethod</code>","text":"<p>Visit a bound False predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_false(self) -&gt; T:\n    \"\"\"Visit a bound False predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_greater_than","title":"<code>visit_greater_than(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound GreaterThan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_greater_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound GreaterThan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_greater_than_or_equal","title":"<code>visit_greater_than_or_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound GreaterThanOrEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_greater_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound GreaterThanOrEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_in","title":"<code>visit_in(term, literals)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound In predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; T:\n    \"\"\"Visit a bound In predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_is_nan","title":"<code>visit_is_nan(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound IsNan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_is_nan(self, term: BoundTerm[L]) -&gt; T:\n    \"\"\"Visit a bound IsNan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_is_null","title":"<code>visit_is_null(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound IsNull predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_is_null(self, term: BoundTerm[L]) -&gt; T:\n    \"\"\"Visit a bound IsNull predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_less_than","title":"<code>visit_less_than(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound LessThan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_less_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound LessThan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_less_than_or_equal","title":"<code>visit_less_than_or_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound LessThanOrEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_less_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound LessThanOrEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not","title":"<code>visit_not(child_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Not predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not(self, child_result: T) -&gt; T:\n    \"\"\"Visit a bound Not predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_equal","title":"<code>visit_not_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit a bound NotEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_in","title":"<code>visit_not_in(term, literals)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotIn predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; T:\n    \"\"\"Visit a bound NotIn predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_nan","title":"<code>visit_not_nan(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotNan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_nan(self, term: BoundTerm[L]) -&gt; T:\n    \"\"\"Visit a bound NotNan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_null","title":"<code>visit_not_null(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotNull predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_null(self, term: BoundTerm[L]) -&gt; T:\n    \"\"\"Visit a bound NotNull predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_starts_with","title":"<code>visit_not_starts_with(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit bound NotStartsWith predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit bound NotStartsWith predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_or","title":"<code>visit_or(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Or predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_or(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit a bound Or predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_starts_with","title":"<code>visit_starts_with(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit bound StartsWith predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; T:\n    \"\"\"Visit bound StartsWith predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_true","title":"<code>visit_true()</code>  <code>abstractmethod</code>","text":"<p>Visit a bound True predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_true(self) -&gt; T:\n    \"\"\"Visit a bound True predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_unbound_predicate","title":"<code>visit_unbound_predicate(predicate)</code>","text":"<p>Visit an unbound predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>UnboundPredicate[L]</code> <p>An unbound predicate.</p> required <p>Raises:     TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; T:\n    \"\"\"Visit an unbound predicate.\n\n    Args:\n        predicate (UnboundPredicate[L]): An unbound predicate.\n    Raises:\n        TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.\n    \"\"\"\n    raise TypeError(f\"Not a bound predicate: {predicate}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.ResidualVisitor","title":"<code>ResidualVisitor</code>","text":"<p>               Bases: <code>BoundBooleanExpressionVisitor[BooleanExpression]</code>, <code>ABC</code></p> <p>Finds the residuals for an Expression the partitions in the given PartitionSpec.</p> <p>A residual expression is made by partially evaluating an expression using partition values. For example, if a table is partitioned by day(utc_timestamp) and is read with a filter expression utc_timestamp &gt; a and utc_timestamp &lt; b, then there are 4 possible residuals expressions for the partition data, d:</p> <ol> <li>If d &gt; day(a) and d &lt; day(b), the residual is always true</li> <li>If d == day(a) and d != day(b), the residual is utc_timestamp &gt; a</li> <li>if d == day(b) and d != day(a), the residual is utc_timestamp &lt; b</li> <li>If d == day(a) == day(b), the residual is utc_timestamp &gt; a and utc_timestamp &lt; b Partition data is passed using StructLike. Residuals are returned by residualFor(StructLike).</li> </ol> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class ResidualVisitor(BoundBooleanExpressionVisitor[BooleanExpression], ABC):\n    \"\"\"Finds the residuals for an Expression the partitions in the given PartitionSpec.\n\n    A residual expression is made by partially evaluating an expression using partition values.\n    For example, if a table is partitioned by day(utc_timestamp) and is read with a filter expression\n    utc_timestamp &gt; a and utc_timestamp &lt; b, then there are 4 possible residuals expressions\n    for the partition data, d:\n\n\n    1. If d &gt; day(a) and d &amp;lt; day(b), the residual is always true\n    2. If d == day(a) and d != day(b), the residual is utc_timestamp &gt; a\n    3. if d == day(b) and d != day(a), the residual is utc_timestamp &lt; b\n    4. If d == day(a) == day(b), the residual is utc_timestamp &gt; a and utc_timestamp &lt; b\n    Partition data is passed using StructLike. Residuals are returned by residualFor(StructLike).\n    \"\"\"\n\n    schema: Schema\n    spec: PartitionSpec\n    case_sensitive: bool\n    expr: BooleanExpression\n\n    def __init__(self, schema: Schema, spec: PartitionSpec, case_sensitive: bool, expr: BooleanExpression) -&gt; None:\n        self.schema = schema\n        self.spec = spec\n        self.case_sensitive = case_sensitive\n        self.expr = expr\n\n    def eval(self, partition_data: Record) -&gt; BooleanExpression:\n        self.struct = partition_data\n        return visit(self.expr, visitor=self)\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return Not(child_result)\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left_result, right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left_result, right_result)\n\n    def visit_is_null(self, term: BoundTerm[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) is None:\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_not_null(self, term: BoundTerm[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) is not None:\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_is_nan(self, term: BoundTerm[L]) -&gt; BooleanExpression:\n        val = term.eval(self.struct)\n        if isinstance(val, SupportsFloat) and math.isnan(val):\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_nan(self, term: BoundTerm[L]) -&gt; BooleanExpression:\n        val = term.eval(self.struct)\n        if isinstance(val, SupportsFloat) and not math.isnan(val):\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_less_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) &lt; literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_less_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) &lt;= literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_greater_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) &gt; literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_greater_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) &gt;= literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) == literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) != literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) in literals:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) not in literals:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        eval_res = term.eval(self.struct)\n        if eval_res is not None and str(eval_res).startswith(str(literal.value)):\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_not_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; BooleanExpression:\n        if not self.visit_starts_with(term, literal):\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[Any]) -&gt; BooleanExpression:\n        \"\"\"\n        If there is no strict projection or if it evaluates to false, then return the predicate.\n\n        Get the strict projection and inclusive projection of this predicate in partition data,\n        then use them to determine whether to return the original predicate. The strict projection\n        returns true iff the original predicate would have returned true, so the predicate can be\n        eliminated if the strict projection evaluates to true. Similarly the inclusive projection\n        returns false iff the original predicate would have returned false, so the predicate can\n        also be eliminated if the inclusive projection evaluates to false.\n\n        \"\"\"\n        parts = self.spec.fields_by_source_id(predicate.term.ref().field.field_id)\n        if parts == []:\n            return predicate\n\n        def struct_to_schema(struct: StructType) -&gt; Schema:\n            return Schema(*struct.fields)\n\n        for part in parts:\n            strict_projection = part.transform.strict_project(part.name, predicate)\n            strict_result = None\n\n            if strict_projection is not None:\n                bound = strict_projection.bind(\n                    struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n                )\n                if isinstance(bound, BoundPredicate):\n                    strict_result = super().visit_bound_predicate(bound)\n                else:\n                    # if the result is not a predicate, then it must be a constant like alwaysTrue or alwaysFalse\n                    strict_result = bound\n\n            if isinstance(strict_result, AlwaysTrue):\n                return AlwaysTrue()\n\n            inclusive_projection = part.transform.project(part.name, predicate)\n            inclusive_result = None\n            if inclusive_projection is not None:\n                bound_inclusive = inclusive_projection.bind(\n                    struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n                )\n                if isinstance(bound_inclusive, BoundPredicate):\n                    # using predicate method specific to inclusive\n                    inclusive_result = super().visit_bound_predicate(bound_inclusive)\n                else:\n                    # if the result is not a predicate, then it must be a constant like alwaysTrue or\n                    # alwaysFalse\n                    inclusive_result = bound_inclusive\n            if isinstance(inclusive_result, AlwaysFalse):\n                return AlwaysFalse()\n\n        return predicate\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; BooleanExpression:\n        bound = predicate.bind(self.schema, case_sensitive=self.case_sensitive)\n\n        if isinstance(bound, BoundPredicate):\n            bound_residual = self.visit_bound_predicate(predicate=bound)\n            if not isinstance(bound_residual, (AlwaysFalse, AlwaysTrue)):\n                # replace inclusive original unbound predicate\n                return predicate\n\n            # use the non-predicate residual (e.g. alwaysTrue)\n            return bound_residual\n\n        # if binding didn't result in a Predicate, return the expression\n        return bound\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.ResidualVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>","text":"<p>If there is no strict projection or if it evaluates to false, then return the predicate.</p> <p>Get the strict projection and inclusive projection of this predicate in partition data, then use them to determine whether to return the original predicate. The strict projection returns true iff the original predicate would have returned true, so the predicate can be eliminated if the strict projection evaluates to true. Similarly the inclusive projection returns false iff the original predicate would have returned false, so the predicate can also be eliminated if the inclusive projection evaluates to false.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_bound_predicate(self, predicate: BoundPredicate[Any]) -&gt; BooleanExpression:\n    \"\"\"\n    If there is no strict projection or if it evaluates to false, then return the predicate.\n\n    Get the strict projection and inclusive projection of this predicate in partition data,\n    then use them to determine whether to return the original predicate. The strict projection\n    returns true iff the original predicate would have returned true, so the predicate can be\n    eliminated if the strict projection evaluates to true. Similarly the inclusive projection\n    returns false iff the original predicate would have returned false, so the predicate can\n    also be eliminated if the inclusive projection evaluates to false.\n\n    \"\"\"\n    parts = self.spec.fields_by_source_id(predicate.term.ref().field.field_id)\n    if parts == []:\n        return predicate\n\n    def struct_to_schema(struct: StructType) -&gt; Schema:\n        return Schema(*struct.fields)\n\n    for part in parts:\n        strict_projection = part.transform.strict_project(part.name, predicate)\n        strict_result = None\n\n        if strict_projection is not None:\n            bound = strict_projection.bind(\n                struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n            )\n            if isinstance(bound, BoundPredicate):\n                strict_result = super().visit_bound_predicate(bound)\n            else:\n                # if the result is not a predicate, then it must be a constant like alwaysTrue or alwaysFalse\n                strict_result = bound\n\n        if isinstance(strict_result, AlwaysTrue):\n            return AlwaysTrue()\n\n        inclusive_projection = part.transform.project(part.name, predicate)\n        inclusive_result = None\n        if inclusive_projection is not None:\n            bound_inclusive = inclusive_projection.bind(\n                struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n            )\n            if isinstance(bound_inclusive, BoundPredicate):\n                # using predicate method specific to inclusive\n                inclusive_result = super().visit_bound_predicate(bound_inclusive)\n            else:\n                # if the result is not a predicate, then it must be a constant like alwaysTrue or\n                # alwaysFalse\n                inclusive_result = bound_inclusive\n        if isinstance(inclusive_result, AlwaysFalse):\n            return AlwaysFalse()\n\n    return predicate\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._ColumnNameTranslator","title":"<code>_ColumnNameTranslator</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[BooleanExpression]</code></p> <p>Converts the column names with the ones in the actual file.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema</code> <p>The schema of the file.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding a reference to a field in a schema, defaults to True.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case of an UnboundPredicate.</p> <code>ValueError</code> <p>When a column name cannot be found.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class _ColumnNameTranslator(BooleanExpressionVisitor[BooleanExpression]):\n    \"\"\"Converts the column names with the ones in the actual file.\n\n    Args:\n      file_schema (Schema): The schema of the file.\n      case_sensitive (bool): Whether to consider case when binding a reference to a field in a schema, defaults to True.\n\n    Raises:\n        TypeError: In the case of an UnboundPredicate.\n        ValueError: When a column name cannot be found.\n    \"\"\"\n\n    file_schema: Schema\n    case_sensitive: bool\n\n    def __init__(self, file_schema: Schema, case_sensitive: bool) -&gt; None:\n        self.file_schema = file_schema\n        self.case_sensitive = case_sensitive\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return Not(child=child_result)\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left=left_result, right=right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left=left_result, right=right_result)\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; BooleanExpression:\n        raise TypeError(f\"Expected Bound Predicate, got: {predicate.term}\")\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; BooleanExpression:\n        file_column_name = self.file_schema.find_column_name(predicate.term.ref().field.field_id)\n\n        if file_column_name is None:\n            # In the case of schema evolution, the column might not be present\n            # in the file schema when reading older data\n            if isinstance(predicate, BoundIsNull):\n                return AlwaysTrue()\n            else:\n                return AlwaysFalse()\n\n        if isinstance(predicate, BoundUnaryPredicate):\n            return predicate.as_unbound(file_column_name)\n        elif isinstance(predicate, BoundLiteralPredicate):\n            return predicate.as_unbound(file_column_name, predicate.literal)\n        elif isinstance(predicate, BoundSetPredicate):\n            return predicate.as_unbound(file_column_name, predicate.literals)\n        else:\n            raise ValueError(f\"Unsupported predicate: {predicate}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._ExpressionFieldIDs","title":"<code>_ExpressionFieldIDs</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[Set[int]]</code></p> <p>Extracts the field IDs used in the BooleanExpression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class _ExpressionFieldIDs(BooleanExpressionVisitor[Set[int]]):\n    \"\"\"Extracts the field IDs used in the BooleanExpression.\"\"\"\n\n    def visit_true(self) -&gt; Set[int]:\n        return set()\n\n    def visit_false(self) -&gt; Set[int]:\n        return set()\n\n    def visit_not(self, child_result: Set[int]) -&gt; Set[int]:\n        return child_result\n\n    def visit_and(self, left_result: Set[int], right_result: Set[int]) -&gt; Set[int]:\n        return left_result.union(right_result)\n\n    def visit_or(self, left_result: Set[int], right_result: Set[int]) -&gt; Set[int]:\n        return left_result.union(right_result)\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; Set[int]:\n        raise ValueError(\"Only works on bound records\")\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; Set[int]:\n        return {predicate.term.ref().field.field_id}\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._InclusiveMetricsEvaluator","title":"<code>_InclusiveMetricsEvaluator</code>","text":"<p>               Bases: <code>_MetricsEvaluator</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class _InclusiveMetricsEvaluator(_MetricsEvaluator):\n    struct: StructType\n    expr: BooleanExpression\n\n    def __init__(\n        self, schema: Schema, expr: BooleanExpression, case_sensitive: bool = True, include_empty_files: bool = False\n    ) -&gt; None:\n        self.struct = schema.as_struct()\n        self.include_empty_files = include_empty_files\n        self.expr = bind(schema, rewrite_not(expr), case_sensitive)\n\n    def eval(self, file: DataFile) -&gt; bool:\n        \"\"\"Test whether the file may contain records that match the expression.\"\"\"\n        if not self.include_empty_files and file.record_count == 0:\n            return ROWS_CANNOT_MATCH\n\n        if file.record_count &lt; 0:\n            # Older version don't correctly implement record count from avro file and thus\n            # set record count -1 when importing avro tables to iceberg tables. This should\n            # be updated once we implemented and set correct record count.\n            return ROWS_MIGHT_MATCH\n\n        self.value_counts = file.value_counts or EMPTY_DICT\n        self.null_counts = file.null_value_counts or EMPTY_DICT\n        self.nan_counts = file.nan_value_counts or EMPTY_DICT\n        self.lower_bounds = file.lower_bounds or EMPTY_DICT\n        self.upper_bounds = file.upper_bounds or EMPTY_DICT\n\n        return visit(self.expr, self)\n\n    def _may_contain_null(self, field_id: int) -&gt; bool:\n        return self.null_counts is None or (field_id in self.null_counts and self.null_counts.get(field_id) is not None)\n\n    def _contains_nans_only(self, field_id: int) -&gt; bool:\n        if (nan_count := self.nan_counts.get(field_id)) and (value_count := self.value_counts.get(field_id)):\n            return nan_count == value_count\n        return False\n\n    def visit_is_null(self, term: BoundTerm[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self.null_counts.get(field_id) == 0:\n            return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_not_null(self, term: BoundTerm[L]) -&gt; bool:\n        # no need to check whether the field is required because binding evaluates that case\n        # if the column has no non-null values, the expression cannot match\n        field_id = term.ref().field.field_id\n\n        if self._contains_nulls_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_is_nan(self, term: BoundTerm[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self.nan_counts.get(field_id) == 0:\n            return ROWS_CANNOT_MATCH\n\n        # when there's no nanCounts information, but we already know the column only contains null,\n        # it's guaranteed that there's no NaN value\n        if self._contains_nulls_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_not_nan(self, term: BoundTerm[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_less_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if lower_bound_bytes := self.lower_bounds.get(field_id):\n            lower_bound = from_bytes(field.field_type, lower_bound_bytes)\n\n            if self._is_nan(lower_bound):\n                # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                return ROWS_MIGHT_MATCH\n\n            if lower_bound &gt;= literal.value:  # type: ignore[operator]\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_less_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if lower_bound_bytes := self.lower_bounds.get(field_id):\n            lower_bound = from_bytes(field.field_type, lower_bound_bytes)\n            if self._is_nan(lower_bound):\n                # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                return ROWS_MIGHT_MATCH\n\n            if lower_bound &gt; literal.value:  # type: ignore[operator]\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_greater_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if upper_bound_bytes := self.upper_bounds.get(field_id):\n            upper_bound = from_bytes(field.field_type, upper_bound_bytes)\n            if upper_bound &lt;= literal.value:  # type: ignore[operator]\n                if self._is_nan(upper_bound):\n                    # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                    return ROWS_MIGHT_MATCH\n\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_greater_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if upper_bound_bytes := self.upper_bounds.get(field_id):\n            upper_bound = from_bytes(field.field_type, upper_bound_bytes)\n            if upper_bound &lt; literal.value:  # type: ignore[operator]\n                if self._is_nan(upper_bound):\n                    # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                    return ROWS_MIGHT_MATCH\n\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if lower_bound_bytes := self.lower_bounds.get(field_id):\n            lower_bound = from_bytes(field.field_type, lower_bound_bytes)\n            if self._is_nan(lower_bound):\n                # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                return ROWS_MIGHT_MATCH\n\n            if lower_bound &gt; literal.value:  # type: ignore[operator]\n                return ROWS_CANNOT_MATCH\n\n        if upper_bound_bytes := self.upper_bounds.get(field_id):\n            upper_bound = from_bytes(field.field_type, upper_bound_bytes)\n            if self._is_nan(upper_bound):\n                # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                return ROWS_MIGHT_MATCH\n\n            if upper_bound &lt; literal.value:  # type: ignore[operator]\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_not_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        return ROWS_MIGHT_MATCH\n\n    def visit_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; bool:\n        field = term.ref().field\n        field_id = field.field_id\n\n        if self._contains_nulls_only(field_id) or self._contains_nans_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if len(literals) &gt; IN_PREDICATE_LIMIT:\n            # skip evaluating the predicate if the number of values is too big\n            return ROWS_MIGHT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        if lower_bound_bytes := self.lower_bounds.get(field_id):\n            lower_bound = from_bytes(field.field_type, lower_bound_bytes)\n            if self._is_nan(lower_bound):\n                # NaN indicates unreliable bounds. See the InclusiveMetricsEvaluator docs for more.\n                return ROWS_MIGHT_MATCH\n\n            literals = {lit for lit in literals if lower_bound &lt;= lit}  # type: ignore[operator]\n            if len(literals) == 0:\n                return ROWS_CANNOT_MATCH\n\n        if upper_bound_bytes := self.upper_bounds.get(field_id):\n            upper_bound = from_bytes(field.field_type, upper_bound_bytes)\n            # this is different from Java, here NaN is always larger\n            if self._is_nan(upper_bound):\n                return ROWS_MIGHT_MATCH\n\n            literals = {lit for lit in literals if upper_bound &gt;= lit}  # type: ignore[operator]\n            if len(literals) == 0:\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_not_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; bool:\n        # because the bounds are not necessarily a min or max value, this cannot be answered using\n        # them. notIn(col, {X, ...}) with (X, Y) doesn't guarantee that X is a value in col.\n        return ROWS_MIGHT_MATCH\n\n    def visit_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id: int = field.field_id\n\n        if self._contains_nulls_only(field_id):\n            return ROWS_CANNOT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        prefix = str(literal.value)\n        len_prefix = len(prefix)\n\n        if lower_bound_bytes := self.lower_bounds.get(field_id):\n            lower_bound = str(from_bytes(field.field_type, lower_bound_bytes))\n\n            # truncate lower bound so that its length is not greater than the length of prefix\n            if lower_bound and lower_bound[:len_prefix] &gt; prefix:\n                return ROWS_CANNOT_MATCH\n\n        if upper_bound_bytes := self.upper_bounds.get(field_id):\n            upper_bound = str(from_bytes(field.field_type, upper_bound_bytes))\n\n            # truncate upper bound so that its length is not greater than the length of prefix\n            if upper_bound is not None and upper_bound[:len_prefix] &lt; prefix:\n                return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n\n    def visit_not_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        field = term.ref().field\n        field_id: int = field.field_id\n\n        if self._may_contain_null(field_id):\n            return ROWS_MIGHT_MATCH\n\n        if not isinstance(field.field_type, PrimitiveType):\n            raise ValueError(f\"Expected PrimitiveType: {field.field_type}\")\n\n        prefix = str(literal.value)\n        len_prefix = len(prefix)\n\n        # not_starts_with will match unless all values must start with the prefix. This happens when\n        # the lower and upper bounds both start with the prefix.\n        if (lower_bound_bytes := self.lower_bounds.get(field_id)) and (upper_bound_bytes := self.upper_bounds.get(field_id)):\n            lower_bound = str(from_bytes(field.field_type, lower_bound_bytes))\n            upper_bound = str(from_bytes(field.field_type, upper_bound_bytes))\n\n            # if lower is shorter than the prefix then lower doesn't start with the prefix\n            if len(lower_bound) &lt; len_prefix:\n                return ROWS_MIGHT_MATCH\n\n            if lower_bound[:len_prefix] == prefix:\n                # if upper is shorter than the prefix then upper can't start with the prefix\n                if len(upper_bound) &lt; len_prefix:\n                    return ROWS_MIGHT_MATCH\n\n                if upper_bound[:len_prefix] == prefix:\n                    return ROWS_CANNOT_MATCH\n\n        return ROWS_MIGHT_MATCH\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._InclusiveMetricsEvaluator.eval","title":"<code>eval(file)</code>","text":"<p>Test whether the file may contain records that match the expression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def eval(self, file: DataFile) -&gt; bool:\n    \"\"\"Test whether the file may contain records that match the expression.\"\"\"\n    if not self.include_empty_files and file.record_count == 0:\n        return ROWS_CANNOT_MATCH\n\n    if file.record_count &lt; 0:\n        # Older version don't correctly implement record count from avro file and thus\n        # set record count -1 when importing avro tables to iceberg tables. This should\n        # be updated once we implemented and set correct record count.\n        return ROWS_MIGHT_MATCH\n\n    self.value_counts = file.value_counts or EMPTY_DICT\n    self.null_counts = file.null_value_counts or EMPTY_DICT\n    self.nan_counts = file.nan_value_counts or EMPTY_DICT\n    self.lower_bounds = file.lower_bounds or EMPTY_DICT\n    self.upper_bounds = file.upper_bounds or EMPTY_DICT\n\n    return visit(self.expr, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._RewriteNotVisitor","title":"<code>_RewriteNotVisitor</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[BooleanExpression]</code></p> <p>Inverts the negations.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class _RewriteNotVisitor(BooleanExpressionVisitor[BooleanExpression]):\n    \"\"\"Inverts the negations.\"\"\"\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return ~child_result\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left=left_result, right=right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left=left_result, right=right_result)\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate[L]) -&gt; BooleanExpression:\n        return predicate\n\n    def visit_bound_predicate(self, predicate: BoundPredicate[L]) -&gt; BooleanExpression:\n        return predicate\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._StrictMetricsEvaluator","title":"<code>_StrictMetricsEvaluator</code>","text":"<p>               Bases: <code>_MetricsEvaluator</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class _StrictMetricsEvaluator(_MetricsEvaluator):\n    struct: StructType\n    expr: BooleanExpression\n\n    def __init__(\n        self, schema: Schema, expr: BooleanExpression, case_sensitive: bool = True, include_empty_files: bool = False\n    ) -&gt; None:\n        self.struct = schema.as_struct()\n        self.include_empty_files = include_empty_files\n        self.expr = bind(schema, rewrite_not(expr), case_sensitive)\n\n    def eval(self, file: DataFile) -&gt; bool:\n        \"\"\"Test whether all records within the file match the expression.\n\n        Args:\n            file: A data file\n\n        Returns: false if the file may contain any row that doesn't match\n                    the expression, true otherwise.\n        \"\"\"\n        if file.record_count &lt;= 0:\n            # Older version don't correctly implement record count from avro file and thus\n            # set record count -1 when importing avro tables to iceberg tables. This should\n            # be updated once we implemented and set correct record count.\n            return ROWS_MUST_MATCH\n\n        self.value_counts = file.value_counts or EMPTY_DICT\n        self.null_counts = file.null_value_counts or EMPTY_DICT\n        self.nan_counts = file.nan_value_counts or EMPTY_DICT\n        self.lower_bounds = file.lower_bounds or EMPTY_DICT\n        self.upper_bounds = file.upper_bounds or EMPTY_DICT\n\n        return visit(self.expr, self)\n\n    def visit_is_null(self, term: BoundTerm[L]) -&gt; bool:\n        # no need to check whether the field is required because binding evaluates that case\n        # if the column has any non-null values, the expression does not match\n        field_id = term.ref().field.field_id\n\n        if self._contains_nulls_only(field_id):\n            return ROWS_MUST_MATCH\n        else:\n            return ROWS_MIGHT_NOT_MATCH\n\n    def visit_not_null(self, term: BoundTerm[L]) -&gt; bool:\n        # no need to check whether the field is required because binding evaluates that case\n        # if the column has any non-null values, the expression does not match\n        field_id = term.ref().field.field_id\n\n        if (null_count := self.null_counts.get(field_id)) is not None and null_count == 0:\n            return ROWS_MUST_MATCH\n        else:\n            return ROWS_MIGHT_NOT_MATCH\n\n    def visit_is_nan(self, term: BoundTerm[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self._contains_nans_only(field_id):\n            return ROWS_MUST_MATCH\n        else:\n            return ROWS_MIGHT_NOT_MATCH\n\n    def visit_not_nan(self, term: BoundTerm[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if (nan_count := self.nan_counts.get(field_id)) is not None and nan_count == 0:\n            return ROWS_MUST_MATCH\n\n        if self._contains_nulls_only(field_id):\n            return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_less_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when: &lt;----------Min----Max---X-------&gt;\n\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        if upper_bytes := self.upper_bounds.get(field_id):\n            field = self._get_field(field_id)\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n\n            if upper &lt; literal.value:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_less_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when: &lt;----------Min----Max---X-------&gt;\n\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        if upper_bytes := self.upper_bounds.get(field_id):\n            field = self._get_field(field_id)\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n\n            if upper &lt;= literal.value:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_greater_than(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when: &lt;-------X---Min----Max----------&gt;\n\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        if lower_bytes := self.lower_bounds.get(field_id):\n            field = self._get_field(field_id)\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n\n            if self._is_nan(lower):\n                # NaN indicates unreliable bounds.\n                # See the _StrictMetricsEvaluator docs for more.\n                return ROWS_MIGHT_NOT_MATCH\n\n            if lower &gt; literal.value:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_greater_than_or_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when: &lt;-------X---Min----Max----------&gt;\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        if lower_bytes := self.lower_bounds.get(field_id):\n            field = self._get_field(field_id)\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n\n            if self._is_nan(lower):\n                # NaN indicates unreliable bounds.\n                # See the _StrictMetricsEvaluator docs for more.\n                return ROWS_MIGHT_NOT_MATCH\n\n            if lower &gt;= literal.value:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when Min == X == Max\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        if (lower_bytes := self.lower_bounds.get(field_id)) and (upper_bytes := self.upper_bounds.get(field_id)):\n            field = self._get_field(field_id)\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n\n            if lower != literal.value or upper != literal.value:\n                return ROWS_MIGHT_NOT_MATCH\n            else:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_not_equal(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        # Rows must match when X &lt; Min or Max &lt; X because it is not in the range\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MUST_MATCH\n\n        field = self._get_field(field_id)\n\n        if lower_bytes := self.lower_bounds.get(field_id):\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n\n            if self._is_nan(lower):\n                # NaN indicates unreliable bounds.\n                # See the _StrictMetricsEvaluator docs for more.\n                return ROWS_MIGHT_NOT_MATCH\n\n            if lower &gt; literal.value:\n                return ROWS_MUST_MATCH\n\n        if upper_bytes := self.upper_bounds.get(field_id):\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n\n            if upper &lt; literal.value:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MIGHT_NOT_MATCH\n\n        field = self._get_field(field_id)\n\n        if (lower_bytes := self.lower_bounds.get(field_id)) and (upper_bytes := self.upper_bounds.get(field_id)):\n            # similar to the implementation in eq, first check if the lower bound is in the set\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n            if lower not in literals:\n                return ROWS_MIGHT_NOT_MATCH\n\n            # check if the upper bound is in the set\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n            if upper not in literals:\n                return ROWS_MIGHT_NOT_MATCH\n\n            # finally check if the lower bound and the upper bound are equal\n            if lower != upper:\n                return ROWS_MIGHT_NOT_MATCH\n\n            # All values must be in the set if the lower bound and the upper bound are\n            # in the set and are equal.\n            return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_not_in(self, term: BoundTerm[L], literals: Set[L]) -&gt; bool:\n        field_id = term.ref().field.field_id\n\n        if self._can_contain_nulls(field_id) or self._can_contain_nans(field_id):\n            return ROWS_MUST_MATCH\n\n        field = self._get_field(field_id)\n\n        if lower_bytes := self.lower_bounds.get(field_id):\n            lower = _from_byte_buffer(field.field_type, lower_bytes)\n\n            if self._is_nan(lower):\n                # NaN indicates unreliable bounds.\n                # See the StrictMetricsEvaluator docs for more.\n                return ROWS_MIGHT_NOT_MATCH\n\n            literals = {val for val in literals if lower &lt;= val}\n            if len(literals) == 0:\n                return ROWS_MUST_MATCH\n\n        if upper_bytes := self.upper_bounds.get(field_id):\n            upper = _from_byte_buffer(field.field_type, upper_bytes)\n\n            literals = {val for val in literals if upper &gt;= val}\n\n            if len(literals) == 0:\n                return ROWS_MUST_MATCH\n\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        return ROWS_MIGHT_NOT_MATCH\n\n    def visit_not_starts_with(self, term: BoundTerm[L], literal: Literal[L]) -&gt; bool:\n        return ROWS_MIGHT_NOT_MATCH\n\n    def _get_field(self, field_id: int) -&gt; NestedField:\n        field = self.struct.field(field_id=field_id)\n        if field is None:\n            raise ValueError(f\"Cannot find field, might be nested or missing: {field_id}\")\n\n        return field\n\n    def _can_contain_nulls(self, field_id: int) -&gt; bool:\n        return (null_count := self.null_counts.get(field_id)) is not None and null_count &gt; 0\n\n    def _can_contain_nans(self, field_id: int) -&gt; bool:\n        return (nan_count := self.nan_counts.get(field_id)) is not None and nan_count &gt; 0\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors._StrictMetricsEvaluator.eval","title":"<code>eval(file)</code>","text":"<p>Test whether all records within the file match the expression.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>DataFile</code> <p>A data file</p> required <p>false if the file may contain any row that doesn't match</p> Type Description <code>bool</code> <p>the expression, true otherwise.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def eval(self, file: DataFile) -&gt; bool:\n    \"\"\"Test whether all records within the file match the expression.\n\n    Args:\n        file: A data file\n\n    Returns: false if the file may contain any row that doesn't match\n                the expression, true otherwise.\n    \"\"\"\n    if file.record_count &lt;= 0:\n        # Older version don't correctly implement record count from avro file and thus\n        # set record count -1 when importing avro tables to iceberg tables. This should\n        # be updated once we implemented and set correct record count.\n        return ROWS_MUST_MATCH\n\n    self.value_counts = file.value_counts or EMPTY_DICT\n    self.null_counts = file.null_value_counts or EMPTY_DICT\n    self.nan_counts = file.nan_value_counts or EMPTY_DICT\n    self.lower_bounds = file.lower_bounds or EMPTY_DICT\n    self.upper_bounds = file.upper_bounds or EMPTY_DICT\n\n    return visit(self.expr, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.bind","title":"<code>bind(schema, expression, case_sensitive)</code>","text":"<p>Travers over an expression to bind the predicates to the schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>A schema to use when binding the expression.</p> required <code>expression</code> <code>BooleanExpression</code> <p>An expression containing UnboundPredicates that can be bound.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding a reference to a field in a schema, defaults to True.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case a predicate is already bound.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def bind(schema: Schema, expression: BooleanExpression, case_sensitive: bool) -&gt; BooleanExpression:\n    \"\"\"Travers over an expression to bind the predicates to the schema.\n\n    Args:\n      schema (Schema): A schema to use when binding the expression.\n      expression (BooleanExpression): An expression containing UnboundPredicates that can be bound.\n      case_sensitive (bool): Whether to consider case when binding a reference to a field in a schema, defaults to True.\n\n    Raises:\n        TypeError: In the case a predicate is already bound.\n    \"\"\"\n    return visit(expression, BindVisitor(schema, case_sensitive))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.expression_to_plain_format","title":"<code>expression_to_plain_format(expressions, cast_int_to_datetime=False)</code>","text":"<p>Format a Disjunctive Normal Form expression.</p> <p>These are the formats that the expression can be fed into:</p> <ul> <li>https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html</li> <li>https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html</li> </ul> <p>Contrary to normal DNF that may contain Not expressions, but here they should have been rewritten. This can be done using <code>rewrite_not(...)</code>.</p> <p>Keep in mind that this is only used for page skipping, and still needs to filter on a row level.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>Tuple[BooleanExpression, ...]</code> <p>Expression in Disjunctive Normal Form.</p> required <p>Returns:</p> Type Description <code>List[List[Tuple[str, str, Any]]]</code> <p>Formatter filter compatible with Dask and PyArrow.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def expression_to_plain_format(\n    expressions: Tuple[BooleanExpression, ...], cast_int_to_datetime: bool = False\n) -&gt; List[List[Tuple[str, str, Any]]]:\n    \"\"\"Format a Disjunctive Normal Form expression.\n\n    These are the formats that the expression can be fed into:\n\n    - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html\n    - https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html\n\n    Contrary to normal DNF that may contain Not expressions, but here they should have\n    been rewritten. This can be done using ``rewrite_not(...)``.\n\n    Keep in mind that this is only used for page skipping, and still needs to filter\n    on a row level.\n\n    Args:\n        expressions: Expression in Disjunctive Normal Form.\n\n    Returns:\n        Formatter filter compatible with Dask and PyArrow.\n    \"\"\"\n    # In the form of expr1 \u2228 expr2 \u2228 ... \u2228 exprN\n    visitor = ExpressionToPlainFormat(cast_int_to_datetime)\n    return [visit(expression, visitor) for expression in expressions]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.visit","title":"<code>visit(obj, visitor)</code>","text":"<p>Apply a boolean expression visitor to any point within an expression.</p> <p>The function traverses the expression in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>BooleanExpression</code> <p>An instance of a BooleanExpression.</p> required <code>visitor</code> <code>BooleanExpressionVisitor[T]</code> <p>An instance of an implementation of the generic BooleanExpressionVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unsupported expression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@singledispatch\ndef visit(obj: BooleanExpression, visitor: BooleanExpressionVisitor[T]) -&gt; T:\n    \"\"\"Apply a boolean expression visitor to any point within an expression.\n\n    The function traverses the expression in post-order fashion.\n\n    Args:\n        obj (BooleanExpression): An instance of a BooleanExpression.\n        visitor (BooleanExpressionVisitor[T]): An instance of an implementation of the generic BooleanExpressionVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unsupported expression.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit unsupported expression: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/","title":"io","text":"<p>Base FileIO classes for implementing reading and writing table files.</p> <p>The FileIO abstraction includes a subset of full filesystem implementations. Specifically, Iceberg needs to read or write a file at a given location (as a seekable stream), as well as check if a file exists. An implementation of the FileIO abstract base class is responsible for returning an InputFile instance, an OutputFile instance, and deleting a file given its location.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO","title":"<code>FileIO</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for FileIO implementations.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class FileIO(ABC):\n    \"\"\"A base class for FileIO implementations.\"\"\"\n\n    properties: Properties\n\n    def __init__(self, properties: Properties = EMPTY_DICT):\n        self.properties = properties\n\n    @abstractmethod\n    def new_input(self, location: str) -&gt; InputFile:\n        \"\"\"Get an InputFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n        \"\"\"\n\n    @abstractmethod\n    def new_output(self, location: str) -&gt; OutputFile:\n        \"\"\"Get an OutputFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n        \"\"\"\n\n    @abstractmethod\n    def delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n        \"\"\"Delete the file at the given path.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): A URI or a path to a local file--if an InputFile instance or\n                an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.\n\n        Raises:\n            PermissionError: If the file at location cannot be accessed due to a permission error.\n            FileNotFoundError: When the file at the provided location does not exist.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.delete","title":"<code>delete(location)</code>  <code>abstractmethod</code>","text":"<p>Delete the file at the given path.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>A URI or a path to a local file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at location cannot be accessed due to a permission error.</p> <code>FileNotFoundError</code> <p>When the file at the provided location does not exist.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n    \"\"\"Delete the file at the given path.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): A URI or a path to a local file--if an InputFile instance or\n            an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.\n\n    Raises:\n        PermissionError: If the file at location cannot be accessed due to a permission error.\n        FileNotFoundError: When the file at the provided location does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.new_input","title":"<code>new_input(location)</code>  <code>abstractmethod</code>","text":"<p>Get an InputFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef new_input(self, location: str) -&gt; InputFile:\n    \"\"\"Get an InputFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.new_output","title":"<code>new_output(location)</code>  <code>abstractmethod</code>","text":"<p>Get an OutputFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef new_output(self, location: str) -&gt; OutputFile:\n    \"\"\"Get an OutputFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile","title":"<code>InputFile</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for InputFile implementations.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location</code> <code>str</code> <p>The URI or path to a local file for an InputFile instance.</p> <code>exists</code> <code>bool</code> <p>Whether the file exists or not.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class InputFile(ABC):\n    \"\"\"A base class for InputFile implementations.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location (str): The URI or path to a local file for an InputFile instance.\n        exists (bool): Whether the file exists or not.\n    \"\"\"\n\n    def __init__(self, location: str):\n        self._location = location\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n\n    @property\n    def location(self) -&gt; str:\n        \"\"\"The fully-qualified location of the input file.\"\"\"\n        return self._location\n\n    @abstractmethod\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        \"\"\"\n\n    @abstractmethod\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Return an object that matches the InputStream protocol.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            InputStream: An object that matches the InputStream protocol.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n            FileNotFoundError: If the file at self.location does not exist.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.location","title":"<code>location</code>  <code>property</code>","text":"<p>The fully-qualified location of the input file.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.exists","title":"<code>exists()</code>  <code>abstractmethod</code>","text":"<p>Check whether the location exists.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.open","title":"<code>open(seekable=True)</code>  <code>abstractmethod</code>","text":"<p>Return an object that matches the InputStream protocol.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InputStream</code> <code>InputStream</code> <p>An object that matches the InputStream protocol.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> <code>FileNotFoundError</code> <p>If the file at self.location does not exist.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Return an object that matches the InputStream protocol.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        InputStream: An object that matches the InputStream protocol.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        FileNotFoundError: If the file at self.location does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream","title":"<code>InputStream</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for the file-like object returned by InputFile.open(...).</p> <p>This outlines the minimally required methods for a seekable input stream returned from an InputFile implementation's <code>open(...)</code> method. These methods are a subset of IOBase/RawIOBase.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@runtime_checkable\nclass InputStream(Protocol):\n    \"\"\"A protocol for the file-like object returned by InputFile.open(...).\n\n    This outlines the minimally required methods for a seekable input stream returned from an InputFile\n    implementation's `open(...)` method. These methods are a subset of IOBase/RawIOBase.\n    \"\"\"\n\n    @abstractmethod\n    def read(self, size: int = 0) -&gt; bytes: ...\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = SEEK_SET) -&gt; int: ...\n\n    @abstractmethod\n    def tell(self) -&gt; int: ...\n\n    @abstractmethod\n    def close(self) -&gt; None: ...\n\n    def __enter__(self) -&gt; InputStream:\n        \"\"\"Provide setup when opening an InputStream using a 'with' statement.\"\"\"\n\n    @abstractmethod\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream.__enter__","title":"<code>__enter__()</code>","text":"<p>Provide setup when opening an InputStream using a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>def __enter__(self) -&gt; InputStream:\n    \"\"\"Provide setup when opening an InputStream using a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>  <code>abstractmethod</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __exit__(\n    self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile","title":"<code>OutputFile</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for OutputFile implementations.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location</code> <code>str</code> <p>The URI or path to a local file for an OutputFile instance.</p> <code>exists</code> <code>bool</code> <p>Whether the file exists or not.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class OutputFile(ABC):\n    \"\"\"A base class for OutputFile implementations.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location (str): The URI or path to a local file for an OutputFile instance.\n        exists (bool): Whether the file exists or not.\n    \"\"\"\n\n    def __init__(self, location: str):\n        self._location = location\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n\n    @property\n    def location(self) -&gt; str:\n        \"\"\"The fully-qualified location of the output file.\"\"\"\n        return self._location\n\n    @abstractmethod\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        \"\"\"\n\n    @abstractmethod\n    def to_input_file(self) -&gt; InputFile:\n        \"\"\"Return an InputFile for the location of this output file.\"\"\"\n\n    @abstractmethod\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Return an object that matches the OutputStream protocol.\n\n        Args:\n            overwrite (bool): If the file already exists at `self.location`\n                and `overwrite` is False a FileExistsError should be raised.\n\n        Returns:\n            OutputStream: An object that matches the OutputStream protocol.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n            FileExistsError: If the file at self.location already exists and `overwrite=False`.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.location","title":"<code>location</code>  <code>property</code>","text":"<p>The fully-qualified location of the output file.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.create","title":"<code>create(overwrite=False)</code>  <code>abstractmethod</code>","text":"<p>Return an object that matches the OutputStream protocol.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>If the file already exists at <code>self.location</code> and <code>overwrite</code> is False a FileExistsError should be raised.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>OutputStream</code> <code>OutputStream</code> <p>An object that matches the OutputStream protocol.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> <code>FileExistsError</code> <p>If the file at self.location already exists and <code>overwrite=False</code>.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Return an object that matches the OutputStream protocol.\n\n    Args:\n        overwrite (bool): If the file already exists at `self.location`\n            and `overwrite` is False a FileExistsError should be raised.\n\n    Returns:\n        OutputStream: An object that matches the OutputStream protocol.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        FileExistsError: If the file at self.location already exists and `overwrite=False`.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.exists","title":"<code>exists()</code>  <code>abstractmethod</code>","text":"<p>Check whether the location exists.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.to_input_file","title":"<code>to_input_file()</code>  <code>abstractmethod</code>","text":"<p>Return an InputFile for the location of this output file.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef to_input_file(self) -&gt; InputFile:\n    \"\"\"Return an InputFile for the location of this output file.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream","title":"<code>OutputStream</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for the file-like object returned by OutputFile.create(...).</p> <p>This outlines the minimally required methods for a writable output stream returned from an OutputFile implementation's <code>create(...)</code> method. These methods are a subset of IOBase/RawIOBase.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@runtime_checkable\nclass OutputStream(Protocol):  # pragma: no cover\n    \"\"\"A protocol for the file-like object returned by OutputFile.create(...).\n\n    This outlines the minimally required methods for a writable output stream returned from an OutputFile\n    implementation's `create(...)` method. These methods are a subset of IOBase/RawIOBase.\n    \"\"\"\n\n    @abstractmethod\n    def write(self, b: bytes) -&gt; int: ...\n\n    @abstractmethod\n    def close(self) -&gt; None: ...\n\n    @abstractmethod\n    def __enter__(self) -&gt; OutputStream:\n        \"\"\"Provide setup when opening an OutputStream using a 'with' statement.\"\"\"\n\n    @abstractmethod\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream.__enter__","title":"<code>__enter__()</code>  <code>abstractmethod</code>","text":"<p>Provide setup when opening an OutputStream using a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __enter__(self) -&gt; OutputStream:\n    \"\"\"Provide setup when opening an OutputStream using a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>  <code>abstractmethod</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __exit__(\n    self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io._parse_location","title":"<code>_parse_location(location)</code>","text":"<p>Return the path without the scheme.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>def _parse_location(location: str) -&gt; Tuple[str, str, str]:\n    \"\"\"Return the path without the scheme.\"\"\"\n    uri = urlparse(location)\n    if not uri.scheme:\n        return \"file\", uri.netloc, os.path.abspath(location)\n    elif uri.scheme in (\"hdfs\", \"viewfs\"):\n        return uri.scheme, uri.netloc, uri.path\n    else:\n        return uri.scheme, uri.netloc, f\"{uri.netloc}{uri.path}\"\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/","title":"fsspec","text":"<p>FileIO implementation for reading and writing table files that uses fsspec compatible filesystems.</p>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO","title":"<code>FsspecFileIO</code>","text":"<p>               Bases: <code>FileIO</code></p> <p>A FileIO implementation that uses fsspec.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecFileIO(FileIO):\n    \"\"\"A FileIO implementation that uses fsspec.\"\"\"\n\n    def __init__(self, properties: Properties):\n        self._scheme_to_fs = {}\n        self._scheme_to_fs.update(SCHEME_TO_FS)\n        self.get_fs: Callable[[str], AbstractFileSystem] = lru_cache(self._get_fs)\n        super().__init__(properties=properties)\n\n    def new_input(self, location: str) -&gt; FsspecInputFile:\n        \"\"\"Get an FsspecInputFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            FsspecInputFile: An FsspecInputFile instance for the given location.\n        \"\"\"\n        uri = urlparse(location)\n        fs = self.get_fs(uri.scheme)\n        return FsspecInputFile(location=location, fs=fs)\n\n    def new_output(self, location: str) -&gt; FsspecOutputFile:\n        \"\"\"Get an FsspecOutputFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            FsspecOutputFile: An FsspecOutputFile instance for the given location.\n        \"\"\"\n        uri = urlparse(location)\n        fs = self.get_fs(uri.scheme)\n        return FsspecOutputFile(location=location, fs=fs)\n\n    def delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n        \"\"\"Delete the file at the given location.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an\n                OutputFile instance is provided, the location attribute for that instance is used as the location\n                to delete.\n        \"\"\"\n        if isinstance(location, (InputFile, OutputFile)):\n            str_location = location.location  # Use InputFile or OutputFile location\n        else:\n            str_location = location\n\n        uri = urlparse(str_location)\n        fs = self.get_fs(uri.scheme)\n        fs.rm(str_location)\n\n    def _get_fs(self, scheme: str) -&gt; AbstractFileSystem:\n        \"\"\"Get a filesystem for a specific scheme.\"\"\"\n        if scheme not in self._scheme_to_fs:\n            raise ValueError(f\"No registered filesystem for scheme: {scheme}\")\n        return self._scheme_to_fs[scheme](self.properties)\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        \"\"\"Create a dictionary of the FsSpecFileIO fields used when pickling.\"\"\"\n        fileio_copy = copy(self.__dict__)\n        fileio_copy[\"get_fs\"] = None\n        return fileio_copy\n\n    def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n        \"\"\"Deserialize the state into a FsSpecFileIO instance.\"\"\"\n        self.__dict__ = state\n        self.get_fs = lru_cache(self._get_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a dictionary of the FsSpecFileIO fields used when pickling.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __getstate__(self) -&gt; Dict[str, Any]:\n    \"\"\"Create a dictionary of the FsSpecFileIO fields used when pickling.\"\"\"\n    fileio_copy = copy(self.__dict__)\n    fileio_copy[\"get_fs\"] = None\n    return fileio_copy\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Deserialize the state into a FsSpecFileIO instance.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n    \"\"\"Deserialize the state into a FsSpecFileIO instance.\"\"\"\n    self.__dict__ = state\n    self.get_fs = lru_cache(self._get_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO._get_fs","title":"<code>_get_fs(scheme)</code>","text":"<p>Get a filesystem for a specific scheme.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def _get_fs(self, scheme: str) -&gt; AbstractFileSystem:\n    \"\"\"Get a filesystem for a specific scheme.\"\"\"\n    if scheme not in self._scheme_to_fs:\n        raise ValueError(f\"No registered filesystem for scheme: {scheme}\")\n    return self._scheme_to_fs[scheme](self.properties)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.delete","title":"<code>delete(location)</code>","text":"<p>Delete the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>The URI to the file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the location to delete.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n    \"\"\"Delete the file at the given location.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an\n            OutputFile instance is provided, the location attribute for that instance is used as the location\n            to delete.\n    \"\"\"\n    if isinstance(location, (InputFile, OutputFile)):\n        str_location = location.location  # Use InputFile or OutputFile location\n    else:\n        str_location = location\n\n    uri = urlparse(str_location)\n    fs = self.get_fs(uri.scheme)\n    fs.rm(str_location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.new_input","title":"<code>new_input(location)</code>","text":"<p>Get an FsspecInputFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>FsspecInputFile</code> <code>FsspecInputFile</code> <p>An FsspecInputFile instance for the given location.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def new_input(self, location: str) -&gt; FsspecInputFile:\n    \"\"\"Get an FsspecInputFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        FsspecInputFile: An FsspecInputFile instance for the given location.\n    \"\"\"\n    uri = urlparse(location)\n    fs = self.get_fs(uri.scheme)\n    return FsspecInputFile(location=location, fs=fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.new_output","title":"<code>new_output(location)</code>","text":"<p>Get an FsspecOutputFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>FsspecOutputFile</code> <code>FsspecOutputFile</code> <p>An FsspecOutputFile instance for the given location.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def new_output(self, location: str) -&gt; FsspecOutputFile:\n    \"\"\"Get an FsspecOutputFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        FsspecOutputFile: An FsspecOutputFile instance for the given location.\n    \"\"\"\n    uri = urlparse(location)\n    fs = self.get_fs(uri.scheme)\n    return FsspecOutputFile(location=location, fs=fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile","title":"<code>FsspecInputFile</code>","text":"<p>               Bases: <code>InputFile</code></p> <p>An input file implementation for the FsspecFileIO.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI to a file location.</p> required <code>fs</code> <code>AbstractFileSystem</code> <p>An fsspec filesystem instance.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecInputFile(InputFile):\n    \"\"\"An input file implementation for the FsspecFileIO.\n\n    Args:\n        location (str): A URI to a file location.\n        fs (AbstractFileSystem): An fsspec filesystem instance.\n    \"\"\"\n\n    def __init__(self, location: str, fs: AbstractFileSystem):\n        self._fs = fs\n        super().__init__(location=location)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        object_info = self._fs.info(self.location)\n        if size := object_info.get(\"Size\"):\n            return size\n        elif size := object_info.get(\"size\"):\n            return size\n        raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        return self._fs.lexists(self.location)\n\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Create an input stream for reading the contents of the file.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            OpenFile: An fsspec compliant file-like object.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n        \"\"\"\n        try:\n            return self._fs.open(self.location, \"rb\")\n        except FileNotFoundError as e:\n            # To have a consistent error handling experience, make sure exception contains missing file location.\n            raise e if e.filename else FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.location) from e\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    object_info = self._fs.info(self.location)\n    if size := object_info.get(\"Size\"):\n        return size\n    elif size := object_info.get(\"size\"):\n        return size\n    raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    return self._fs.lexists(self.location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.open","title":"<code>open(seekable=True)</code>","text":"<p>Create an input stream for reading the contents of the file.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>OpenFile</code> <code>InputStream</code> <p>An fsspec compliant file-like object.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Create an input stream for reading the contents of the file.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        OpenFile: An fsspec compliant file-like object.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    try:\n        return self._fs.open(self.location, \"rb\")\n    except FileNotFoundError as e:\n        # To have a consistent error handling experience, make sure exception contains missing file location.\n        raise e if e.filename else FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.location) from e\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile","title":"<code>FsspecOutputFile</code>","text":"<p>               Bases: <code>OutputFile</code></p> <p>An output file implementation for the FsspecFileIO.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI to a file location.</p> required <code>fs</code> <code>AbstractFileSystem</code> <p>An fsspec filesystem instance.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecOutputFile(OutputFile):\n    \"\"\"An output file implementation for the FsspecFileIO.\n\n    Args:\n        location (str): A URI to a file location.\n        fs (AbstractFileSystem): An fsspec filesystem instance.\n    \"\"\"\n\n    def __init__(self, location: str, fs: AbstractFileSystem):\n        self._fs = fs\n        super().__init__(location=location)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        object_info = self._fs.info(self.location)\n        if size := object_info.get(\"Size\"):\n            return size\n        elif size := object_info.get(\"size\"):\n            return size\n        raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        return self._fs.lexists(self.location)\n\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Create an output stream for reading the contents of the file.\n\n        Args:\n            overwrite (bool): Whether to overwrite the file if it already exists.\n\n        Returns:\n            OpenFile: An fsspec compliant file-like object.\n\n        Raises:\n            FileExistsError: If the file already exists at the location and overwrite is set to False.\n\n        Note:\n            If overwrite is set to False, a check is first performed to verify that the file does not exist.\n            This is not thread-safe and a possibility does exist that the file can be created by a concurrent\n            process after the existence check yet before the output stream is created. In such a case, the default\n            behavior will truncate the contents of the existing file when opening the output stream.\n        \"\"\"\n        if not overwrite and self.exists():\n            raise FileExistsError(f\"Cannot create file, file already exists: {self.location}\")\n        return self._fs.open(self.location, \"wb\")\n\n    def to_input_file(self) -&gt; FsspecInputFile:\n        \"\"\"Return a new FsspecInputFile for the location at `self.location`.\"\"\"\n        return FsspecInputFile(location=self.location, fs=self._fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    object_info = self._fs.info(self.location)\n    if size := object_info.get(\"Size\"):\n        return size\n    elif size := object_info.get(\"size\"):\n        return size\n    raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.create","title":"<code>create(overwrite=False)</code>","text":"<p>Create an output stream for reading the contents of the file.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>OpenFile</code> <code>OutputStream</code> <p>An fsspec compliant file-like object.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file already exists at the location and overwrite is set to False.</p> Note <p>If overwrite is set to False, a check is first performed to verify that the file does not exist. This is not thread-safe and a possibility does exist that the file can be created by a concurrent process after the existence check yet before the output stream is created. In such a case, the default behavior will truncate the contents of the existing file when opening the output stream.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Create an output stream for reading the contents of the file.\n\n    Args:\n        overwrite (bool): Whether to overwrite the file if it already exists.\n\n    Returns:\n        OpenFile: An fsspec compliant file-like object.\n\n    Raises:\n        FileExistsError: If the file already exists at the location and overwrite is set to False.\n\n    Note:\n        If overwrite is set to False, a check is first performed to verify that the file does not exist.\n        This is not thread-safe and a possibility does exist that the file can be created by a concurrent\n        process after the existence check yet before the output stream is created. In such a case, the default\n        behavior will truncate the contents of the existing file when opening the output stream.\n    \"\"\"\n    if not overwrite and self.exists():\n        raise FileExistsError(f\"Cannot create file, file already exists: {self.location}\")\n    return self._fs.open(self.location, \"wb\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    return self._fs.lexists(self.location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.to_input_file","title":"<code>to_input_file()</code>","text":"<p>Return a new FsspecInputFile for the location at <code>self.location</code>.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def to_input_file(self) -&gt; FsspecInputFile:\n    \"\"\"Return a new FsspecInputFile for the location at `self.location`.\"\"\"\n    return FsspecInputFile(location=self.location, fs=self._fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/","title":"pyarrow","text":"<p>FileIO implementation for reading and writing table files that uses pyarrow.fs.</p> <p>This file contains a FileIO implementation that relies on the filesystem interface provided by PyArrow. It relies on PyArrow's <code>from_uri</code> method that infers the correct filesystem type to use. Theoretically, this allows the supported storage types to grow naturally with the pyarrow library.</p>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan","title":"<code>ArrowScan</code>","text":"Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class ArrowScan:\n    _table_metadata: TableMetadata\n    _io: FileIO\n    _projected_schema: Schema\n    _bound_row_filter: BooleanExpression\n    _case_sensitive: bool\n    _limit: Optional[int]\n    \"\"\"Scan the Iceberg Table and create an Arrow construct.\n\n    Attributes:\n        _table_metadata: Current table metadata of the Iceberg table\n        _io: PyIceberg FileIO implementation from which to fetch the io properties\n        _projected_schema: Iceberg Schema to project onto the data files\n        _bound_row_filter: Schema bound row expression to filter the data with\n        _case_sensitive: Case sensitivity when looking up column names\n        _limit: Limit the number of records.\n    \"\"\"\n\n    def __init__(\n        self,\n        table_metadata: TableMetadata,\n        io: FileIO,\n        projected_schema: Schema,\n        row_filter: BooleanExpression,\n        case_sensitive: bool = True,\n        limit: Optional[int] = None,\n    ) -&gt; None:\n        self._table_metadata = table_metadata\n        self._io = io\n        self._projected_schema = projected_schema\n        self._bound_row_filter = bind(table_metadata.schema(), row_filter, case_sensitive=case_sensitive)\n        self._case_sensitive = case_sensitive\n        self._limit = limit\n\n    @property\n    def _use_large_types(self) -&gt; bool:\n        \"\"\"Whether to represent data as large arrow types.\n\n        Defaults to True.\n        \"\"\"\n        return property_as_bool(self._io.properties, PYARROW_USE_LARGE_TYPES_ON_READ, True)\n\n    @property\n    def _projected_field_ids(self) -&gt; Set[int]:\n        \"\"\"Set of field IDs that should be projected from the data files.\"\"\"\n        return {\n            id\n            for id in self._projected_schema.field_ids\n            if not isinstance(self._projected_schema.find_type(id), (MapType, ListType))\n        }.union(extract_field_ids(self._bound_row_filter))\n\n    def to_table(self, tasks: Iterable[FileScanTask]) -&gt; pa.Table:\n        \"\"\"Scan the Iceberg table and return a pa.Table.\n\n        Returns a pa.Table with data from the Iceberg table by resolving the\n        right columns that match the current table schema. Only data that\n        matches the provided row_filter expression is returned.\n\n        Args:\n            tasks: FileScanTasks representing the data files and delete files to read from.\n\n        Returns:\n            A PyArrow table. Total number of rows will be capped if specified.\n\n        Raises:\n            ResolveError: When a required field cannot be found in the file\n            ValueError: When a field type in the file cannot be projected to the schema type\n        \"\"\"\n        deletes_per_file = _read_all_delete_files(self._io, tasks)\n        executor = ExecutorFactory.get_or_create()\n\n        def _table_from_scan_task(task: FileScanTask) -&gt; pa.Table:\n            batches = list(self._record_batches_from_scan_tasks_and_deletes([task], deletes_per_file))\n            if len(batches) &gt; 0:\n                return pa.Table.from_batches(batches)\n            else:\n                return None\n\n        futures = [\n            executor.submit(\n                _table_from_scan_task,\n                task,\n            )\n            for task in tasks\n        ]\n        total_row_count = 0\n        # for consistent ordering, we need to maintain future order\n        futures_index = {f: i for i, f in enumerate(futures)}\n        completed_futures: SortedList[Future[pa.Table]] = SortedList(iterable=[], key=lambda f: futures_index[f])\n        for future in concurrent.futures.as_completed(futures):\n            completed_futures.add(future)\n            if table_result := future.result():\n                total_row_count += len(table_result)\n            # stop early if limit is satisfied\n            if self._limit is not None and total_row_count &gt;= self._limit:\n                break\n\n        # by now, we've either completed all tasks or satisfied the limit\n        if self._limit is not None:\n            _ = [f.cancel() for f in futures if not f.done()]\n\n        tables = [f.result() for f in completed_futures if f.result()]\n\n        if len(tables) &lt; 1:\n            return pa.Table.from_batches([], schema=schema_to_pyarrow(self._projected_schema, include_field_ids=False))\n\n        result = pa.concat_tables(tables, promote_options=\"permissive\")\n\n        if self._limit is not None:\n            return result.slice(0, self._limit)\n\n        return result\n\n    def to_record_batches(self, tasks: Iterable[FileScanTask]) -&gt; Iterator[pa.RecordBatch]:\n        \"\"\"Scan the Iceberg table and return an Iterator[pa.RecordBatch].\n\n        Returns an Iterator of pa.RecordBatch with data from the Iceberg table\n        by resolving the right columns that match the current table schema.\n        Only data that matches the provided row_filter expression is returned.\n\n        Args:\n            tasks: FileScanTasks representing the data files and delete files to read from.\n\n        Returns:\n            An Iterator of PyArrow RecordBatches.\n            Total number of rows will be capped if specified.\n\n        Raises:\n            ResolveError: When a required field cannot be found in the file\n            ValueError: When a field type in the file cannot be projected to the schema type\n        \"\"\"\n        deletes_per_file = _read_all_delete_files(self._io, tasks)\n        return self._record_batches_from_scan_tasks_and_deletes(tasks, deletes_per_file)\n\n    def _record_batches_from_scan_tasks_and_deletes(\n        self, tasks: Iterable[FileScanTask], deletes_per_file: Dict[str, List[ChunkedArray]]\n    ) -&gt; Iterator[pa.RecordBatch]:\n        total_row_count = 0\n        for task in tasks:\n            if self._limit is not None and total_row_count &gt;= self._limit:\n                break\n            batches = _task_to_record_batches(\n                _fs_from_file_path(self._io, task.file.file_path),\n                task,\n                self._bound_row_filter,\n                self._projected_schema,\n                self._projected_field_ids,\n                deletes_per_file.get(task.file.file_path),\n                self._case_sensitive,\n                self._table_metadata.name_mapping(),\n                self._use_large_types,\n                self._table_metadata.spec(),\n            )\n            for batch in batches:\n                if self._limit is not None:\n                    if total_row_count &gt;= self._limit:\n                        break\n                    elif total_row_count + len(batch) &gt;= self._limit:\n                        batch = batch.slice(0, self._limit - total_row_count)\n                yield batch\n                total_row_count += len(batch)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan._limit","title":"<code>_limit = limit</code>  <code>instance-attribute</code>","text":"<p>Scan the Iceberg Table and create an Arrow construct.</p> <p>Attributes:</p> Name Type Description <code>_table_metadata</code> <p>Current table metadata of the Iceberg table</p> <code>_io</code> <p>PyIceberg FileIO implementation from which to fetch the io properties</p> <code>_projected_schema</code> <p>Iceberg Schema to project onto the data files</p> <code>_bound_row_filter</code> <p>Schema bound row expression to filter the data with</p> <code>_case_sensitive</code> <p>Case sensitivity when looking up column names</p> <code>_limit</code> <p>Limit the number of records.</p>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan._projected_field_ids","title":"<code>_projected_field_ids</code>  <code>property</code>","text":"<p>Set of field IDs that should be projected from the data files.</p>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan._use_large_types","title":"<code>_use_large_types</code>  <code>property</code>","text":"<p>Whether to represent data as large arrow types.</p> <p>Defaults to True.</p>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan.to_record_batches","title":"<code>to_record_batches(tasks)</code>","text":"<p>Scan the Iceberg table and return an Iterator[pa.RecordBatch].</p> <p>Returns an Iterator of pa.RecordBatch with data from the Iceberg table by resolving the right columns that match the current table schema. Only data that matches the provided row_filter expression is returned.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[FileScanTask]</code> <p>FileScanTasks representing the data files and delete files to read from.</p> required <p>Returns:</p> Type Description <code>Iterator[RecordBatch]</code> <p>An Iterator of PyArrow RecordBatches.</p> <code>Iterator[RecordBatch]</code> <p>Total number of rows will be capped if specified.</p> <p>Raises:</p> Type Description <code>ResolveError</code> <p>When a required field cannot be found in the file</p> <code>ValueError</code> <p>When a field type in the file cannot be projected to the schema type</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_record_batches(self, tasks: Iterable[FileScanTask]) -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"Scan the Iceberg table and return an Iterator[pa.RecordBatch].\n\n    Returns an Iterator of pa.RecordBatch with data from the Iceberg table\n    by resolving the right columns that match the current table schema.\n    Only data that matches the provided row_filter expression is returned.\n\n    Args:\n        tasks: FileScanTasks representing the data files and delete files to read from.\n\n    Returns:\n        An Iterator of PyArrow RecordBatches.\n        Total number of rows will be capped if specified.\n\n    Raises:\n        ResolveError: When a required field cannot be found in the file\n        ValueError: When a field type in the file cannot be projected to the schema type\n    \"\"\"\n    deletes_per_file = _read_all_delete_files(self._io, tasks)\n    return self._record_batches_from_scan_tasks_and_deletes(tasks, deletes_per_file)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan.to_table","title":"<code>to_table(tasks)</code>","text":"<p>Scan the Iceberg table and return a pa.Table.</p> <p>Returns a pa.Table with data from the Iceberg table by resolving the right columns that match the current table schema. Only data that matches the provided row_filter expression is returned.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[FileScanTask]</code> <p>FileScanTasks representing the data files and delete files to read from.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>A PyArrow table. Total number of rows will be capped if specified.</p> <p>Raises:</p> Type Description <code>ResolveError</code> <p>When a required field cannot be found in the file</p> <code>ValueError</code> <p>When a field type in the file cannot be projected to the schema type</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_table(self, tasks: Iterable[FileScanTask]) -&gt; pa.Table:\n    \"\"\"Scan the Iceberg table and return a pa.Table.\n\n    Returns a pa.Table with data from the Iceberg table by resolving the\n    right columns that match the current table schema. Only data that\n    matches the provided row_filter expression is returned.\n\n    Args:\n        tasks: FileScanTasks representing the data files and delete files to read from.\n\n    Returns:\n        A PyArrow table. Total number of rows will be capped if specified.\n\n    Raises:\n        ResolveError: When a required field cannot be found in the file\n        ValueError: When a field type in the file cannot be projected to the schema type\n    \"\"\"\n    deletes_per_file = _read_all_delete_files(self._io, tasks)\n    executor = ExecutorFactory.get_or_create()\n\n    def _table_from_scan_task(task: FileScanTask) -&gt; pa.Table:\n        batches = list(self._record_batches_from_scan_tasks_and_deletes([task], deletes_per_file))\n        if len(batches) &gt; 0:\n            return pa.Table.from_batches(batches)\n        else:\n            return None\n\n    futures = [\n        executor.submit(\n            _table_from_scan_task,\n            task,\n        )\n        for task in tasks\n    ]\n    total_row_count = 0\n    # for consistent ordering, we need to maintain future order\n    futures_index = {f: i for i, f in enumerate(futures)}\n    completed_futures: SortedList[Future[pa.Table]] = SortedList(iterable=[], key=lambda f: futures_index[f])\n    for future in concurrent.futures.as_completed(futures):\n        completed_futures.add(future)\n        if table_result := future.result():\n            total_row_count += len(table_result)\n        # stop early if limit is satisfied\n        if self._limit is not None and total_row_count &gt;= self._limit:\n            break\n\n    # by now, we've either completed all tasks or satisfied the limit\n    if self._limit is not None:\n        _ = [f.cancel() for f in futures if not f.done()]\n\n    tables = [f.result() for f in completed_futures if f.result()]\n\n    if len(tables) &lt; 1:\n        return pa.Table.from_batches([], schema=schema_to_pyarrow(self._projected_schema, include_field_ids=False))\n\n    result = pa.concat_tables(tables, promote_options=\"permissive\")\n\n    if self._limit is not None:\n        return result.slice(0, self._limit)\n\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile","title":"<code>PyArrowFile</code>","text":"<p>               Bases: <code>InputFile</code>, <code>OutputFile</code></p> <p>A combined InputFile and OutputFile implementation that uses a pyarrow filesystem to generate pyarrow.lib.NativeFile instances.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location(str)</code> <p>The URI or path to a local file for a PyArrowFile instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.io.pyarrow import PyArrowFile\n&gt;&gt;&gt; # input_file = PyArrowFile(\"s3://foo/bar.txt\")\n&gt;&gt;&gt; # Read the contents of the PyArrowFile instance\n&gt;&gt;&gt; # Make sure that you have permissions to read/write\n&gt;&gt;&gt; # file_content = input_file.open().read()\n</code></pre> <pre><code>&gt;&gt;&gt; # output_file = PyArrowFile(\"s3://baz/qux.txt\")\n&gt;&gt;&gt; # Write bytes to a file\n&gt;&gt;&gt; # Make sure that you have permissions to read/write\n&gt;&gt;&gt; # output_file.create().write(b'foobytes')\n</code></pre> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowFile(InputFile, OutputFile):\n    \"\"\"A combined InputFile and OutputFile implementation that uses a pyarrow filesystem to generate pyarrow.lib.NativeFile instances.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location(str): The URI or path to a local file for a PyArrowFile instance.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.io.pyarrow import PyArrowFile\n        &gt;&gt;&gt; # input_file = PyArrowFile(\"s3://foo/bar.txt\")\n        &gt;&gt;&gt; # Read the contents of the PyArrowFile instance\n        &gt;&gt;&gt; # Make sure that you have permissions to read/write\n        &gt;&gt;&gt; # file_content = input_file.open().read()\n\n        &gt;&gt;&gt; # output_file = PyArrowFile(\"s3://baz/qux.txt\")\n        &gt;&gt;&gt; # Write bytes to a file\n        &gt;&gt;&gt; # Make sure that you have permissions to read/write\n        &gt;&gt;&gt; # output_file.create().write(b'foobytes')\n    \"\"\"\n\n    _filesystem: FileSystem\n    _path: str\n    _buffer_size: int\n\n    def __init__(self, location: str, path: str, fs: FileSystem, buffer_size: int = ONE_MEGABYTE):\n        self._filesystem = fs\n        self._path = path\n        self._buffer_size = buffer_size\n        super().__init__(location=location)\n\n    def _file_info(self) -&gt; FileInfo:\n        \"\"\"Retrieve a pyarrow.fs.FileInfo object for the location.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        try:\n            file_info = self._filesystem.get_file_info(self._path)\n        except OSError as e:\n            if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot get file info, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n\n        if file_info.type == FileType.NotFound:\n            raise FileNotFoundError(f\"Cannot get file info, file not found: {self.location}\")\n        return file_info\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        file_info = self._file_info()\n        return file_info.size\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        try:\n            self._file_info()  # raises FileNotFoundError if it does not exist\n            return True\n        except FileNotFoundError:\n            return False\n\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Open the location using a PyArrow FileSystem inferred from the location.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            pyarrow.lib.NativeFile: A NativeFile instance for the file located at `self.location`.\n\n        Raises:\n            FileNotFoundError: If the file at self.location does not exist.\n            PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        try:\n            if seekable:\n                input_file = self._filesystem.open_input_file(self._path)\n            else:\n                input_file = self._filesystem.open_input_stream(self._path, buffer_size=self._buffer_size)\n        except FileNotFoundError:\n            raise\n        except PermissionError:\n            raise\n        except OSError as e:\n            if e.errno == 2 or \"Path does not exist\" in str(e):\n                raise FileNotFoundError(f\"Cannot open file, does not exist: {self.location}\") from e\n            elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot open file, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n        return input_file\n\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.\n\n        Args:\n            overwrite (bool): Whether to overwrite the file if it already exists.\n\n        Returns:\n            pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.\n\n        Raises:\n            FileExistsError: If the file already exists at `self.location` and `overwrite` is False.\n\n        Note:\n            This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False,\n            a check is first performed to verify that the file does not exist. This is not thread-safe and\n            a possibility does exist that the file can be created by a concurrent process after the existence\n            check yet before the output stream is created. In such a case, the default pyarrow behavior will\n            truncate the contents of the existing file when opening the output stream.\n        \"\"\"\n        try:\n            if not overwrite and self.exists() is True:\n                raise FileExistsError(f\"Cannot create file, already exists: {self.location}\")\n            output_file = self._filesystem.open_output_stream(self._path, buffer_size=self._buffer_size)\n        except PermissionError:\n            raise\n        except OSError as e:\n            if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot create file, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n        return output_file\n\n    def to_input_file(self) -&gt; PyArrowFile:\n        \"\"\"Return a new PyArrowFile for the location of an existing PyArrowFile instance.\n\n        This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single\n        PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns\n        a copy of the same instance.\n        \"\"\"\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    file_info = self._file_info()\n    return file_info.size\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile._file_info","title":"<code>_file_info()</code>","text":"<p>Retrieve a pyarrow.fs.FileInfo object for the location.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error such as an AWS error code 15.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _file_info(self) -&gt; FileInfo:\n    \"\"\"Retrieve a pyarrow.fs.FileInfo object for the location.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n            an AWS error code 15.\n    \"\"\"\n    try:\n        file_info = self._filesystem.get_file_info(self._path)\n    except OSError as e:\n        if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot get file info, access denied: {self.location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n\n    if file_info.type == FileType.NotFound:\n        raise FileNotFoundError(f\"Cannot get file info, file not found: {self.location}\")\n    return file_info\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.create","title":"<code>create(overwrite=False)</code>","text":"<p>Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>OutputStream</code> <p>pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file already exists at <code>self.location</code> and <code>overwrite</code> is False.</p> Note <p>This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False, a check is first performed to verify that the file does not exist. This is not thread-safe and a possibility does exist that the file can be created by a concurrent process after the existence check yet before the output stream is created. In such a case, the default pyarrow behavior will truncate the contents of the existing file when opening the output stream.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.\n\n    Args:\n        overwrite (bool): Whether to overwrite the file if it already exists.\n\n    Returns:\n        pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.\n\n    Raises:\n        FileExistsError: If the file already exists at `self.location` and `overwrite` is False.\n\n    Note:\n        This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False,\n        a check is first performed to verify that the file does not exist. This is not thread-safe and\n        a possibility does exist that the file can be created by a concurrent process after the existence\n        check yet before the output stream is created. In such a case, the default pyarrow behavior will\n        truncate the contents of the existing file when opening the output stream.\n    \"\"\"\n    try:\n        if not overwrite and self.exists() is True:\n            raise FileExistsError(f\"Cannot create file, already exists: {self.location}\")\n        output_file = self._filesystem.open_output_stream(self._path, buffer_size=self._buffer_size)\n    except PermissionError:\n        raise\n    except OSError as e:\n        if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot create file, access denied: {self.location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n    return output_file\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    try:\n        self._file_info()  # raises FileNotFoundError if it does not exist\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.open","title":"<code>open(seekable=True)</code>","text":"<p>Open the location using a PyArrow FileSystem inferred from the location.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Type Description <code>InputStream</code> <p>pyarrow.lib.NativeFile: A NativeFile instance for the file located at <code>self.location</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file at self.location does not exist.</p> <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error such as an AWS error code 15.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Open the location using a PyArrow FileSystem inferred from the location.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        pyarrow.lib.NativeFile: A NativeFile instance for the file located at `self.location`.\n\n    Raises:\n        FileNotFoundError: If the file at self.location does not exist.\n        PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n            an AWS error code 15.\n    \"\"\"\n    try:\n        if seekable:\n            input_file = self._filesystem.open_input_file(self._path)\n        else:\n            input_file = self._filesystem.open_input_stream(self._path, buffer_size=self._buffer_size)\n    except FileNotFoundError:\n        raise\n    except PermissionError:\n        raise\n    except OSError as e:\n        if e.errno == 2 or \"Path does not exist\" in str(e):\n            raise FileNotFoundError(f\"Cannot open file, does not exist: {self.location}\") from e\n        elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot open file, access denied: {self.location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n    return input_file\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.to_input_file","title":"<code>to_input_file()</code>","text":"<p>Return a new PyArrowFile for the location of an existing PyArrowFile instance.</p> <p>This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns a copy of the same instance.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_input_file(self) -&gt; PyArrowFile:\n    \"\"\"Return a new PyArrowFile for the location of an existing PyArrowFile instance.\n\n    This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single\n    PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns\n    a copy of the same instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO","title":"<code>PyArrowFileIO</code>","text":"<p>               Bases: <code>FileIO</code></p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowFileIO(FileIO):\n    fs_by_scheme: Callable[[str, Optional[str]], FileSystem]\n\n    def __init__(self, properties: Properties = EMPTY_DICT):\n        self.fs_by_scheme: Callable[[str, Optional[str]], FileSystem] = lru_cache(self._initialize_fs)\n        super().__init__(properties=properties)\n\n    @staticmethod\n    def parse_location(location: str) -&gt; Tuple[str, str, str]:\n        \"\"\"Return the path without the scheme.\"\"\"\n        uri = urlparse(location)\n        if not uri.scheme:\n            return \"file\", uri.netloc, os.path.abspath(location)\n        elif uri.scheme in (\"hdfs\", \"viewfs\"):\n            return uri.scheme, uri.netloc, uri.path\n        else:\n            return uri.scheme, uri.netloc, f\"{uri.netloc}{uri.path}\"\n\n    def _initialize_fs(self, scheme: str, netloc: Optional[str] = None) -&gt; FileSystem:\n        \"\"\"Initialize FileSystem for different scheme.\"\"\"\n        if scheme in {\"oss\"}:\n            return self._initialize_oss_fs()\n\n        elif scheme in {\"s3\", \"s3a\", \"s3n\"}:\n            return self._initialize_s3_fs(netloc)\n\n        elif scheme in {\"hdfs\", \"viewfs\"}:\n            return self._initialize_hdfs_fs(scheme, netloc)\n\n        elif scheme in {\"gs\", \"gcs\"}:\n            return self._initialize_gcs_fs()\n\n        elif scheme in {\"file\"}:\n            return self._initialize_local_fs()\n\n        else:\n            raise ValueError(f\"Unrecognized filesystem type in URI: {scheme}\")\n\n    def _initialize_oss_fs(self) -&gt; FileSystem:\n        from pyarrow.fs import S3FileSystem\n\n        client_kwargs: Dict[str, Any] = {\n            \"endpoint_override\": self.properties.get(S3_ENDPOINT),\n            \"access_key\": get_first_property_value(self.properties, S3_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            \"secret_key\": get_first_property_value(self.properties, S3_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            \"session_token\": get_first_property_value(self.properties, S3_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            \"region\": get_first_property_value(self.properties, S3_REGION, AWS_REGION),\n        }\n\n        if proxy_uri := self.properties.get(S3_PROXY_URI):\n            client_kwargs[\"proxy_options\"] = proxy_uri\n\n        if connect_timeout := self.properties.get(S3_CONNECT_TIMEOUT):\n            client_kwargs[\"connect_timeout\"] = float(connect_timeout)\n\n        if request_timeout := self.properties.get(S3_REQUEST_TIMEOUT):\n            client_kwargs[\"request_timeout\"] = float(request_timeout)\n\n        if role_arn := get_first_property_value(self.properties, S3_ROLE_ARN, AWS_ROLE_ARN):\n            client_kwargs[\"role_arn\"] = role_arn\n\n        if session_name := get_first_property_value(self.properties, S3_ROLE_SESSION_NAME, AWS_ROLE_SESSION_NAME):\n            client_kwargs[\"session_name\"] = session_name\n\n        if force_virtual_addressing := self.properties.get(S3_FORCE_VIRTUAL_ADDRESSING):\n            client_kwargs[\"force_virtual_addressing\"] = property_as_bool(self.properties, force_virtual_addressing, False)\n\n        return S3FileSystem(**client_kwargs)\n\n    def _initialize_s3_fs(self, netloc: Optional[str]) -&gt; FileSystem:\n        from pyarrow.fs import S3FileSystem\n\n        provided_region = get_first_property_value(self.properties, S3_REGION, AWS_REGION)\n\n        # Do this when we don't provide the region at all, or when we explicitly enable it\n        if provided_region is None or property_as_bool(self.properties, S3_RESOLVE_REGION, False) is True:\n            # Resolve region from netloc(bucket), fallback to user-provided region\n            # Only supported by buckets hosted by S3\n            bucket_region = _cached_resolve_s3_region(bucket=netloc) or provided_region\n            if provided_region is not None and bucket_region != provided_region:\n                logger.warning(\n                    f\"PyArrow FileIO overriding S3 bucket region for bucket {netloc}: \"\n                    f\"provided region {provided_region}, actual region {bucket_region}\"\n                )\n        else:\n            bucket_region = provided_region\n\n        client_kwargs: Dict[str, Any] = {\n            \"endpoint_override\": self.properties.get(S3_ENDPOINT),\n            \"access_key\": get_first_property_value(self.properties, S3_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            \"secret_key\": get_first_property_value(self.properties, S3_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            \"session_token\": get_first_property_value(self.properties, S3_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            \"region\": bucket_region,\n        }\n\n        if proxy_uri := self.properties.get(S3_PROXY_URI):\n            client_kwargs[\"proxy_options\"] = proxy_uri\n\n        if connect_timeout := self.properties.get(S3_CONNECT_TIMEOUT):\n            client_kwargs[\"connect_timeout\"] = float(connect_timeout)\n\n        if request_timeout := self.properties.get(S3_REQUEST_TIMEOUT):\n            client_kwargs[\"request_timeout\"] = float(request_timeout)\n\n        if role_arn := get_first_property_value(self.properties, S3_ROLE_ARN, AWS_ROLE_ARN):\n            client_kwargs[\"role_arn\"] = role_arn\n\n        if session_name := get_first_property_value(self.properties, S3_ROLE_SESSION_NAME, AWS_ROLE_SESSION_NAME):\n            client_kwargs[\"session_name\"] = session_name\n\n        if force_virtual_addressing := self.properties.get(S3_FORCE_VIRTUAL_ADDRESSING):\n            client_kwargs[\"force_virtual_addressing\"] = property_as_bool(self.properties, force_virtual_addressing, False)\n\n        return S3FileSystem(**client_kwargs)\n\n    def _initialize_hdfs_fs(self, scheme: str, netloc: Optional[str]) -&gt; FileSystem:\n        from pyarrow.fs import HadoopFileSystem\n\n        hdfs_kwargs: Dict[str, Any] = {}\n        if netloc:\n            return HadoopFileSystem.from_uri(f\"{scheme}://{netloc}\")\n        if host := self.properties.get(HDFS_HOST):\n            hdfs_kwargs[\"host\"] = host\n        if port := self.properties.get(HDFS_PORT):\n            # port should be an integer type\n            hdfs_kwargs[\"port\"] = int(port)\n        if user := self.properties.get(HDFS_USER):\n            hdfs_kwargs[\"user\"] = user\n        if kerb_ticket := self.properties.get(HDFS_KERB_TICKET):\n            hdfs_kwargs[\"kerb_ticket\"] = kerb_ticket\n\n        return HadoopFileSystem(**hdfs_kwargs)\n\n    def _initialize_gcs_fs(self) -&gt; FileSystem:\n        from pyarrow.fs import GcsFileSystem\n\n        gcs_kwargs: Dict[str, Any] = {}\n        if access_token := self.properties.get(GCS_TOKEN):\n            gcs_kwargs[\"access_token\"] = access_token\n        if expiration := self.properties.get(GCS_TOKEN_EXPIRES_AT_MS):\n            gcs_kwargs[\"credential_token_expiration\"] = millis_to_datetime(int(expiration))\n        if bucket_location := self.properties.get(GCS_DEFAULT_LOCATION):\n            gcs_kwargs[\"default_bucket_location\"] = bucket_location\n        if endpoint := self.properties.get(GCS_SERVICE_HOST):\n            url_parts = urlparse(endpoint)\n            gcs_kwargs[\"scheme\"] = url_parts.scheme\n            gcs_kwargs[\"endpoint_override\"] = url_parts.netloc\n\n        return GcsFileSystem(**gcs_kwargs)\n\n    def _initialize_local_fs(self) -&gt; FileSystem:\n        return PyArrowLocalFileSystem()\n\n    def new_input(self, location: str) -&gt; PyArrowFile:\n        \"\"\"Get a PyArrowFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            PyArrowFile: A PyArrowFile instance for the given location.\n        \"\"\"\n        scheme, netloc, path = self.parse_location(location)\n        return PyArrowFile(\n            fs=self.fs_by_scheme(scheme, netloc),\n            location=location,\n            path=path,\n            buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n        )\n\n    def new_output(self, location: str) -&gt; PyArrowFile:\n        \"\"\"Get a PyArrowFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            PyArrowFile: A PyArrowFile instance for the given location.\n        \"\"\"\n        scheme, netloc, path = self.parse_location(location)\n        return PyArrowFile(\n            fs=self.fs_by_scheme(scheme, netloc),\n            location=location,\n            path=path,\n            buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n        )\n\n    def delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n        \"\"\"Delete the file at the given location.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an OutputFile instance is provided,\n                the location attribute for that instance is used as the location to delete.\n\n        Raises:\n            FileNotFoundError: When the file at the provided location does not exist.\n            PermissionError: If the file at the provided location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        str_location = location.location if isinstance(location, (InputFile, OutputFile)) else location\n        scheme, netloc, path = self.parse_location(str_location)\n        fs = self.fs_by_scheme(scheme, netloc)\n\n        try:\n            fs.delete_file(path)\n        except FileNotFoundError:\n            raise\n        except PermissionError:\n            raise\n        except OSError as e:\n            if e.errno == 2 or \"Path does not exist\" in str(e):\n                raise FileNotFoundError(f\"Cannot delete file, does not exist: {location}\") from e\n            elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot delete file, access denied: {location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        \"\"\"Create a dictionary of the PyArrowFileIO fields used when pickling.\"\"\"\n        fileio_copy = copy(self.__dict__)\n        fileio_copy[\"fs_by_scheme\"] = None\n        return fileio_copy\n\n    def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n        \"\"\"Deserialize the state into a PyArrowFileIO instance.\"\"\"\n        self.__dict__ = state\n        self.fs_by_scheme = lru_cache(self._initialize_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a dictionary of the PyArrowFileIO fields used when pickling.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __getstate__(self) -&gt; Dict[str, Any]:\n    \"\"\"Create a dictionary of the PyArrowFileIO fields used when pickling.\"\"\"\n    fileio_copy = copy(self.__dict__)\n    fileio_copy[\"fs_by_scheme\"] = None\n    return fileio_copy\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Deserialize the state into a PyArrowFileIO instance.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n    \"\"\"Deserialize the state into a PyArrowFileIO instance.\"\"\"\n    self.__dict__ = state\n    self.fs_by_scheme = lru_cache(self._initialize_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO._initialize_fs","title":"<code>_initialize_fs(scheme, netloc=None)</code>","text":"<p>Initialize FileSystem for different scheme.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _initialize_fs(self, scheme: str, netloc: Optional[str] = None) -&gt; FileSystem:\n    \"\"\"Initialize FileSystem for different scheme.\"\"\"\n    if scheme in {\"oss\"}:\n        return self._initialize_oss_fs()\n\n    elif scheme in {\"s3\", \"s3a\", \"s3n\"}:\n        return self._initialize_s3_fs(netloc)\n\n    elif scheme in {\"hdfs\", \"viewfs\"}:\n        return self._initialize_hdfs_fs(scheme, netloc)\n\n    elif scheme in {\"gs\", \"gcs\"}:\n        return self._initialize_gcs_fs()\n\n    elif scheme in {\"file\"}:\n        return self._initialize_local_fs()\n\n    else:\n        raise ValueError(f\"Unrecognized filesystem type in URI: {scheme}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.delete","title":"<code>delete(location)</code>","text":"<p>Delete the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>The URI to the file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the location to delete.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>When the file at the provided location does not exist.</p> <code>PermissionError</code> <p>If the file at the provided location cannot be accessed due to a permission error such as an AWS error code 15.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def delete(self, location: Union[str, InputFile, OutputFile]) -&gt; None:\n    \"\"\"Delete the file at the given location.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an OutputFile instance is provided,\n            the location attribute for that instance is used as the location to delete.\n\n    Raises:\n        FileNotFoundError: When the file at the provided location does not exist.\n        PermissionError: If the file at the provided location cannot be accessed due to a permission error such as\n            an AWS error code 15.\n    \"\"\"\n    str_location = location.location if isinstance(location, (InputFile, OutputFile)) else location\n    scheme, netloc, path = self.parse_location(str_location)\n    fs = self.fs_by_scheme(scheme, netloc)\n\n    try:\n        fs.delete_file(path)\n    except FileNotFoundError:\n        raise\n    except PermissionError:\n        raise\n    except OSError as e:\n        if e.errno == 2 or \"Path does not exist\" in str(e):\n            raise FileNotFoundError(f\"Cannot delete file, does not exist: {location}\") from e\n        elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot delete file, access denied: {location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.new_input","title":"<code>new_input(location)</code>","text":"<p>Get a PyArrowFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>PyArrowFile</code> <code>PyArrowFile</code> <p>A PyArrowFile instance for the given location.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def new_input(self, location: str) -&gt; PyArrowFile:\n    \"\"\"Get a PyArrowFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        PyArrowFile: A PyArrowFile instance for the given location.\n    \"\"\"\n    scheme, netloc, path = self.parse_location(location)\n    return PyArrowFile(\n        fs=self.fs_by_scheme(scheme, netloc),\n        location=location,\n        path=path,\n        buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.new_output","title":"<code>new_output(location)</code>","text":"<p>Get a PyArrowFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>PyArrowFile</code> <code>PyArrowFile</code> <p>A PyArrowFile instance for the given location.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def new_output(self, location: str) -&gt; PyArrowFile:\n    \"\"\"Get a PyArrowFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        PyArrowFile: A PyArrowFile instance for the given location.\n    \"\"\"\n    scheme, netloc, path = self.parse_location(location)\n    return PyArrowFile(\n        fs=self.fs_by_scheme(scheme, netloc),\n        location=location,\n        path=path,\n        buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.parse_location","title":"<code>parse_location(location)</code>  <code>staticmethod</code>","text":"<p>Return the path without the scheme.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@staticmethod\ndef parse_location(location: str) -&gt; Tuple[str, str, str]:\n    \"\"\"Return the path without the scheme.\"\"\"\n    uri = urlparse(location)\n    if not uri.scheme:\n        return \"file\", uri.netloc, os.path.abspath(location)\n    elif uri.scheme in (\"hdfs\", \"viewfs\"):\n        return uri.scheme, uri.netloc, uri.path\n    else:\n        return uri.scheme, uri.netloc, f\"{uri.netloc}{uri.path}\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor","title":"<code>PyArrowSchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowSchemaVisitor(Generic[T], ABC):\n    def before_field(self, field: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n\n    def after_list_element(self, element: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n\n    def before_map_key(self, key: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n\n    def after_map_key(self, key: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n\n    def before_map_value(self, value: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n\n    def after_map_value(self, value: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n\n    @abstractmethod\n    def schema(self, schema: pa.Schema, struct_result: T) -&gt; T:\n        \"\"\"Visit a schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: pa.StructType, field_results: List[T]) -&gt; T:\n        \"\"\"Visit a struct.\"\"\"\n\n    @abstractmethod\n    def field(self, field: pa.Field, field_result: T) -&gt; T:\n        \"\"\"Visit a field.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: pa.ListType, element_result: T) -&gt; T:\n        \"\"\"Visit a list.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: pa.MapType, key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a map.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: pa.DataType) -&gt; T:\n        \"\"\"Visit a primitive type.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_field","title":"<code>after_field(field)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_field(self, field: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_list_element","title":"<code>after_list_element(element)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_list_element(self, element: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_map_key","title":"<code>after_map_key(key)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_map_key(self, key: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_map_value","title":"<code>after_map_value(value)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_map_value(self, value: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_field","title":"<code>before_field(field)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_field(self, field: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_list_element","title":"<code>before_list_element(element)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_list_element(self, element: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_map_key","title":"<code>before_map_key(key)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_map_key(self, key: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_map_value","title":"<code>before_map_value(value)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_map_value(self, value: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef field(self, field: pa.Field, field_result: T) -&gt; T:\n    \"\"\"Visit a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a list.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: pa.ListType, element_result: T) -&gt; T:\n    \"\"\"Visit a list.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a map.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: pa.MapType, key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a primitive type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: pa.DataType) -&gt; T:\n    \"\"\"Visit a primitive type.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a schema.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: pa.Schema, struct_result: T) -&gt; T:\n    \"\"\"Visit a schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a struct.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: pa.StructType, field_results: List[T]) -&gt; T:\n    \"\"\"Visit a struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.UnsupportedPyArrowTypeException","title":"<code>UnsupportedPyArrowTypeException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Cannot convert PyArrow type to corresponding Iceberg type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class UnsupportedPyArrowTypeException(Exception):\n    \"\"\"Cannot convert PyArrow type to corresponding Iceberg type.\"\"\"\n\n    def __init__(self, field: pa.Field, *args: Any):\n        self.field = field\n        super().__init__(*args)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._ConvertToIceberg","title":"<code>_ConvertToIceberg</code>","text":"<p>               Bases: <code>PyArrowSchemaVisitor[Union[IcebergType, Schema]]</code></p> <p>Converts PyArrowSchema to Iceberg Schema. Applies the IDs from name_mapping if provided.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class _ConvertToIceberg(PyArrowSchemaVisitor[Union[IcebergType, Schema]]):\n    \"\"\"Converts PyArrowSchema to Iceberg Schema. Applies the IDs from name_mapping if provided.\"\"\"\n\n    _field_names: List[str]\n\n    def __init__(self, downcast_ns_timestamp_to_us: bool = False) -&gt; None:\n        self._field_names = []\n        self._downcast_ns_timestamp_to_us = downcast_ns_timestamp_to_us\n\n    def _field_id(self, field: pa.Field) -&gt; int:\n        if (field_id := _get_field_id(field)) is not None:\n            return field_id\n        else:\n            raise ValueError(f\"Cannot convert {field} to Iceberg Field as field_id is empty.\")\n\n    def schema(self, schema: pa.Schema, struct_result: StructType) -&gt; Schema:\n        return Schema(*struct_result.fields)\n\n    def struct(self, struct: pa.StructType, field_results: List[NestedField]) -&gt; StructType:\n        return StructType(*field_results)\n\n    def field(self, field: pa.Field, field_result: IcebergType) -&gt; NestedField:\n        field_id = self._field_id(field)\n        field_doc = doc_str.decode() if (field.metadata and (doc_str := field.metadata.get(PYARROW_FIELD_DOC_KEY))) else None\n        field_type = field_result\n        return NestedField(field_id, field.name, field_type, required=not field.nullable, doc=field_doc)\n\n    def list(self, list_type: pa.ListType, element_result: IcebergType) -&gt; ListType:\n        element_field = list_type.value_field\n        self._field_names.append(LIST_ELEMENT_NAME)\n        element_id = self._field_id(element_field)\n        self._field_names.pop()\n        return ListType(element_id, element_result, element_required=not element_field.nullable)\n\n    def map(self, map_type: pa.MapType, key_result: IcebergType, value_result: IcebergType) -&gt; MapType:\n        key_field = map_type.key_field\n        self._field_names.append(MAP_KEY_NAME)\n        key_id = self._field_id(key_field)\n        self._field_names.pop()\n        value_field = map_type.item_field\n        self._field_names.append(MAP_VALUE_NAME)\n        value_id = self._field_id(value_field)\n        self._field_names.pop()\n        return MapType(key_id, key_result, value_id, value_result, value_required=not value_field.nullable)\n\n    def primitive(self, primitive: pa.DataType) -&gt; PrimitiveType:\n        if pa.types.is_boolean(primitive):\n            return BooleanType()\n        elif pa.types.is_integer(primitive):\n            width = primitive.bit_width\n            if width &lt;= 32:\n                return IntegerType()\n            elif width &lt;= 64:\n                return LongType()\n            else:\n                # Does not exist (yet)\n                raise TypeError(f\"Unsupported integer type: {primitive}\")\n        elif pa.types.is_float32(primitive):\n            return FloatType()\n        elif pa.types.is_float64(primitive):\n            return DoubleType()\n        elif isinstance(primitive, pa.Decimal128Type):\n            primitive = cast(pa.Decimal128Type, primitive)\n            return DecimalType(primitive.precision, primitive.scale)\n        elif pa.types.is_string(primitive) or pa.types.is_large_string(primitive):\n            return StringType()\n        elif pa.types.is_date32(primitive):\n            return DateType()\n        elif isinstance(primitive, pa.Time64Type) and primitive.unit == \"us\":\n            return TimeType()\n        elif pa.types.is_timestamp(primitive):\n            primitive = cast(pa.TimestampType, primitive)\n            if primitive.unit in (\"s\", \"ms\", \"us\"):\n                # Supported types, will be upcast automatically to 'us'\n                pass\n            elif primitive.unit == \"ns\":\n                if self._downcast_ns_timestamp_to_us:\n                    logger.warning(\"Iceberg does not yet support 'ns' timestamp precision. Downcasting to 'us'.\")\n                else:\n                    raise TypeError(\n                        \"Iceberg does not yet support 'ns' timestamp precision. Use 'downcast-ns-timestamp-to-us-on-write' configuration property to automatically downcast 'ns' to 'us' on write.\",\n                    )\n            else:\n                raise TypeError(f\"Unsupported precision for timestamp type: {primitive.unit}\")\n\n            if primitive.tz in UTC_ALIASES:\n                return TimestamptzType()\n            elif primitive.tz is None:\n                return TimestampType()\n\n        elif pa.types.is_binary(primitive) or pa.types.is_large_binary(primitive):\n            return BinaryType()\n        elif pa.types.is_fixed_size_binary(primitive):\n            primitive = cast(pa.FixedSizeBinaryType, primitive)\n            return FixedType(primitive.byte_width)\n\n        raise TypeError(f\"Unsupported type: {primitive}\")\n\n    def before_field(self, field: pa.Field) -&gt; None:\n        self._field_names.append(field.name)\n\n    def after_field(self, field: pa.Field) -&gt; None:\n        self._field_names.pop()\n\n    def before_list_element(self, element: pa.Field) -&gt; None:\n        self._field_names.append(LIST_ELEMENT_NAME)\n\n    def after_list_element(self, element: pa.Field) -&gt; None:\n        self._field_names.pop()\n\n    def before_map_key(self, key: pa.Field) -&gt; None:\n        self._field_names.append(MAP_KEY_NAME)\n\n    def after_map_key(self, element: pa.Field) -&gt; None:\n        self._field_names.pop()\n\n    def before_map_value(self, value: pa.Field) -&gt; None:\n        self._field_names.append(MAP_VALUE_NAME)\n\n    def after_map_value(self, element: pa.Field) -&gt; None:\n        self._field_names.pop()\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._ConvertToIcebergWithoutIDs","title":"<code>_ConvertToIcebergWithoutIDs</code>","text":"<p>               Bases: <code>_ConvertToIceberg</code></p> <p>Converts PyArrowSchema to Iceberg Schema with all -1 ids.</p> <p>The schema generated through this visitor should always be used in conjunction with <code>new_table_metadata</code> function to assign new field ids in order. This is currently used only when creating an Iceberg Schema from a PyArrow schema when creating a new Iceberg table.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class _ConvertToIcebergWithoutIDs(_ConvertToIceberg):\n    \"\"\"\n    Converts PyArrowSchema to Iceberg Schema with all -1 ids.\n\n    The schema generated through this visitor should always be\n    used in conjunction with `new_table_metadata` function to\n    assign new field ids in order. This is currently used only\n    when creating an Iceberg Schema from a PyArrow schema when\n    creating a new Iceberg table.\n    \"\"\"\n\n    def _field_id(self, field: pa.Field) -&gt; int:\n        return -1\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector","title":"<code>_NullNaNUnmentionedTermsCollector</code>","text":"<p>               Bases: <code>BoundBooleanExpressionVisitor[None]</code></p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class _NullNaNUnmentionedTermsCollector(BoundBooleanExpressionVisitor[None]):\n    # BoundTerms which have either is_null or is_not_null appearing at least once in the boolean expr.\n    is_null_or_not_bound_terms: set[BoundTerm[Any]]\n    # The remaining BoundTerms appearing in the boolean expr.\n    null_unmentioned_bound_terms: set[BoundTerm[Any]]\n    # BoundTerms which have either is_nan or is_not_nan appearing at least once in the boolean expr.\n    is_nan_or_not_bound_terms: set[BoundTerm[Any]]\n    # The remaining BoundTerms appearing in the boolean expr.\n    nan_unmentioned_bound_terms: set[BoundTerm[Any]]\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.is_null_or_not_bound_terms = set()\n        self.null_unmentioned_bound_terms = set()\n        self.is_nan_or_not_bound_terms = set()\n        self.nan_unmentioned_bound_terms = set()\n\n    def _handle_explicit_is_null_or_not(self, term: BoundTerm[Any]) -&gt; None:\n        \"\"\"Handle the predicate case where either is_null or is_not_null is included.\"\"\"\n        if term in self.null_unmentioned_bound_terms:\n            self.null_unmentioned_bound_terms.remove(term)\n        self.is_null_or_not_bound_terms.add(term)\n\n    def _handle_null_unmentioned(self, term: BoundTerm[Any]) -&gt; None:\n        \"\"\"Handle the predicate case where neither is_null or is_not_null is included.\"\"\"\n        if term not in self.is_null_or_not_bound_terms:\n            self.null_unmentioned_bound_terms.add(term)\n\n    def _handle_explicit_is_nan_or_not(self, term: BoundTerm[Any]) -&gt; None:\n        \"\"\"Handle the predicate case where either is_nan or is_not_nan is included.\"\"\"\n        if term in self.nan_unmentioned_bound_terms:\n            self.nan_unmentioned_bound_terms.remove(term)\n        self.is_nan_or_not_bound_terms.add(term)\n\n    def _handle_nan_unmentioned(self, term: BoundTerm[Any]) -&gt; None:\n        \"\"\"Handle the predicate case where neither is_nan or is_not_nan is included.\"\"\"\n        if term not in self.is_nan_or_not_bound_terms:\n            self.nan_unmentioned_bound_terms.add(term)\n\n    def visit_in(self, term: BoundTerm[Any], literals: Set[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_not_in(self, term: BoundTerm[Any], literals: Set[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_is_nan(self, term: BoundTerm[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_explicit_is_nan_or_not(term)\n\n    def visit_not_nan(self, term: BoundTerm[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_explicit_is_nan_or_not(term)\n\n    def visit_is_null(self, term: BoundTerm[Any]) -&gt; None:\n        self._handle_explicit_is_null_or_not(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_not_null(self, term: BoundTerm[Any]) -&gt; None:\n        self._handle_explicit_is_null_or_not(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_equal(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_not_equal(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_greater_than_or_equal(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_greater_than(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_less_than(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_less_than_or_equal(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_starts_with(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_not_starts_with(self, term: BoundTerm[Any], literal: Literal[Any]) -&gt; None:\n        self._handle_null_unmentioned(term)\n        self._handle_nan_unmentioned(term)\n\n    def visit_true(self) -&gt; None:\n        return\n\n    def visit_false(self) -&gt; None:\n        return\n\n    def visit_not(self, child_result: None) -&gt; None:\n        return\n\n    def visit_and(self, left_result: None, right_result: None) -&gt; None:\n        return\n\n    def visit_or(self, left_result: None, right_result: None) -&gt; None:\n        return\n\n    def collect(\n        self,\n        expr: BooleanExpression,\n    ) -&gt; None:\n        \"\"\"Collect the bound references categorized by having at least one is_null or is_not_null in the expr and the remaining.\"\"\"\n        boolean_expression_visit(expr, self)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector._handle_explicit_is_nan_or_not","title":"<code>_handle_explicit_is_nan_or_not(term)</code>","text":"<p>Handle the predicate case where either is_nan or is_not_nan is included.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _handle_explicit_is_nan_or_not(self, term: BoundTerm[Any]) -&gt; None:\n    \"\"\"Handle the predicate case where either is_nan or is_not_nan is included.\"\"\"\n    if term in self.nan_unmentioned_bound_terms:\n        self.nan_unmentioned_bound_terms.remove(term)\n    self.is_nan_or_not_bound_terms.add(term)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector._handle_explicit_is_null_or_not","title":"<code>_handle_explicit_is_null_or_not(term)</code>","text":"<p>Handle the predicate case where either is_null or is_not_null is included.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _handle_explicit_is_null_or_not(self, term: BoundTerm[Any]) -&gt; None:\n    \"\"\"Handle the predicate case where either is_null or is_not_null is included.\"\"\"\n    if term in self.null_unmentioned_bound_terms:\n        self.null_unmentioned_bound_terms.remove(term)\n    self.is_null_or_not_bound_terms.add(term)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector._handle_nan_unmentioned","title":"<code>_handle_nan_unmentioned(term)</code>","text":"<p>Handle the predicate case where neither is_nan or is_not_nan is included.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _handle_nan_unmentioned(self, term: BoundTerm[Any]) -&gt; None:\n    \"\"\"Handle the predicate case where neither is_nan or is_not_nan is included.\"\"\"\n    if term not in self.is_nan_or_not_bound_terms:\n        self.nan_unmentioned_bound_terms.add(term)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector._handle_null_unmentioned","title":"<code>_handle_null_unmentioned(term)</code>","text":"<p>Handle the predicate case where neither is_null or is_not_null is included.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _handle_null_unmentioned(self, term: BoundTerm[Any]) -&gt; None:\n    \"\"\"Handle the predicate case where neither is_null or is_not_null is included.\"\"\"\n    if term not in self.is_null_or_not_bound_terms:\n        self.null_unmentioned_bound_terms.add(term)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._NullNaNUnmentionedTermsCollector.collect","title":"<code>collect(expr)</code>","text":"<p>Collect the bound references categorized by having at least one is_null or is_not_null in the expr and the remaining.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def collect(\n    self,\n    expr: BooleanExpression,\n) -&gt; None:\n    \"\"\"Collect the bound references categorized by having at least one is_null or is_not_null in the expr and the remaining.\"\"\"\n    boolean_expression_visit(expr, self)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._check_pyarrow_schema_compatible","title":"<code>_check_pyarrow_schema_compatible(requested_schema, provided_schema, downcast_ns_timestamp_to_us=False)</code>","text":"<p>Check if the <code>requested_schema</code> is compatible with <code>provided_schema</code>.</p> <p>Two schemas are considered compatible when they are equal in terms of the Iceberg Schema type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the schemas are not compatible.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _check_pyarrow_schema_compatible(\n    requested_schema: Schema, provided_schema: pa.Schema, downcast_ns_timestamp_to_us: bool = False\n) -&gt; None:\n    \"\"\"\n    Check if the `requested_schema` is compatible with `provided_schema`.\n\n    Two schemas are considered compatible when they are equal in terms of the Iceberg Schema type.\n\n    Raises:\n        ValueError: If the schemas are not compatible.\n    \"\"\"\n    name_mapping = requested_schema.name_mapping\n    try:\n        provided_schema = pyarrow_to_schema(\n            provided_schema, name_mapping=name_mapping, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n        )\n    except ValueError as e:\n        provided_schema = _pyarrow_to_schema_without_ids(provided_schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us)\n        additional_names = set(provided_schema._name_to_id.keys()) - set(requested_schema._name_to_id.keys())\n        raise ValueError(\n            f\"PyArrow table contains more columns: {', '.join(sorted(additional_names))}. Update the schema first (hint, use union_by_name).\"\n        ) from e\n    _check_schema_compatible(requested_schema, provided_schema)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._dataframe_to_data_files","title":"<code>_dataframe_to_data_files(table_metadata, df, io, write_uuid=None, counter=None)</code>","text":"<p>Convert a PyArrow table into a DataFile.</p> <p>Returns:</p> Type Description <code>Iterable[DataFile]</code> <p>An iterable that supplies datafiles that represent the table.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _dataframe_to_data_files(\n    table_metadata: TableMetadata,\n    df: pa.Table,\n    io: FileIO,\n    write_uuid: Optional[uuid.UUID] = None,\n    counter: Optional[itertools.count[int]] = None,\n) -&gt; Iterable[DataFile]:\n    \"\"\"Convert a PyArrow table into a DataFile.\n\n    Returns:\n        An iterable that supplies datafiles that represent the table.\n    \"\"\"\n    from pyiceberg.table import DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE, TableProperties, WriteTask\n\n    counter = counter or itertools.count(0)\n    write_uuid = write_uuid or uuid.uuid4()\n    target_file_size: int = property_as_int(  # type: ignore  # The property is set with non-None value.\n        properties=table_metadata.properties,\n        property_name=TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n        default=TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT,\n    )\n    name_mapping = table_metadata.schema().name_mapping\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    task_schema = pyarrow_to_schema(df.schema, name_mapping=name_mapping, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us)\n\n    if table_metadata.spec().is_unpartitioned():\n        yield from write_file(\n            io=io,\n            table_metadata=table_metadata,\n            tasks=iter(\n                [\n                    WriteTask(write_uuid=write_uuid, task_id=next(counter), record_batches=batches, schema=task_schema)\n                    for batches in bin_pack_arrow_table(df, target_file_size)\n                ]\n            ),\n        )\n    else:\n        partitions = _determine_partitions(spec=table_metadata.spec(), schema=table_metadata.schema(), arrow_table=df)\n        yield from write_file(\n            io=io,\n            table_metadata=table_metadata,\n            tasks=iter(\n                [\n                    WriteTask(\n                        write_uuid=write_uuid,\n                        task_id=next(counter),\n                        record_batches=batches,\n                        partition_key=partition.partition_key,\n                        schema=task_schema,\n                    )\n                    for partition in partitions\n                    for batches in bin_pack_arrow_table(partition.arrow_table_partition, target_file_size)\n                ]\n            ),\n        )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._determine_partitions","title":"<code>_determine_partitions(spec, schema, arrow_table)</code>","text":"<p>Based on the iceberg table partition spec, filter the arrow table into partitions with their keys.</p> <p>Example: Input: An arrow table with partition key of ['n_legs', 'year'] and with data of {'year': [2020, 2022, 2022, 2021, 2022, 2022, 2022, 2019, 2021],  'n_legs': [2, 2, 2, 4, 4, 4, 4, 5, 100],  'animal': [\"Flamingo\", \"Parrot\", \"Parrot\", \"Dog\", \"Horse\", \"Horse\", \"Horse\",\"Brittle stars\", \"Centipede\"]}. The algorithm: - We determine the set of unique partition keys - Then we produce a set of partitions by filtering on each of the combinations - We combine the chunks to create a copy to avoid GIL congestion on the original table</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _determine_partitions(spec: PartitionSpec, schema: Schema, arrow_table: pa.Table) -&gt; List[_TablePartition]:\n    \"\"\"Based on the iceberg table partition spec, filter the arrow table into partitions with their keys.\n\n    Example:\n    Input:\n    An arrow table with partition key of ['n_legs', 'year'] and with data of\n    {'year': [2020, 2022, 2022, 2021, 2022, 2022, 2022, 2019, 2021],\n     'n_legs': [2, 2, 2, 4, 4, 4, 4, 5, 100],\n     'animal': [\"Flamingo\", \"Parrot\", \"Parrot\", \"Dog\", \"Horse\", \"Horse\", \"Horse\",\"Brittle stars\", \"Centipede\"]}.\n    The algorithm:\n    - We determine the set of unique partition keys\n    - Then we produce a set of partitions by filtering on each of the combinations\n    - We combine the chunks to create a copy to avoid GIL congestion on the original table\n    \"\"\"\n    # Assign unique names to columns where the partition transform has been applied\n    # to avoid conflicts\n    partition_fields = [f\"_partition_{field.name}\" for field in spec.fields]\n\n    for partition, name in zip(spec.fields, partition_fields):\n        source_field = schema.find_field(partition.source_id)\n        arrow_table = arrow_table.append_column(\n            name, partition.transform.pyarrow_transform(source_field.field_type)(arrow_table[source_field.name])\n        )\n\n    unique_partition_fields = arrow_table.select(partition_fields).group_by(partition_fields).aggregate([])\n\n    table_partitions = []\n    # TODO: As a next step, we could also play around with yielding instead of materializing the full list\n    for unique_partition in unique_partition_fields.to_pylist():\n        partition_key = PartitionKey(\n            field_values=[\n                PartitionFieldValue(field=field, value=unique_partition[name])\n                for field, name in zip(spec.fields, partition_fields)\n            ],\n            partition_spec=spec,\n            schema=schema,\n        )\n        filtered_table = arrow_table.filter(\n            functools.reduce(\n                operator.and_,\n                [\n                    pc.field(partition_field_name) == unique_partition[partition_field_name]\n                    if unique_partition[partition_field_name] is not None\n                    else pc.field(partition_field_name).is_null()\n                    for field, partition_field_name in zip(spec.fields, partition_fields)\n                ],\n            )\n        )\n        filtered_table = filtered_table.drop_columns(partition_fields)\n\n        # The combine_chunks seems to be counter-intuitive to do, but it actually returns\n        # fresh buffers that don't interfere with each other when it is written out to file\n        table_partitions.append(\n            _TablePartition(partition_key=partition_key, arrow_table_partition=filtered_table.combine_chunks())\n        )\n\n    return table_partitions\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._expression_to_complementary_pyarrow","title":"<code>_expression_to_complementary_pyarrow(expr)</code>","text":"<p>Complementary filter conversion function of expression_to_pyarrow.</p> <p>Could not use expression_to_pyarrow(Not(expr)) to achieve this complementary effect because ~ in pyarrow.compute.Expression does not handle null.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _expression_to_complementary_pyarrow(expr: BooleanExpression) -&gt; pc.Expression:\n    \"\"\"Complementary filter conversion function of expression_to_pyarrow.\n\n    Could not use expression_to_pyarrow(Not(expr)) to achieve this complementary effect because ~ in pyarrow.compute.Expression does not handle null.\n    \"\"\"\n    collector = _NullNaNUnmentionedTermsCollector()\n    collector.collect(expr)\n\n    # Convert the set of terms to a sorted list so that layout of the expression to build is deterministic.\n    null_unmentioned_bound_terms: List[BoundTerm[Any]] = sorted(\n        collector.null_unmentioned_bound_terms, key=lambda term: term.ref().field.name\n    )\n    nan_unmentioned_bound_terms: List[BoundTerm[Any]] = sorted(\n        collector.nan_unmentioned_bound_terms, key=lambda term: term.ref().field.name\n    )\n\n    preserve_expr: BooleanExpression = Not(expr)\n    for term in null_unmentioned_bound_terms:\n        preserve_expr = Or(preserve_expr, BoundIsNull(term=term))\n    for term in nan_unmentioned_bound_terms:\n        preserve_expr = Or(preserve_expr, BoundIsNaN(term=term))\n    return expression_to_pyarrow(preserve_expr)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow._get_column_projection_values","title":"<code>_get_column_projection_values(file, projected_schema, partition_spec, file_project_field_ids)</code>","text":"<p>Apply Column Projection rules to File Schema.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def _get_column_projection_values(\n    file: DataFile, projected_schema: Schema, partition_spec: Optional[PartitionSpec], file_project_field_ids: Set[int]\n) -&gt; Tuple[bool, Dict[str, Any]]:\n    \"\"\"Apply Column Projection rules to File Schema.\"\"\"\n    project_schema_diff = projected_schema.field_ids.difference(file_project_field_ids)\n    should_project_columns = len(project_schema_diff) &gt; 0\n    projected_missing_fields: Dict[str, Any] = {}\n\n    if not should_project_columns:\n        return False, {}\n\n    partition_schema: StructType\n    accessors: Dict[int, Accessor]\n\n    if partition_spec is not None:\n        partition_schema = partition_spec.partition_type(projected_schema)\n        accessors = build_position_accessors(partition_schema)\n    else:\n        return False, {}\n\n    for field_id in project_schema_diff:\n        for partition_field in partition_spec.fields_by_source_id(field_id):\n            if isinstance(partition_field.transform, IdentityTransform):\n                accessor = accessors.get(partition_field.field_id)\n\n                if accessor is None:\n                    continue\n\n                # The partition field may not exist in the partition record of the data file.\n                # This can happen when new partition fields are introduced after the file was written.\n                try:\n                    if partition_value := accessor.get(file.partition):\n                        projected_missing_fields[partition_field.name] = partition_value\n                except IndexError:\n                    continue\n\n    return True, projected_missing_fields\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.compute_statistics_plan","title":"<code>compute_statistics_plan(schema, table_properties)</code>","text":"<p>Compute the statistics plan for all columns.</p> <p>The resulting list is assumed to have the same length and same order as the columns in the pyarrow table. This allows the list to map from the column index to the Iceberg column ID. For each element, the desired metrics collection that was provided by the user in the configuration is computed and then adjusted according to the data type of the column. For nested columns the minimum and maximum values are not computed. And truncation is only applied to text of binary strings.</p> <p>Parameters:</p> Name Type Description Default <code>table_properties</code> <code>from pyiceberg.table.metadata.TableMetadata</code> <p>The Iceberg table metadata properties. They are required to compute the mapping of column position to iceberg schema type id. It's also used to set the mode for column metrics collection</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def compute_statistics_plan(\n    schema: Schema,\n    table_properties: Dict[str, str],\n) -&gt; Dict[int, StatisticsCollector]:\n    \"\"\"\n    Compute the statistics plan for all columns.\n\n    The resulting list is assumed to have the same length and same order as the columns in the pyarrow table.\n    This allows the list to map from the column index to the Iceberg column ID.\n    For each element, the desired metrics collection that was provided by the user in the configuration\n    is computed and then adjusted according to the data type of the column. For nested columns the minimum\n    and maximum values are not computed. And truncation is only applied to text of binary strings.\n\n    Args:\n        table_properties (from pyiceberg.table.metadata.TableMetadata): The Iceberg table metadata properties.\n            They are required to compute the mapping of column position to iceberg schema type id. It's also\n            used to set the mode for column metrics collection\n    \"\"\"\n    stats_cols = pre_order_visit(schema, PyArrowStatisticsCollector(schema, table_properties))\n    result: Dict[int, StatisticsCollector] = {}\n    for stats_col in stats_cols:\n        result[stats_col.field_id] = stats_col\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.data_file_statistics_from_parquet_metadata","title":"<code>data_file_statistics_from_parquet_metadata(parquet_metadata, stats_columns, parquet_column_mapping)</code>","text":"<p>Compute and return DataFileStatistics that includes the following.</p> <ul> <li>record_count</li> <li>column_sizes</li> <li>value_counts</li> <li>null_value_counts</li> <li>nan_value_counts</li> <li>column_aggregates</li> <li>split_offsets</li> </ul> <p>Parameters:</p> Name Type Description Default <code>parquet_metadata</code> <code>FileMetaData</code> <p>A pyarrow metadata object.</p> required <code>stats_columns</code> <code>Dict[int, StatisticsCollector]</code> <p>The statistics gathering plan. It is required to set the mode for column metrics collection</p> required <code>parquet_column_mapping</code> <code>Dict[str, int]</code> <p>The mapping of the parquet file name to the field ID</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def data_file_statistics_from_parquet_metadata(\n    parquet_metadata: pq.FileMetaData,\n    stats_columns: Dict[int, StatisticsCollector],\n    parquet_column_mapping: Dict[str, int],\n) -&gt; DataFileStatistics:\n    \"\"\"\n    Compute and return DataFileStatistics that includes the following.\n\n    - record_count\n    - column_sizes\n    - value_counts\n    - null_value_counts\n    - nan_value_counts\n    - column_aggregates\n    - split_offsets\n\n    Args:\n        parquet_metadata (pyarrow.parquet.FileMetaData): A pyarrow metadata object.\n        stats_columns (Dict[int, StatisticsCollector]): The statistics gathering plan. It is required to\n            set the mode for column metrics collection\n        parquet_column_mapping (Dict[str, int]): The mapping of the parquet file name to the field ID\n    \"\"\"\n    column_sizes: Dict[int, int] = {}\n    value_counts: Dict[int, int] = {}\n    split_offsets: List[int] = []\n\n    null_value_counts: Dict[int, int] = {}\n    nan_value_counts: Dict[int, int] = {}\n\n    col_aggs = {}\n\n    invalidate_col: Set[int] = set()\n    for r in range(parquet_metadata.num_row_groups):\n        # References:\n        # https://github.com/apache/iceberg/blob/fc381a81a1fdb8f51a0637ca27cd30673bd7aad3/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L232\n        # https://github.com/apache/parquet-mr/blob/ac29db4611f86a07cc6877b416aa4b183e09b353/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java#L184\n\n        row_group = parquet_metadata.row_group(r)\n\n        data_offset = row_group.column(0).data_page_offset\n        dictionary_offset = row_group.column(0).dictionary_page_offset\n\n        if row_group.column(0).has_dictionary_page and dictionary_offset &lt; data_offset:\n            split_offsets.append(dictionary_offset)\n        else:\n            split_offsets.append(data_offset)\n\n        for pos in range(parquet_metadata.num_columns):\n            column = row_group.column(pos)\n            field_id = parquet_column_mapping[column.path_in_schema]\n\n            stats_col = stats_columns[field_id]\n\n            column_sizes.setdefault(field_id, 0)\n            column_sizes[field_id] += column.total_compressed_size\n\n            if stats_col.mode == MetricsMode(MetricModeTypes.NONE):\n                continue\n\n            value_counts[field_id] = value_counts.get(field_id, 0) + column.num_values\n\n            if column.is_stats_set:\n                try:\n                    statistics = column.statistics\n\n                    if statistics.has_null_count:\n                        null_value_counts[field_id] = null_value_counts.get(field_id, 0) + statistics.null_count\n\n                    if stats_col.mode == MetricsMode(MetricModeTypes.COUNTS):\n                        continue\n\n                    if field_id not in col_aggs:\n                        col_aggs[field_id] = StatsAggregator(\n                            stats_col.iceberg_type, statistics.physical_type, stats_col.mode.length\n                        )\n\n                    col_aggs[field_id].update_min(statistics.min)\n                    col_aggs[field_id].update_max(statistics.max)\n\n                except pyarrow.lib.ArrowNotImplementedError as e:\n                    invalidate_col.add(field_id)\n                    logger.warning(e)\n            else:\n                invalidate_col.add(field_id)\n                logger.warning(\"PyArrow statistics missing for column %d when writing file\", pos)\n\n    split_offsets.sort()\n\n    for field_id in invalidate_col:\n        col_aggs.pop(field_id, None)\n        null_value_counts.pop(field_id, None)\n\n    return DataFileStatistics(\n        record_count=parquet_metadata.num_rows,\n        column_sizes=column_sizes,\n        value_counts=value_counts,\n        null_value_counts=null_value_counts,\n        nan_value_counts=nan_value_counts,\n        column_aggregates=col_aggs,\n        split_offsets=split_offsets,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.parquet_path_to_id_mapping","title":"<code>parquet_path_to_id_mapping(schema)</code>","text":"<p>Compute the mapping of parquet column path to Iceberg ID.</p> <p>For each column, the parquet file metadata has a path_in_schema attribute that follows a specific naming scheme for nested columnds. This function computes a mapping of the full paths to the corresponding Iceberg IDs.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The current table schema.</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def parquet_path_to_id_mapping(\n    schema: Schema,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Compute the mapping of parquet column path to Iceberg ID.\n\n    For each column, the parquet file metadata has a path_in_schema attribute that follows\n    a specific naming scheme for nested columnds. This function computes a mapping of\n    the full paths to the corresponding Iceberg IDs.\n\n    Args:\n        schema (pyiceberg.schema.Schema): The current table schema.\n    \"\"\"\n    result: Dict[str, int] = {}\n    for pair in pre_order_visit(schema, ID2ParquetPathVisitor()):\n        result[pair.parquet_path] = pair.field_id\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.visit_pyarrow","title":"<code>visit_pyarrow(obj, visitor)</code>","text":"<p>Apply a pyarrow schema visitor to any point within a schema.</p> <p>The function traverses the schema in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[DataType, Schema]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>PyArrowSchemaVisitor[T]</code> <p>An instance of an implementation of the generic PyarrowSchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@singledispatch\ndef visit_pyarrow(obj: Union[pa.DataType, pa.Schema], visitor: PyArrowSchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a pyarrow schema visitor to any point within a schema.\n\n    The function traverses the schema in post-order fashion.\n\n    Args:\n        obj (Union[pa.DataType, pa.Schema]): An instance of a Schema or an IcebergType.\n        visitor (PyArrowSchemaVisitor[T]): An instance of an implementation of the generic PyarrowSchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/","title":"table","text":""},{"location":"reference/pyiceberg/table/#pyiceberg.table.AddFileTask","title":"<code>AddFileTask</code>  <code>dataclass</code>","text":"<p>Task with the parameters for adding a Parquet file as a DataFile.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass(frozen=True)\nclass AddFileTask:\n    \"\"\"Task with the parameters for adding a Parquet file as a DataFile.\"\"\"\n\n    file_path: str\n    partition_field_value: Record\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CommitTableRequest","title":"<code>CommitTableRequest</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A pydantic BaseModel for a table commit request.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CommitTableRequest(IcebergBaseModel):\n    \"\"\"A pydantic BaseModel for a table commit request.\"\"\"\n\n    identifier: TableIdentifier = Field()\n    requirements: Tuple[TableRequirement, ...] = Field(default_factory=tuple)\n    updates: Tuple[TableUpdate, ...] = Field(default_factory=tuple)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CommitTableResponse","title":"<code>CommitTableResponse</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A pydantic BaseModel for a table commit response.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CommitTableResponse(IcebergBaseModel):\n    \"\"\"A pydantic BaseModel for a table commit response.\"\"\"\n\n    metadata: TableMetadata\n    metadata_location: str = Field(alias=\"metadata-location\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CreateTableTransaction","title":"<code>CreateTableTransaction</code>","text":"<p>               Bases: <code>Transaction</code></p> <p>A transaction that involves the creation of a a new table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CreateTableTransaction(Transaction):\n    \"\"\"A transaction that involves the creation of a a new table.\"\"\"\n\n    def _initial_changes(self, table_metadata: TableMetadata) -&gt; None:\n        \"\"\"Set the initial changes that can reconstruct the initial table metadata when creating the CreateTableTransaction.\"\"\"\n        self._updates += (\n            AssignUUIDUpdate(uuid=table_metadata.table_uuid),\n            UpgradeFormatVersionUpdate(format_version=table_metadata.format_version),\n        )\n\n        schema: Schema = table_metadata.schema()\n        self._updates += (\n            AddSchemaUpdate(schema_=schema, last_column_id=schema.highest_field_id),\n            SetCurrentSchemaUpdate(schema_id=-1),\n        )\n\n        spec: PartitionSpec = table_metadata.spec()\n        if spec.is_unpartitioned():\n            self._updates += (AddPartitionSpecUpdate(spec=UNPARTITIONED_PARTITION_SPEC),)\n        else:\n            self._updates += (AddPartitionSpecUpdate(spec=spec),)\n        self._updates += (SetDefaultSpecUpdate(spec_id=-1),)\n\n        sort_order: Optional[SortOrder] = table_metadata.sort_order_by_id(table_metadata.default_sort_order_id)\n        if sort_order is None or sort_order.is_unsorted:\n            self._updates += (AddSortOrderUpdate(sort_order=UNSORTED_SORT_ORDER),)\n        else:\n            self._updates += (AddSortOrderUpdate(sort_order=sort_order),)\n        self._updates += (SetDefaultSortOrderUpdate(sort_order_id=-1),)\n\n        self._updates += (\n            SetLocationUpdate(location=table_metadata.location),\n            SetPropertiesUpdate(updates=table_metadata.properties),\n        )\n\n    def __init__(self, table: StagedTable):\n        super().__init__(table, autocommit=False)\n        self._initial_changes(table.metadata)\n\n    def commit_transaction(self) -&gt; Table:\n        \"\"\"Commit the changes to the catalog.\n\n        In the case of a CreateTableTransaction, the only requirement is AssertCreate.\n        Returns:\n            The table with the updates applied.\n        \"\"\"\n        self._requirements = (AssertCreate(),)\n        self._table._do_commit(  # pylint: disable=W0212\n            updates=self._updates,\n            requirements=self._requirements,\n        )\n        return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CreateTableTransaction._initial_changes","title":"<code>_initial_changes(table_metadata)</code>","text":"<p>Set the initial changes that can reconstruct the initial table metadata when creating the CreateTableTransaction.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _initial_changes(self, table_metadata: TableMetadata) -&gt; None:\n    \"\"\"Set the initial changes that can reconstruct the initial table metadata when creating the CreateTableTransaction.\"\"\"\n    self._updates += (\n        AssignUUIDUpdate(uuid=table_metadata.table_uuid),\n        UpgradeFormatVersionUpdate(format_version=table_metadata.format_version),\n    )\n\n    schema: Schema = table_metadata.schema()\n    self._updates += (\n        AddSchemaUpdate(schema_=schema, last_column_id=schema.highest_field_id),\n        SetCurrentSchemaUpdate(schema_id=-1),\n    )\n\n    spec: PartitionSpec = table_metadata.spec()\n    if spec.is_unpartitioned():\n        self._updates += (AddPartitionSpecUpdate(spec=UNPARTITIONED_PARTITION_SPEC),)\n    else:\n        self._updates += (AddPartitionSpecUpdate(spec=spec),)\n    self._updates += (SetDefaultSpecUpdate(spec_id=-1),)\n\n    sort_order: Optional[SortOrder] = table_metadata.sort_order_by_id(table_metadata.default_sort_order_id)\n    if sort_order is None or sort_order.is_unsorted:\n        self._updates += (AddSortOrderUpdate(sort_order=UNSORTED_SORT_ORDER),)\n    else:\n        self._updates += (AddSortOrderUpdate(sort_order=sort_order),)\n    self._updates += (SetDefaultSortOrderUpdate(sort_order_id=-1),)\n\n    self._updates += (\n        SetLocationUpdate(location=table_metadata.location),\n        SetPropertiesUpdate(updates=table_metadata.properties),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CreateTableTransaction.commit_transaction","title":"<code>commit_transaction()</code>","text":"<p>Commit the changes to the catalog.</p> <p>In the case of a CreateTableTransaction, the only requirement is AssertCreate. Returns:     The table with the updates applied.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def commit_transaction(self) -&gt; Table:\n    \"\"\"Commit the changes to the catalog.\n\n    In the case of a CreateTableTransaction, the only requirement is AssertCreate.\n    Returns:\n        The table with the updates applied.\n    \"\"\"\n    self._requirements = (AssertCreate(),)\n    self._table._do_commit(  # pylint: disable=W0212\n        updates=self._updates,\n        requirements=self._requirements,\n    )\n    return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan","title":"<code>DataScan</code>","text":"<p>               Bases: <code>TableScan</code></p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class DataScan(TableScan):\n    def _build_partition_projection(self, spec_id: int) -&gt; BooleanExpression:\n        project = inclusive_projection(self.table_metadata.schema(), self.table_metadata.specs()[spec_id], self.case_sensitive)\n        return project(self.row_filter)\n\n    @cached_property\n    def partition_filters(self) -&gt; KeyDefaultDict[int, BooleanExpression]:\n        return KeyDefaultDict(self._build_partition_projection)\n\n    def _build_manifest_evaluator(self, spec_id: int) -&gt; Callable[[ManifestFile], bool]:\n        spec = self.table_metadata.specs()[spec_id]\n        return manifest_evaluator(spec, self.table_metadata.schema(), self.partition_filters[spec_id], self.case_sensitive)\n\n    def _build_partition_evaluator(self, spec_id: int) -&gt; Callable[[DataFile], bool]:\n        spec = self.table_metadata.specs()[spec_id]\n        partition_type = spec.partition_type(self.table_metadata.schema())\n        partition_schema = Schema(*partition_type.fields)\n        partition_expr = self.partition_filters[spec_id]\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _EvaluatorExpression methods bound to a single\n        # shared instance across multiple threads.\n        return lambda data_file: expression_evaluator(partition_schema, partition_expr, self.case_sensitive)(data_file.partition)\n\n    def _build_metrics_evaluator(self) -&gt; Callable[[DataFile], bool]:\n        schema = self.table_metadata.schema()\n        include_empty_files = strtobool(self.options.get(\"include_empty_files\", \"false\"))\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _InclusiveMetricsEvaluator methods bound to a single\n        # shared instance across multiple threads.\n        return lambda data_file: _InclusiveMetricsEvaluator(\n            schema,\n            self.row_filter,\n            self.case_sensitive,\n            include_empty_files,\n        ).eval(data_file)\n\n    def _build_residual_evaluator(self, spec_id: int) -&gt; Callable[[DataFile], ResidualEvaluator]:\n        spec = self.table_metadata.specs()[spec_id]\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _EvaluatorExpression methods bound to a single\n        # shared instance across multiple threads.\n        # return lambda data_file: (partition_schema, partition_expr, self.case_sensitive)(data_file.partition)\n        from pyiceberg.expressions.visitors import residual_evaluator_of\n\n        # assert self.row_filter == False\n        return lambda datafile: (\n            residual_evaluator_of(\n                spec=spec,\n                expr=self.row_filter,\n                case_sensitive=self.case_sensitive,\n                schema=self.table_metadata.schema(),\n            )\n        )\n\n    def _check_sequence_number(self, min_sequence_number: int, manifest: ManifestFile) -&gt; bool:\n        \"\"\"Ensure that no manifests are loaded that contain deletes that are older than the data.\n\n        Args:\n            min_sequence_number (int): The minimal sequence number.\n            manifest (ManifestFile): A ManifestFile that can be either data or deletes.\n\n        Returns:\n            Boolean indicating if it is either a data file, or a relevant delete file.\n        \"\"\"\n        return manifest.content == ManifestContent.DATA or (\n            # Not interested in deletes that are older than the data\n            manifest.content == ManifestContent.DELETES\n            and (manifest.sequence_number or INITIAL_SEQUENCE_NUMBER) &gt;= min_sequence_number\n        )\n\n    def plan_files(self) -&gt; Iterable[FileScanTask]:\n        \"\"\"Plans the relevant files by filtering on the PartitionSpecs.\n\n        Returns:\n            List of FileScanTasks that contain both data and delete files.\n        \"\"\"\n        snapshot = self.snapshot()\n        if not snapshot:\n            return iter([])\n\n        # step 1: filter manifests using partition summaries\n        # the filter depends on the partition spec used to write the manifest file, so create a cache of filters for each spec id\n\n        manifest_evaluators: Dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)\n\n        residual_evaluators: Dict[int, Callable[[DataFile], ResidualEvaluator]] = KeyDefaultDict(self._build_residual_evaluator)\n\n        manifests = [\n            manifest_file\n            for manifest_file in snapshot.manifests(self.io)\n            if manifest_evaluators[manifest_file.partition_spec_id](manifest_file)\n        ]\n\n        # step 2: filter the data files in each manifest\n        # this filter depends on the partition spec used to write the manifest file\n\n        partition_evaluators: Dict[int, Callable[[DataFile], bool]] = KeyDefaultDict(self._build_partition_evaluator)\n\n        min_sequence_number = _min_sequence_number(manifests)\n\n        data_entries: List[ManifestEntry] = []\n        positional_delete_entries = SortedList(key=lambda entry: entry.sequence_number or INITIAL_SEQUENCE_NUMBER)\n\n        executor = ExecutorFactory.get_or_create()\n        for manifest_entry in chain(\n            *executor.map(\n                lambda args: _open_manifest(*args),\n                [\n                    (\n                        self.io,\n                        manifest,\n                        partition_evaluators[manifest.partition_spec_id],\n                        residual_evaluators[manifest.partition_spec_id],\n                        self._build_metrics_evaluator(),\n                    )\n                    for manifest in manifests\n                    if self._check_sequence_number(min_sequence_number, manifest)\n                ],\n            )\n        ):\n            data_file = manifest_entry.data_file\n            if data_file.content == DataFileContent.DATA:\n                data_entries.append(manifest_entry)\n            elif data_file.content == DataFileContent.POSITION_DELETES:\n                positional_delete_entries.add(manifest_entry)\n            elif data_file.content == DataFileContent.EQUALITY_DELETES:\n                raise ValueError(\"PyIceberg does not yet support equality deletes: https://github.com/apache/iceberg/issues/6568\")\n            else:\n                raise ValueError(f\"Unknown DataFileContent ({data_file.content}): {manifest_entry}\")\n\n        return [\n            FileScanTask(\n                data_entry.data_file,\n                delete_files=_match_deletes_to_data_file(\n                    data_entry,\n                    positional_delete_entries,\n                ),\n                residual=residual_evaluators[data_entry.data_file.spec_id](data_entry.data_file).residual_for(\n                    data_entry.data_file.partition\n                ),\n            )\n            for data_entry in data_entries\n        ]\n\n    def to_arrow(self) -&gt; pa.Table:\n        \"\"\"Read an Arrow table eagerly from this DataScan.\n\n        All rows will be loaded into memory at once.\n\n        Returns:\n            pa.Table: Materialized Arrow Table from the Iceberg table's DataScan\n        \"\"\"\n        from pyiceberg.io.pyarrow import ArrowScan\n\n        return ArrowScan(\n            self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n        ).to_table(self.plan_files())\n\n    def to_arrow_batch_reader(self) -&gt; pa.RecordBatchReader:\n        \"\"\"Return an Arrow RecordBatchReader from this DataScan.\n\n        For large results, using a RecordBatchReader requires less memory than\n        loading an Arrow Table for the same DataScan, because a RecordBatch\n        is read one at a time.\n\n        Returns:\n            pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan\n                which can be used to read a stream of record batches one by one.\n        \"\"\"\n        import pyarrow as pa\n\n        from pyiceberg.io.pyarrow import ArrowScan, schema_to_pyarrow\n\n        target_schema = schema_to_pyarrow(self.projection())\n        batches = ArrowScan(\n            self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n        ).to_record_batches(self.plan_files())\n\n        return pa.RecordBatchReader.from_batches(\n            target_schema,\n            batches,\n        )\n\n    def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Read a Pandas DataFrame eagerly from this Iceberg table.\n\n        Returns:\n            pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table\n        \"\"\"\n        return self.to_arrow().to_pandas(**kwargs)\n\n    def to_duckdb(self, table_name: str, connection: Optional[DuckDBPyConnection] = None) -&gt; DuckDBPyConnection:\n        \"\"\"Shorthand for loading the Iceberg Table in DuckDB.\n\n        Returns:\n            DuckDBPyConnection: In memory DuckDB connection with the Iceberg table.\n        \"\"\"\n        import duckdb\n\n        con = connection or duckdb.connect(database=\":memory:\")\n        con.register(table_name, self.to_arrow())\n\n        return con\n\n    def to_ray(self) -&gt; ray.data.dataset.Dataset:\n        \"\"\"Read a Ray Dataset eagerly from this Iceberg table.\n\n        Returns:\n            ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table\n        \"\"\"\n        import ray\n\n        return ray.data.from_arrow(self.to_arrow())\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Read a Polars DataFrame from this Iceberg table.\n\n        Returns:\n            pl.DataFrame: Materialized Polars Dataframe from the Iceberg table\n        \"\"\"\n        import polars as pl\n\n        result = pl.from_arrow(self.to_arrow())\n        if isinstance(result, pl.Series):\n            result = result.to_frame()\n\n        return result\n\n    def count(self) -&gt; int:\n        from pyiceberg.io.pyarrow import ArrowScan\n\n        # Usage: Calculates the total number of records in a Scan that haven't had positional deletes.\n        res = 0\n        # every task is a FileScanTask\n        tasks = self.plan_files()\n\n        for task in tasks:\n            # task.residual is a Boolean Expression if the filter condition is fully satisfied by the\n            # partition value and task.delete_files represents that positional delete haven't been merged yet\n            # hence those files have to read as a pyarrow table applying the filter and deletes\n            if task.residual == AlwaysTrue() and len(task.delete_files) == 0:\n                # Every File has a metadata stat that stores the file record count\n                res += task.file.record_count\n            else:\n                arrow_scan = ArrowScan(\n                    table_metadata=self.table_metadata,\n                    io=self.io,\n                    projected_schema=self.projection(),\n                    row_filter=self.row_filter,\n                    case_sensitive=self.case_sensitive,\n                )\n                tbl = arrow_scan.to_table([task])\n                res += len(tbl)\n        return res\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan._check_sequence_number","title":"<code>_check_sequence_number(min_sequence_number, manifest)</code>","text":"<p>Ensure that no manifests are loaded that contain deletes that are older than the data.</p> <p>Parameters:</p> Name Type Description Default <code>min_sequence_number</code> <code>int</code> <p>The minimal sequence number.</p> required <code>manifest</code> <code>ManifestFile</code> <p>A ManifestFile that can be either data or deletes.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Boolean indicating if it is either a data file, or a relevant delete file.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _check_sequence_number(self, min_sequence_number: int, manifest: ManifestFile) -&gt; bool:\n    \"\"\"Ensure that no manifests are loaded that contain deletes that are older than the data.\n\n    Args:\n        min_sequence_number (int): The minimal sequence number.\n        manifest (ManifestFile): A ManifestFile that can be either data or deletes.\n\n    Returns:\n        Boolean indicating if it is either a data file, or a relevant delete file.\n    \"\"\"\n    return manifest.content == ManifestContent.DATA or (\n        # Not interested in deletes that are older than the data\n        manifest.content == ManifestContent.DELETES\n        and (manifest.sequence_number or INITIAL_SEQUENCE_NUMBER) &gt;= min_sequence_number\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.plan_files","title":"<code>plan_files()</code>","text":"<p>Plans the relevant files by filtering on the PartitionSpecs.</p> <p>Returns:</p> Type Description <code>Iterable[FileScanTask]</code> <p>List of FileScanTasks that contain both data and delete files.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def plan_files(self) -&gt; Iterable[FileScanTask]:\n    \"\"\"Plans the relevant files by filtering on the PartitionSpecs.\n\n    Returns:\n        List of FileScanTasks that contain both data and delete files.\n    \"\"\"\n    snapshot = self.snapshot()\n    if not snapshot:\n        return iter([])\n\n    # step 1: filter manifests using partition summaries\n    # the filter depends on the partition spec used to write the manifest file, so create a cache of filters for each spec id\n\n    manifest_evaluators: Dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)\n\n    residual_evaluators: Dict[int, Callable[[DataFile], ResidualEvaluator]] = KeyDefaultDict(self._build_residual_evaluator)\n\n    manifests = [\n        manifest_file\n        for manifest_file in snapshot.manifests(self.io)\n        if manifest_evaluators[manifest_file.partition_spec_id](manifest_file)\n    ]\n\n    # step 2: filter the data files in each manifest\n    # this filter depends on the partition spec used to write the manifest file\n\n    partition_evaluators: Dict[int, Callable[[DataFile], bool]] = KeyDefaultDict(self._build_partition_evaluator)\n\n    min_sequence_number = _min_sequence_number(manifests)\n\n    data_entries: List[ManifestEntry] = []\n    positional_delete_entries = SortedList(key=lambda entry: entry.sequence_number or INITIAL_SEQUENCE_NUMBER)\n\n    executor = ExecutorFactory.get_or_create()\n    for manifest_entry in chain(\n        *executor.map(\n            lambda args: _open_manifest(*args),\n            [\n                (\n                    self.io,\n                    manifest,\n                    partition_evaluators[manifest.partition_spec_id],\n                    residual_evaluators[manifest.partition_spec_id],\n                    self._build_metrics_evaluator(),\n                )\n                for manifest in manifests\n                if self._check_sequence_number(min_sequence_number, manifest)\n            ],\n        )\n    ):\n        data_file = manifest_entry.data_file\n        if data_file.content == DataFileContent.DATA:\n            data_entries.append(manifest_entry)\n        elif data_file.content == DataFileContent.POSITION_DELETES:\n            positional_delete_entries.add(manifest_entry)\n        elif data_file.content == DataFileContent.EQUALITY_DELETES:\n            raise ValueError(\"PyIceberg does not yet support equality deletes: https://github.com/apache/iceberg/issues/6568\")\n        else:\n            raise ValueError(f\"Unknown DataFileContent ({data_file.content}): {manifest_entry}\")\n\n    return [\n        FileScanTask(\n            data_entry.data_file,\n            delete_files=_match_deletes_to_data_file(\n                data_entry,\n                positional_delete_entries,\n            ),\n            residual=residual_evaluators[data_entry.data_file.spec_id](data_entry.data_file).residual_for(\n                data_entry.data_file.partition\n            ),\n        )\n        for data_entry in data_entries\n    ]\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Read an Arrow table eagerly from this DataScan.</p> <p>All rows will be loaded into memory at once.</p> <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: Materialized Arrow Table from the Iceberg table's DataScan</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Read an Arrow table eagerly from this DataScan.\n\n    All rows will be loaded into memory at once.\n\n    Returns:\n        pa.Table: Materialized Arrow Table from the Iceberg table's DataScan\n    \"\"\"\n    from pyiceberg.io.pyarrow import ArrowScan\n\n    return ArrowScan(\n        self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n    ).to_table(self.plan_files())\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_arrow_batch_reader","title":"<code>to_arrow_batch_reader()</code>","text":"<p>Return an Arrow RecordBatchReader from this DataScan.</p> <p>For large results, using a RecordBatchReader requires less memory than loading an Arrow Table for the same DataScan, because a RecordBatch is read one at a time.</p> <p>Returns:</p> Type Description <code>RecordBatchReader</code> <p>pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan which can be used to read a stream of record batches one by one.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_arrow_batch_reader(self) -&gt; pa.RecordBatchReader:\n    \"\"\"Return an Arrow RecordBatchReader from this DataScan.\n\n    For large results, using a RecordBatchReader requires less memory than\n    loading an Arrow Table for the same DataScan, because a RecordBatch\n    is read one at a time.\n\n    Returns:\n        pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan\n            which can be used to read a stream of record batches one by one.\n    \"\"\"\n    import pyarrow as pa\n\n    from pyiceberg.io.pyarrow import ArrowScan, schema_to_pyarrow\n\n    target_schema = schema_to_pyarrow(self.projection())\n    batches = ArrowScan(\n        self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n    ).to_record_batches(self.plan_files())\n\n    return pa.RecordBatchReader.from_batches(\n        target_schema,\n        batches,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_duckdb","title":"<code>to_duckdb(table_name, connection=None)</code>","text":"<p>Shorthand for loading the Iceberg Table in DuckDB.</p> <p>Returns:</p> Name Type Description <code>DuckDBPyConnection</code> <code>DuckDBPyConnection</code> <p>In memory DuckDB connection with the Iceberg table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_duckdb(self, table_name: str, connection: Optional[DuckDBPyConnection] = None) -&gt; DuckDBPyConnection:\n    \"\"\"Shorthand for loading the Iceberg Table in DuckDB.\n\n    Returns:\n        DuckDBPyConnection: In memory DuckDB connection with the Iceberg table.\n    \"\"\"\n    import duckdb\n\n    con = connection or duckdb.connect(database=\":memory:\")\n    con.register(table_name, self.to_arrow())\n\n    return con\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_pandas","title":"<code>to_pandas(**kwargs)</code>","text":"<p>Read a Pandas DataFrame eagerly from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Read a Pandas DataFrame eagerly from this Iceberg table.\n\n    Returns:\n        pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table\n    \"\"\"\n    return self.to_arrow().to_pandas(**kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_polars","title":"<code>to_polars()</code>","text":"<p>Read a Polars DataFrame from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Materialized Polars Dataframe from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Read a Polars DataFrame from this Iceberg table.\n\n    Returns:\n        pl.DataFrame: Materialized Polars Dataframe from the Iceberg table\n    \"\"\"\n    import polars as pl\n\n    result = pl.from_arrow(self.to_arrow())\n    if isinstance(result, pl.Series):\n        result = result.to_frame()\n\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_ray","title":"<code>to_ray()</code>","text":"<p>Read a Ray Dataset eagerly from this Iceberg table.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_ray(self) -&gt; ray.data.dataset.Dataset:\n    \"\"\"Read a Ray Dataset eagerly from this Iceberg table.\n\n    Returns:\n        ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table\n    \"\"\"\n    import ray\n\n    return ray.data.from_arrow(self.to_arrow())\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.FileScanTask","title":"<code>FileScanTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ScanTask</code></p> <p>Task representing a data file and its corresponding delete files.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass(init=False)\nclass FileScanTask(ScanTask):\n    \"\"\"Task representing a data file and its corresponding delete files.\"\"\"\n\n    file: DataFile\n    delete_files: Set[DataFile]\n    start: int\n    length: int\n    residual: BooleanExpression\n\n    def __init__(\n        self,\n        data_file: DataFile,\n        delete_files: Optional[Set[DataFile]] = None,\n        start: Optional[int] = None,\n        length: Optional[int] = None,\n        residual: BooleanExpression = ALWAYS_TRUE,\n    ) -&gt; None:\n        self.file = data_file\n        self.delete_files = delete_files or set()\n        self.start = start or 0\n        self.length = length or data_file.file_size_in_bytes\n        self.residual = residual\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Namespace","title":"<code>Namespace</code>","text":"<p>               Bases: <code>IcebergRootModel[List[str]]</code></p> <p>Reference to one or more levels of a namespace.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Namespace(IcebergRootModel[List[str]]):\n    \"\"\"Reference to one or more levels of a namespace.\"\"\"\n\n    root: List[str] = Field(\n        ...,\n        description=\"Reference to one or more levels of a namespace\",\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.StaticTable","title":"<code>StaticTable</code>","text":"<p>               Bases: <code>Table</code></p> <p>Load a table directly from a metadata file (i.e., without using a catalog).</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class StaticTable(Table):\n    \"\"\"Load a table directly from a metadata file (i.e., without using a catalog).\"\"\"\n\n    def refresh(self) -&gt; Table:\n        \"\"\"Refresh the current table metadata.\"\"\"\n        raise NotImplementedError(\"To be implemented\")\n\n    @classmethod\n    def from_metadata(cls, metadata_location: str, properties: Properties = EMPTY_DICT) -&gt; StaticTable:\n        io = load_file_io(properties=properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n\n        from pyiceberg.serializers import FromInputFile\n\n        metadata = FromInputFile.table_metadata(file)\n\n        from pyiceberg.catalog.noop import NoopCatalog\n\n        return cls(\n            identifier=(\"static-table\", metadata_location),\n            metadata_location=metadata_location,\n            metadata=metadata,\n            io=load_file_io({**properties, **metadata.properties}),\n            catalog=NoopCatalog(\"static-table\"),\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.StaticTable.refresh","title":"<code>refresh()</code>","text":"<p>Refresh the current table metadata.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refresh(self) -&gt; Table:\n    \"\"\"Refresh the current table metadata.\"\"\"\n    raise NotImplementedError(\"To be implemented\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table","title":"<code>Table</code>","text":"<p>An Iceberg table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Table:\n    \"\"\"An Iceberg table.\"\"\"\n\n    _identifier: Identifier = Field()\n    metadata: TableMetadata\n    metadata_location: str = Field()\n    io: FileIO\n    catalog: Catalog\n    config: Dict[str, str]\n\n    def __init__(\n        self,\n        identifier: Identifier,\n        metadata: TableMetadata,\n        metadata_location: str,\n        io: FileIO,\n        catalog: Catalog,\n        config: Dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        self._identifier = identifier\n        self.metadata = metadata\n        self.metadata_location = metadata_location\n        self.io = io\n        self.catalog = catalog\n        self.config = config\n\n    def transaction(self) -&gt; Transaction:\n        \"\"\"Create a new transaction object to first stage the changes, and then commit them to the catalog.\n\n        Returns:\n            The transaction object\n        \"\"\"\n        return Transaction(self)\n\n    @property\n    def inspect(self) -&gt; InspectTable:\n        \"\"\"Return the InspectTable object to browse the table metadata.\n\n        Returns:\n            InspectTable object based on this Table.\n        \"\"\"\n        return InspectTable(self)\n\n    def refresh(self) -&gt; Table:\n        \"\"\"Refresh the current table metadata.\n\n        Returns:\n            An updated instance of the same Iceberg table\n        \"\"\"\n        fresh = self.catalog.load_table(self._identifier)\n        self.metadata = fresh.metadata\n        self.io = fresh.io\n        self.metadata_location = fresh.metadata_location\n        return self\n\n    def name(self) -&gt; Identifier:\n        \"\"\"Return the identifier of this table.\n\n        Returns:\n            An Identifier tuple of the table name\n        \"\"\"\n        return self._identifier\n\n    def scan(\n        self,\n        row_filter: Union[str, BooleanExpression] = ALWAYS_TRUE,\n        selected_fields: Tuple[str, ...] = (\"*\",),\n        case_sensitive: bool = True,\n        snapshot_id: Optional[int] = None,\n        options: Properties = EMPTY_DICT,\n        limit: Optional[int] = None,\n    ) -&gt; DataScan:\n        \"\"\"Fetch a DataScan based on the table's current metadata.\n\n            The data scan can be used to project the table's data\n            that matches the provided row_filter onto the table's\n            current schema.\n\n        Args:\n            row_filter:\n                A string or BooleanExpression that describes the\n                desired rows\n            selected_fields:\n                A tuple of strings representing the column names\n                to return in the output dataframe.\n            case_sensitive:\n                If True column matching is case sensitive\n            snapshot_id:\n                Optional Snapshot ID to time travel to. If None,\n                scans the table as of the current snapshot ID.\n            options:\n                Additional Table properties as a dictionary of\n                string key value pairs to use for this scan.\n            limit:\n                An integer representing the number of rows to\n                return in the scan result. If None, fetches all\n                matching rows.\n\n        Returns:\n            A DataScan based on the table's current metadata.\n        \"\"\"\n        return DataScan(\n            table_metadata=self.metadata,\n            io=self.io,\n            row_filter=row_filter,\n            selected_fields=selected_fields,\n            case_sensitive=case_sensitive,\n            snapshot_id=snapshot_id,\n            options=options,\n            limit=limit,\n        )\n\n    @property\n    def format_version(self) -&gt; TableVersion:\n        return self.metadata.format_version\n\n    def schema(self) -&gt; Schema:\n        \"\"\"Return the schema for this table.\"\"\"\n        return next(schema for schema in self.metadata.schemas if schema.schema_id == self.metadata.current_schema_id)\n\n    def schemas(self) -&gt; Dict[int, Schema]:\n        \"\"\"Return a dict of the schema of this table.\"\"\"\n        return {schema.schema_id: schema for schema in self.metadata.schemas}\n\n    def spec(self) -&gt; PartitionSpec:\n        \"\"\"Return the partition spec of this table.\"\"\"\n        return next(spec for spec in self.metadata.partition_specs if spec.spec_id == self.metadata.default_spec_id)\n\n    def specs(self) -&gt; Dict[int, PartitionSpec]:\n        \"\"\"Return a dict the partition specs this table.\"\"\"\n        return {spec.spec_id: spec for spec in self.metadata.partition_specs}\n\n    def sort_order(self) -&gt; SortOrder:\n        \"\"\"Return the sort order of this table.\"\"\"\n        return next(\n            sort_order for sort_order in self.metadata.sort_orders if sort_order.order_id == self.metadata.default_sort_order_id\n        )\n\n    def sort_orders(self) -&gt; Dict[int, SortOrder]:\n        \"\"\"Return a dict of the sort orders of this table.\"\"\"\n        return {sort_order.order_id: sort_order for sort_order in self.metadata.sort_orders}\n\n    def last_partition_id(self) -&gt; int:\n        \"\"\"Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.\"\"\"\n        if self.metadata.last_partition_id:\n            return self.metadata.last_partition_id\n        return PARTITION_FIELD_ID_START - 1\n\n    @property\n    def properties(self) -&gt; Dict[str, str]:\n        \"\"\"Properties of the table.\"\"\"\n        return self.metadata.properties\n\n    def location(self) -&gt; str:\n        \"\"\"Return the table's base location.\"\"\"\n        return self.metadata.location\n\n    def location_provider(self) -&gt; LocationProvider:\n        \"\"\"Return the table's location provider.\"\"\"\n        return load_location_provider(table_location=self.metadata.location, table_properties=self.metadata.properties)\n\n    @property\n    def last_sequence_number(self) -&gt; int:\n        return self.metadata.last_sequence_number\n\n    def current_snapshot(self) -&gt; Optional[Snapshot]:\n        \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n        if self.metadata.current_snapshot_id is not None:\n            return self.snapshot_by_id(self.metadata.current_snapshot_id)\n        return None\n\n    def snapshots(self) -&gt; List[Snapshot]:\n        return self.metadata.snapshots\n\n    def snapshot_by_id(self, snapshot_id: int) -&gt; Optional[Snapshot]:\n        \"\"\"Get the snapshot of this table with the given id, or None if there is no matching snapshot.\"\"\"\n        return self.metadata.snapshot_by_id(snapshot_id)\n\n    def snapshot_by_name(self, name: str) -&gt; Optional[Snapshot]:\n        \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n        if ref := self.metadata.refs.get(name):\n            return self.snapshot_by_id(ref.snapshot_id)\n        return None\n\n    def snapshot_as_of_timestamp(self, timestamp_ms: int, inclusive: bool = True) -&gt; Optional[Snapshot]:\n        \"\"\"Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.\n\n        Args:\n            timestamp_ms: Find snapshot that was current at/before this timestamp\n            inclusive: Includes timestamp_ms in search when True. Excludes timestamp_ms when False\n        \"\"\"\n        for log_entry in reversed(self.history()):\n            if (inclusive and log_entry.timestamp_ms &lt;= timestamp_ms) or log_entry.timestamp_ms &lt; timestamp_ms:\n                return self.snapshot_by_id(log_entry.snapshot_id)\n        return None\n\n    def history(self) -&gt; List[SnapshotLogEntry]:\n        \"\"\"Get the snapshot history of this table.\"\"\"\n        return self.metadata.snapshot_log\n\n    def manage_snapshots(self) -&gt; ManageSnapshots:\n        \"\"\"\n        Shorthand to run snapshot management operations like create branch, create tag, etc.\n\n        Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n        Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n        Pending changes are applied on commit.\n\n        We can also use context managers to make more changes. For example,\n\n        with table.manage_snapshots() as ms:\n           ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n        \"\"\"\n        return ManageSnapshots(transaction=Transaction(self, autocommit=True))\n\n    def update_statistics(self) -&gt; UpdateStatistics:\n        \"\"\"\n        Shorthand to run statistics management operations like add statistics and remove statistics.\n\n        Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n        Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n        Pending changes are applied on commit.\n\n        We can also use context managers to make more changes. For example:\n\n        with table.update_statistics() as update:\n            update.set_statistics(statistics_file=statistics_file)\n            update.remove_statistics(snapshot_id=2)\n        \"\"\"\n        return UpdateStatistics(transaction=Transaction(self, autocommit=True))\n\n    def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n        \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n        Args:\n            allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n            case_sensitive: If field names are case-sensitive.\n\n        Returns:\n            A new UpdateSchema.\n        \"\"\"\n        return UpdateSchema(\n            transaction=Transaction(self, autocommit=True),\n            allow_incompatible_changes=allow_incompatible_changes,\n            case_sensitive=case_sensitive,\n            name_mapping=self.name_mapping(),\n        )\n\n    def name_mapping(self) -&gt; Optional[NameMapping]:\n        \"\"\"Return the table's field-id NameMapping.\"\"\"\n        return self.metadata.name_mapping()\n\n    def upsert(\n        self,\n        df: pa.Table,\n        join_cols: Optional[List[str]] = None,\n        when_matched_update_all: bool = True,\n        when_not_matched_insert_all: bool = True,\n        case_sensitive: bool = True,\n    ) -&gt; UpsertResult:\n        \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n        Args:\n\n            df: The input dataframe to upsert with the table's data.\n            join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n            when_matched_update_all: Bool indicating to update rows that are matched but require an update due to a value in a non-key column changing\n            when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any existing rows in the table\n            case_sensitive: Bool indicating if the match should be case-sensitive\n\n            To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n                Example Use Cases:\n                    Case 1: Both Parameters = True (Full Upsert)\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Insert it\n\n                    Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                    Existing row found \u2192 Do nothing (no updates)\n                    New row found \u2192 Insert it\n\n                    Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Do nothing (no inserts)\n\n                    Case 4: Both Parameters = False (No Merge Effect)\n                    Existing row found \u2192 Do nothing\n                    New row found \u2192 Do nothing\n                    (Function effectively does nothing)\n\n\n        Returns:\n            An UpsertResult class (contains details of rows updated and inserted)\n        \"\"\"\n        try:\n            import pyarrow as pa  # noqa: F401\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import expression_to_pyarrow\n        from pyiceberg.table import upsert_util\n\n        if join_cols is None:\n            join_cols = []\n            for field_id in self.schema().identifier_field_ids:\n                col = self.schema().find_column_name(field_id)\n                if col is not None:\n                    join_cols.append(col)\n                else:\n                    raise ValueError(f\"Field-ID could not be found: {join_cols}\")\n\n        if len(join_cols) == 0:\n            raise ValueError(\"Join columns could not be found, please set identifier-field-ids or pass in explicitly.\")\n\n        if not when_matched_update_all and not when_not_matched_insert_all:\n            raise ValueError(\"no upsert options selected...exiting\")\n\n        if upsert_util.has_duplicate_rows(df, join_cols):\n            raise ValueError(\"Duplicate rows found in source dataset based on the key columns. No upsert executed\")\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n        )\n\n        # get list of rows that exist so we don't have to load the entire target table\n        matched_predicate = upsert_util.create_match_filter(df, join_cols)\n        matched_iceberg_table = self.scan(row_filter=matched_predicate, case_sensitive=case_sensitive).to_arrow()\n\n        update_row_cnt = 0\n        insert_row_cnt = 0\n\n        with self.transaction() as tx:\n            if when_matched_update_all:\n                # function get_rows_to_update is doing a check on non-key columns to see if any of the values have actually changed\n                # we don't want to do just a blanket overwrite for matched rows if the actual non-key column data hasn't changed\n                # this extra step avoids unnecessary IO and writes\n                rows_to_update = upsert_util.get_rows_to_update(df, matched_iceberg_table, join_cols)\n\n                update_row_cnt = len(rows_to_update)\n\n                # build the match predicate filter\n                overwrite_mask_predicate = upsert_util.create_match_filter(rows_to_update, join_cols)\n\n                tx.overwrite(rows_to_update, overwrite_filter=overwrite_mask_predicate)\n\n            if when_not_matched_insert_all:\n                expr_match = upsert_util.create_match_filter(matched_iceberg_table, join_cols)\n                expr_match_bound = bind(self.schema(), expr_match, case_sensitive=case_sensitive)\n                expr_match_arrow = expression_to_pyarrow(expr_match_bound)\n                rows_to_insert = df.filter(~expr_match_arrow)\n\n                insert_row_cnt = len(rows_to_insert)\n\n                tx.append(rows_to_insert)\n\n        return UpsertResult(rows_updated=update_row_cnt, rows_inserted=insert_row_cnt)\n\n    def append(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n        \"\"\"\n        Shorthand API for appending a PyArrow table to the table.\n\n        Args:\n            df: The Arrow dataframe that will be appended to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        \"\"\"\n        with self.transaction() as tx:\n            tx.append(df=df, snapshot_properties=snapshot_properties)\n\n    def dynamic_partition_overwrite(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n        \"\"\"Shorthand for dynamic overwriting the table with a PyArrow table.\n\n        Old partitions are auto detected and replaced with data files created for input arrow table.\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        \"\"\"\n        with self.transaction() as tx:\n            tx.dynamic_partition_overwrite(df=df, snapshot_properties=snapshot_properties)\n\n    def overwrite(\n        self,\n        df: pa.Table,\n        overwrite_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for overwriting the table with a PyArrow table.\n\n        An overwrite may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - REPLACE: In case existing Parquet files need to be rewritten.\n            - APPEND: In case new data is being inserted into the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                              or a boolean expression in case of a partial overwrite\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n        \"\"\"\n        with self.transaction() as tx:\n            tx.overwrite(\n                df=df, overwrite_filter=overwrite_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties\n            )\n\n    def delete(\n        self,\n        delete_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for deleting rows from the table.\n\n        Args:\n            delete_filter: The predicate that used to remove rows\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n        \"\"\"\n        with self.transaction() as tx:\n            tx.delete(delete_filter=delete_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties)\n\n    def add_files(\n        self, file_paths: List[str], snapshot_properties: Dict[str, str] = EMPTY_DICT, check_duplicate_files: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Shorthand API for adding files as data files to the table.\n\n        Args:\n            file_paths: The list of full file paths to be added as data files to the table\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n        \"\"\"\n        with self.transaction() as tx:\n            tx.add_files(\n                file_paths=file_paths, snapshot_properties=snapshot_properties, check_duplicate_files=check_duplicate_files\n            )\n\n    def update_spec(self, case_sensitive: bool = True) -&gt; UpdateSpec:\n        return UpdateSpec(Transaction(self, autocommit=True), case_sensitive=case_sensitive)\n\n    def refs(self) -&gt; Dict[str, SnapshotRef]:\n        \"\"\"Return the snapshot references in the table.\"\"\"\n        return self.metadata.refs\n\n    def _do_commit(self, updates: Tuple[TableUpdate, ...], requirements: Tuple[TableRequirement, ...]) -&gt; None:\n        response = self.catalog.commit_table(self, requirements, updates)\n\n        # https://github.com/apache/iceberg/blob/f6faa58/core/src/main/java/org/apache/iceberg/CatalogUtil.java#L527\n        # delete old metadata if METADATA_DELETE_AFTER_COMMIT_ENABLED is set to true and uses\n        # TableProperties.METADATA_PREVIOUS_VERSIONS_MAX to determine how many previous versions to keep -\n        # everything else will be removed.\n        try:\n            self.catalog._delete_old_metadata(self.io, self.metadata, response.metadata)\n        except Exception as e:\n            warnings.warn(f\"Failed to delete old metadata after commit: {e}\")\n\n        self.metadata = response.metadata\n        self.metadata_location = response.metadata_location\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Table class.\"\"\"\n        return (\n            self.name() == other.name() and self.metadata == other.metadata and self.metadata_location == other.metadata_location\n            if isinstance(other, Table)\n            else False\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Table class.\"\"\"\n        table_name = self.catalog.table_name_from(self._identifier)\n        schema_str = \",\\n  \".join(str(column) for column in self.schema().columns if self.schema())\n        partition_str = f\"partition by: [{', '.join(field.name for field in self.spec().fields if self.spec())}]\"\n        sort_order_str = f\"sort order: [{', '.join(str(field) for field in self.sort_order().fields if self.sort_order())}]\"\n        snapshot_str = f\"snapshot: {str(self.current_snapshot()) if self.current_snapshot() else 'null'}\"\n        result_str = f\"{table_name}(\\n  {schema_str}\\n),\\n{partition_str},\\n{sort_order_str},\\n{snapshot_str}\"\n        return result_str\n\n    def to_daft(self) -&gt; daft.DataFrame:\n        \"\"\"Read a Daft DataFrame lazily from this Iceberg table.\n\n        Returns:\n            daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table\n        \"\"\"\n        import daft\n\n        return daft.read_iceberg(self)\n\n    def to_polars(self) -&gt; pl.LazyFrame:\n        \"\"\"Lazily read from this Apache Iceberg table.\n\n        Returns:\n            pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table\n        \"\"\"\n        import polars as pl\n\n        return pl.scan_iceberg(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.inspect","title":"<code>inspect</code>  <code>property</code>","text":"<p>Return the InspectTable object to browse the table metadata.</p> <p>Returns:</p> Type Description <code>InspectTable</code> <p>InspectTable object based on this Table.</p>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.properties","title":"<code>properties</code>  <code>property</code>","text":"<p>Properties of the table.</p>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Table class.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Table class.\"\"\"\n    return (\n        self.name() == other.name() and self.metadata == other.metadata and self.metadata_location == other.metadata_location\n        if isinstance(other, Table)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Table class.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Table class.\"\"\"\n    table_name = self.catalog.table_name_from(self._identifier)\n    schema_str = \",\\n  \".join(str(column) for column in self.schema().columns if self.schema())\n    partition_str = f\"partition by: [{', '.join(field.name for field in self.spec().fields if self.spec())}]\"\n    sort_order_str = f\"sort order: [{', '.join(str(field) for field in self.sort_order().fields if self.sort_order())}]\"\n    snapshot_str = f\"snapshot: {str(self.current_snapshot()) if self.current_snapshot() else 'null'}\"\n    result_str = f\"{table_name}(\\n  {schema_str}\\n),\\n{partition_str},\\n{sort_order_str},\\n{snapshot_str}\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.add_files","title":"<code>add_files(file_paths, snapshot_properties=EMPTY_DICT, check_duplicate_files=True)</code>","text":"<p>Shorthand API for adding files as data files to the table.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>The list of full file paths to be added as data files to the table</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def add_files(\n    self, file_paths: List[str], snapshot_properties: Dict[str, str] = EMPTY_DICT, check_duplicate_files: bool = True\n) -&gt; None:\n    \"\"\"\n    Shorthand API for adding files as data files to the table.\n\n    Args:\n        file_paths: The list of full file paths to be added as data files to the table\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    with self.transaction() as tx:\n        tx.add_files(\n            file_paths=file_paths, snapshot_properties=snapshot_properties, check_duplicate_files=check_duplicate_files\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.append","title":"<code>append(df, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand API for appending a PyArrow table to the table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be appended to overwrite the table</p> required <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def append(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n    \"\"\"\n    Shorthand API for appending a PyArrow table to the table.\n\n    Args:\n        df: The Arrow dataframe that will be appended to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    \"\"\"\n    with self.transaction() as tx:\n        tx.append(df=df, snapshot_properties=snapshot_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.current_snapshot","title":"<code>current_snapshot()</code>","text":"<p>Get the current snapshot for this table, or None if there is no current snapshot.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def current_snapshot(self) -&gt; Optional[Snapshot]:\n    \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n    if self.metadata.current_snapshot_id is not None:\n        return self.snapshot_by_id(self.metadata.current_snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.delete","title":"<code>delete(delete_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True)</code>","text":"<p>Shorthand for deleting rows from the table.</p> <p>Parameters:</p> Name Type Description Default <code>delete_filter</code> <code>Union[BooleanExpression, str]</code> <p>The predicate that used to remove rows</p> <code>ALWAYS_TRUE</code> <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>delete_filter</code> is case-sensitive</p> <code>True</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def delete(\n    self,\n    delete_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n    snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n) -&gt; None:\n    \"\"\"\n    Shorthand for deleting rows from the table.\n\n    Args:\n        delete_filter: The predicate that used to remove rows\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n    \"\"\"\n    with self.transaction() as tx:\n        tx.delete(delete_filter=delete_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.dynamic_partition_overwrite","title":"<code>dynamic_partition_overwrite(df, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand for dynamic overwriting the table with a PyArrow table.</p> <p>Old partitions are auto detected and replaced with data files created for input arrow table. Args:     df: The Arrow dataframe that will be used to overwrite the table     snapshot_properties: Custom properties to be added to the snapshot summary</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def dynamic_partition_overwrite(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n    \"\"\"Shorthand for dynamic overwriting the table with a PyArrow table.\n\n    Old partitions are auto detected and replaced with data files created for input arrow table.\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    \"\"\"\n    with self.transaction() as tx:\n        tx.dynamic_partition_overwrite(df=df, snapshot_properties=snapshot_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.history","title":"<code>history()</code>","text":"<p>Get the snapshot history of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def history(self) -&gt; List[SnapshotLogEntry]:\n    \"\"\"Get the snapshot history of this table.\"\"\"\n    return self.metadata.snapshot_log\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.last_partition_id","title":"<code>last_partition_id()</code>","text":"<p>Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def last_partition_id(self) -&gt; int:\n    \"\"\"Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.\"\"\"\n    if self.metadata.last_partition_id:\n        return self.metadata.last_partition_id\n    return PARTITION_FIELD_ID_START - 1\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.location","title":"<code>location()</code>","text":"<p>Return the table's base location.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def location(self) -&gt; str:\n    \"\"\"Return the table's base location.\"\"\"\n    return self.metadata.location\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.location_provider","title":"<code>location_provider()</code>","text":"<p>Return the table's location provider.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def location_provider(self) -&gt; LocationProvider:\n    \"\"\"Return the table's location provider.\"\"\"\n    return load_location_provider(table_location=self.metadata.location, table_properties=self.metadata.properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.manage_snapshots","title":"<code>manage_snapshots()</code>","text":"<p>Shorthand to run snapshot management operations like create branch, create tag, etc.</p> <p>Use table.manage_snapshots().().commit() to run a specific operation. Use table.manage_snapshots().().().commit() to run multiple operations. Pending changes are applied on commit. <p>We can also use context managers to make more changes. For example,</p> <p>with table.manage_snapshots() as ms:    ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def manage_snapshots(self) -&gt; ManageSnapshots:\n    \"\"\"\n    Shorthand to run snapshot management operations like create branch, create tag, etc.\n\n    Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example,\n\n    with table.manage_snapshots() as ms:\n       ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n    \"\"\"\n    return ManageSnapshots(transaction=Transaction(self, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.name","title":"<code>name()</code>","text":"<p>Return the identifier of this table.</p> <p>Returns:</p> Type Description <code>Identifier</code> <p>An Identifier tuple of the table name</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def name(self) -&gt; Identifier:\n    \"\"\"Return the identifier of this table.\n\n    Returns:\n        An Identifier tuple of the table name\n    \"\"\"\n    return self._identifier\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.name_mapping","title":"<code>name_mapping()</code>","text":"<p>Return the table's field-id NameMapping.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def name_mapping(self) -&gt; Optional[NameMapping]:\n    \"\"\"Return the table's field-id NameMapping.\"\"\"\n    return self.metadata.name_mapping()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.overwrite","title":"<code>overwrite(df, overwrite_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True)</code>","text":"<p>Shorthand for overwriting the table with a PyArrow table.</p> <p>An overwrite may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- REPLACE: In case existing Parquet files need to be rewritten.\n- APPEND: In case new data is being inserted into the table.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>overwrite_filter</code> <code>Union[BooleanExpression, str]</code> <p>ALWAYS_TRUE when you overwrite all the data,               or a boolean expression in case of a partial overwrite</p> <code>ALWAYS_TRUE</code> <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>overwrite_filter</code> is case-sensitive</p> <code>True</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def overwrite(\n    self,\n    df: pa.Table,\n    overwrite_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n    snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n) -&gt; None:\n    \"\"\"\n    Shorthand for overwriting the table with a PyArrow table.\n\n    An overwrite may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - REPLACE: In case existing Parquet files need to be rewritten.\n        - APPEND: In case new data is being inserted into the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                          or a boolean expression in case of a partial overwrite\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n    \"\"\"\n    with self.transaction() as tx:\n        tx.overwrite(\n            df=df, overwrite_filter=overwrite_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.refresh","title":"<code>refresh()</code>","text":"<p>Refresh the current table metadata.</p> <p>Returns:</p> Type Description <code>Table</code> <p>An updated instance of the same Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refresh(self) -&gt; Table:\n    \"\"\"Refresh the current table metadata.\n\n    Returns:\n        An updated instance of the same Iceberg table\n    \"\"\"\n    fresh = self.catalog.load_table(self._identifier)\n    self.metadata = fresh.metadata\n    self.io = fresh.io\n    self.metadata_location = fresh.metadata_location\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.refs","title":"<code>refs()</code>","text":"<p>Return the snapshot references in the table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refs(self) -&gt; Dict[str, SnapshotRef]:\n    \"\"\"Return the snapshot references in the table.\"\"\"\n    return self.metadata.refs\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.scan","title":"<code>scan(row_filter=ALWAYS_TRUE, selected_fields=('*',), case_sensitive=True, snapshot_id=None, options=EMPTY_DICT, limit=None)</code>","text":"<p>Fetch a DataScan based on the table's current metadata.</p> <pre><code>The data scan can be used to project the table's data\nthat matches the provided row_filter onto the table's\ncurrent schema.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>row_filter</code> <code>Union[str, BooleanExpression]</code> <p>A string or BooleanExpression that describes the desired rows</p> <code>ALWAYS_TRUE</code> <code>selected_fields</code> <code>Tuple[str, ...]</code> <p>A tuple of strings representing the column names to return in the output dataframe.</p> <code>('*',)</code> <code>case_sensitive</code> <code>bool</code> <p>If True column matching is case sensitive</p> <code>True</code> <code>snapshot_id</code> <code>Optional[int]</code> <p>Optional Snapshot ID to time travel to. If None, scans the table as of the current snapshot ID.</p> <code>None</code> <code>options</code> <code>Properties</code> <p>Additional Table properties as a dictionary of string key value pairs to use for this scan.</p> <code>EMPTY_DICT</code> <code>limit</code> <code>Optional[int]</code> <p>An integer representing the number of rows to return in the scan result. If None, fetches all matching rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataScan</code> <p>A DataScan based on the table's current metadata.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def scan(\n    self,\n    row_filter: Union[str, BooleanExpression] = ALWAYS_TRUE,\n    selected_fields: Tuple[str, ...] = (\"*\",),\n    case_sensitive: bool = True,\n    snapshot_id: Optional[int] = None,\n    options: Properties = EMPTY_DICT,\n    limit: Optional[int] = None,\n) -&gt; DataScan:\n    \"\"\"Fetch a DataScan based on the table's current metadata.\n\n        The data scan can be used to project the table's data\n        that matches the provided row_filter onto the table's\n        current schema.\n\n    Args:\n        row_filter:\n            A string or BooleanExpression that describes the\n            desired rows\n        selected_fields:\n            A tuple of strings representing the column names\n            to return in the output dataframe.\n        case_sensitive:\n            If True column matching is case sensitive\n        snapshot_id:\n            Optional Snapshot ID to time travel to. If None,\n            scans the table as of the current snapshot ID.\n        options:\n            Additional Table properties as a dictionary of\n            string key value pairs to use for this scan.\n        limit:\n            An integer representing the number of rows to\n            return in the scan result. If None, fetches all\n            matching rows.\n\n    Returns:\n        A DataScan based on the table's current metadata.\n    \"\"\"\n    return DataScan(\n        table_metadata=self.metadata,\n        io=self.io,\n        row_filter=row_filter,\n        selected_fields=selected_fields,\n        case_sensitive=case_sensitive,\n        snapshot_id=snapshot_id,\n        options=options,\n        limit=limit,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.schema","title":"<code>schema()</code>","text":"<p>Return the schema for this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def schema(self) -&gt; Schema:\n    \"\"\"Return the schema for this table.\"\"\"\n    return next(schema for schema in self.metadata.schemas if schema.schema_id == self.metadata.current_schema_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.schemas","title":"<code>schemas()</code>","text":"<p>Return a dict of the schema of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def schemas(self) -&gt; Dict[int, Schema]:\n    \"\"\"Return a dict of the schema of this table.\"\"\"\n    return {schema.schema_id: schema for schema in self.metadata.schemas}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_as_of_timestamp","title":"<code>snapshot_as_of_timestamp(timestamp_ms, inclusive=True)</code>","text":"<p>Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_ms</code> <code>int</code> <p>Find snapshot that was current at/before this timestamp</p> required <code>inclusive</code> <code>bool</code> <p>Includes timestamp_ms in search when True. Excludes timestamp_ms when False</p> <code>True</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_as_of_timestamp(self, timestamp_ms: int, inclusive: bool = True) -&gt; Optional[Snapshot]:\n    \"\"\"Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.\n\n    Args:\n        timestamp_ms: Find snapshot that was current at/before this timestamp\n        inclusive: Includes timestamp_ms in search when True. Excludes timestamp_ms when False\n    \"\"\"\n    for log_entry in reversed(self.history()):\n        if (inclusive and log_entry.timestamp_ms &lt;= timestamp_ms) or log_entry.timestamp_ms &lt; timestamp_ms:\n            return self.snapshot_by_id(log_entry.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_by_id","title":"<code>snapshot_by_id(snapshot_id)</code>","text":"<p>Get the snapshot of this table with the given id, or None if there is no matching snapshot.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_by_id(self, snapshot_id: int) -&gt; Optional[Snapshot]:\n    \"\"\"Get the snapshot of this table with the given id, or None if there is no matching snapshot.\"\"\"\n    return self.metadata.snapshot_by_id(snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_by_name","title":"<code>snapshot_by_name(name)</code>","text":"<p>Return the snapshot referenced by the given name or null if no such reference exists.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_by_name(self, name: str) -&gt; Optional[Snapshot]:\n    \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n    if ref := self.metadata.refs.get(name):\n        return self.snapshot_by_id(ref.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.sort_order","title":"<code>sort_order()</code>","text":"<p>Return the sort order of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def sort_order(self) -&gt; SortOrder:\n    \"\"\"Return the sort order of this table.\"\"\"\n    return next(\n        sort_order for sort_order in self.metadata.sort_orders if sort_order.order_id == self.metadata.default_sort_order_id\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.sort_orders","title":"<code>sort_orders()</code>","text":"<p>Return a dict of the sort orders of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def sort_orders(self) -&gt; Dict[int, SortOrder]:\n    \"\"\"Return a dict of the sort orders of this table.\"\"\"\n    return {sort_order.order_id: sort_order for sort_order in self.metadata.sort_orders}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.spec","title":"<code>spec()</code>","text":"<p>Return the partition spec of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def spec(self) -&gt; PartitionSpec:\n    \"\"\"Return the partition spec of this table.\"\"\"\n    return next(spec for spec in self.metadata.partition_specs if spec.spec_id == self.metadata.default_spec_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.specs","title":"<code>specs()</code>","text":"<p>Return a dict the partition specs this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def specs(self) -&gt; Dict[int, PartitionSpec]:\n    \"\"\"Return a dict the partition specs this table.\"\"\"\n    return {spec.spec_id: spec for spec in self.metadata.partition_specs}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.to_daft","title":"<code>to_daft()</code>","text":"<p>Read a Daft DataFrame lazily from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_daft(self) -&gt; daft.DataFrame:\n    \"\"\"Read a Daft DataFrame lazily from this Iceberg table.\n\n    Returns:\n        daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table\n    \"\"\"\n    import daft\n\n    return daft.read_iceberg(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.to_polars","title":"<code>to_polars()</code>","text":"<p>Lazily read from this Apache Iceberg table.</p> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_polars(self) -&gt; pl.LazyFrame:\n    \"\"\"Lazily read from this Apache Iceberg table.\n\n    Returns:\n        pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table\n    \"\"\"\n    import polars as pl\n\n    return pl.scan_iceberg(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.transaction","title":"<code>transaction()</code>","text":"<p>Create a new transaction object to first stage the changes, and then commit them to the catalog.</p> <p>Returns:</p> Type Description <code>Transaction</code> <p>The transaction object</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def transaction(self) -&gt; Transaction:\n    \"\"\"Create a new transaction object to first stage the changes, and then commit them to the catalog.\n\n    Returns:\n        The transaction object\n    \"\"\"\n    return Transaction(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.update_schema","title":"<code>update_schema(allow_incompatible_changes=False, case_sensitive=True)</code>","text":"<p>Create a new UpdateSchema to alter the columns of this table.</p> <p>Parameters:</p> Name Type Description Default <code>allow_incompatible_changes</code> <code>bool</code> <p>If changes are allowed that might break downstream consumers.</p> <code>False</code> <code>case_sensitive</code> <code>bool</code> <p>If field names are case-sensitive.</p> <code>True</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>A new UpdateSchema.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n    \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n    Args:\n        allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n        case_sensitive: If field names are case-sensitive.\n\n    Returns:\n        A new UpdateSchema.\n    \"\"\"\n    return UpdateSchema(\n        transaction=Transaction(self, autocommit=True),\n        allow_incompatible_changes=allow_incompatible_changes,\n        case_sensitive=case_sensitive,\n        name_mapping=self.name_mapping(),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.update_statistics","title":"<code>update_statistics()</code>","text":"<p>Shorthand to run statistics management operations like add statistics and remove statistics.</p> <p>Use table.update_statistics().().commit() to run a specific operation. Use table.update_statistics().().().commit() to run multiple operations. <p>Pending changes are applied on commit.</p> <p>We can also use context managers to make more changes. For example:</p> <p>with table.update_statistics() as update:     update.set_statistics(statistics_file=statistics_file)     update.remove_statistics(snapshot_id=2)</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_statistics(self) -&gt; UpdateStatistics:\n    \"\"\"\n    Shorthand to run statistics management operations like add statistics and remove statistics.\n\n    Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example:\n\n    with table.update_statistics() as update:\n        update.set_statistics(statistics_file=statistics_file)\n        update.remove_statistics(snapshot_id=2)\n    \"\"\"\n    return UpdateStatistics(transaction=Transaction(self, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.upsert","title":"<code>upsert(df, join_cols=None, when_matched_update_all=True, when_not_matched_insert_all=True, case_sensitive=True)</code>","text":"<p>Shorthand API for performing an upsert to an iceberg table.</p> <p>Args:</p> <pre><code>df: The input dataframe to upsert with the table's data.\njoin_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\nwhen_matched_update_all: Bool indicating to update rows that are matched but require an update due to a value in a non-key column changing\nwhen_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any existing rows in the table\ncase_sensitive: Bool indicating if the match should be case-sensitive\n\nTo learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n    Example Use Cases:\n        Case 1: Both Parameters = True (Full Upsert)\n        Existing row found \u2192 Update it\n        New row found \u2192 Insert it\n\n        Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n        Existing row found \u2192 Do nothing (no updates)\n        New row found \u2192 Insert it\n\n        Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n        Existing row found \u2192 Update it\n        New row found \u2192 Do nothing (no inserts)\n\n        Case 4: Both Parameters = False (No Merge Effect)\n        Existing row found \u2192 Do nothing\n        New row found \u2192 Do nothing\n        (Function effectively does nothing)\n</code></pre> <p>Returns:</p> Type Description <code>UpsertResult</code> <p>An UpsertResult class (contains details of rows updated and inserted)</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def upsert(\n    self,\n    df: pa.Table,\n    join_cols: Optional[List[str]] = None,\n    when_matched_update_all: bool = True,\n    when_not_matched_insert_all: bool = True,\n    case_sensitive: bool = True,\n) -&gt; UpsertResult:\n    \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n    Args:\n\n        df: The input dataframe to upsert with the table's data.\n        join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n        when_matched_update_all: Bool indicating to update rows that are matched but require an update due to a value in a non-key column changing\n        when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any existing rows in the table\n        case_sensitive: Bool indicating if the match should be case-sensitive\n\n        To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n            Example Use Cases:\n                Case 1: Both Parameters = True (Full Upsert)\n                Existing row found \u2192 Update it\n                New row found \u2192 Insert it\n\n                Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                Existing row found \u2192 Do nothing (no updates)\n                New row found \u2192 Insert it\n\n                Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                Existing row found \u2192 Update it\n                New row found \u2192 Do nothing (no inserts)\n\n                Case 4: Both Parameters = False (No Merge Effect)\n                Existing row found \u2192 Do nothing\n                New row found \u2192 Do nothing\n                (Function effectively does nothing)\n\n\n    Returns:\n        An UpsertResult class (contains details of rows updated and inserted)\n    \"\"\"\n    try:\n        import pyarrow as pa  # noqa: F401\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import expression_to_pyarrow\n    from pyiceberg.table import upsert_util\n\n    if join_cols is None:\n        join_cols = []\n        for field_id in self.schema().identifier_field_ids:\n            col = self.schema().find_column_name(field_id)\n            if col is not None:\n                join_cols.append(col)\n            else:\n                raise ValueError(f\"Field-ID could not be found: {join_cols}\")\n\n    if len(join_cols) == 0:\n        raise ValueError(\"Join columns could not be found, please set identifier-field-ids or pass in explicitly.\")\n\n    if not when_matched_update_all and not when_not_matched_insert_all:\n        raise ValueError(\"no upsert options selected...exiting\")\n\n    if upsert_util.has_duplicate_rows(df, join_cols):\n        raise ValueError(\"Duplicate rows found in source dataset based on the key columns. No upsert executed\")\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n    )\n\n    # get list of rows that exist so we don't have to load the entire target table\n    matched_predicate = upsert_util.create_match_filter(df, join_cols)\n    matched_iceberg_table = self.scan(row_filter=matched_predicate, case_sensitive=case_sensitive).to_arrow()\n\n    update_row_cnt = 0\n    insert_row_cnt = 0\n\n    with self.transaction() as tx:\n        if when_matched_update_all:\n            # function get_rows_to_update is doing a check on non-key columns to see if any of the values have actually changed\n            # we don't want to do just a blanket overwrite for matched rows if the actual non-key column data hasn't changed\n            # this extra step avoids unnecessary IO and writes\n            rows_to_update = upsert_util.get_rows_to_update(df, matched_iceberg_table, join_cols)\n\n            update_row_cnt = len(rows_to_update)\n\n            # build the match predicate filter\n            overwrite_mask_predicate = upsert_util.create_match_filter(rows_to_update, join_cols)\n\n            tx.overwrite(rows_to_update, overwrite_filter=overwrite_mask_predicate)\n\n        if when_not_matched_insert_all:\n            expr_match = upsert_util.create_match_filter(matched_iceberg_table, join_cols)\n            expr_match_bound = bind(self.schema(), expr_match, case_sensitive=case_sensitive)\n            expr_match_arrow = expression_to_pyarrow(expr_match_bound)\n            rows_to_insert = df.filter(~expr_match_arrow)\n\n            insert_row_cnt = len(rows_to_insert)\n\n            tx.append(rows_to_insert)\n\n    return UpsertResult(rows_updated=update_row_cnt, rows_inserted=insert_row_cnt)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableIdentifier","title":"<code>TableIdentifier</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Fully Qualified identifier to a table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class TableIdentifier(IcebergBaseModel):\n    \"\"\"Fully Qualified identifier to a table.\"\"\"\n\n    namespace: Namespace\n    name: str\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableScan","title":"<code>TableScan</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class TableScan(ABC):\n    table_metadata: TableMetadata\n    io: FileIO\n    row_filter: BooleanExpression\n    selected_fields: Tuple[str, ...]\n    case_sensitive: bool\n    snapshot_id: Optional[int]\n    options: Properties\n    limit: Optional[int]\n\n    def __init__(\n        self,\n        table_metadata: TableMetadata,\n        io: FileIO,\n        row_filter: Union[str, BooleanExpression] = ALWAYS_TRUE,\n        selected_fields: Tuple[str, ...] = (\"*\",),\n        case_sensitive: bool = True,\n        snapshot_id: Optional[int] = None,\n        options: Properties = EMPTY_DICT,\n        limit: Optional[int] = None,\n    ):\n        self.table_metadata = table_metadata\n        self.io = io\n        self.row_filter = _parse_row_filter(row_filter)\n        self.selected_fields = selected_fields\n        self.case_sensitive = case_sensitive\n        self.snapshot_id = snapshot_id\n        self.options = options\n        self.limit = limit\n\n    def snapshot(self) -&gt; Optional[Snapshot]:\n        if self.snapshot_id:\n            return self.table_metadata.snapshot_by_id(self.snapshot_id)\n        return self.table_metadata.current_snapshot()\n\n    def projection(self) -&gt; Schema:\n        current_schema = self.table_metadata.schema()\n        if self.snapshot_id is not None:\n            snapshot = self.table_metadata.snapshot_by_id(self.snapshot_id)\n            if snapshot is not None:\n                if snapshot.schema_id is not None:\n                    try:\n                        current_schema = next(\n                            schema for schema in self.table_metadata.schemas if schema.schema_id == snapshot.schema_id\n                        )\n                    except StopIteration:\n                        warnings.warn(f\"Metadata does not contain schema with id: {snapshot.schema_id}\")\n            else:\n                raise ValueError(f\"Snapshot not found: {self.snapshot_id}\")\n\n        if \"*\" in self.selected_fields:\n            return current_schema\n\n        return current_schema.select(*self.selected_fields, case_sensitive=self.case_sensitive)\n\n    @abstractmethod\n    def plan_files(self) -&gt; Iterable[ScanTask]: ...\n\n    @abstractmethod\n    def to_arrow(self) -&gt; pa.Table: ...\n\n    @abstractmethod\n    def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame: ...\n\n    @abstractmethod\n    def to_polars(self) -&gt; pl.DataFrame: ...\n\n    def update(self: S, **overrides: Any) -&gt; S:\n        \"\"\"Create a copy of this table scan with updated fields.\"\"\"\n        return type(self)(**{**self.__dict__, **overrides})\n\n    def use_ref(self: S, name: str) -&gt; S:\n        if self.snapshot_id:\n            raise ValueError(f\"Cannot override ref, already set snapshot id={self.snapshot_id}\")\n        if snapshot := self.table_metadata.snapshot_by_name(name):\n            return self.update(snapshot_id=snapshot.snapshot_id)\n\n        raise ValueError(f\"Cannot scan unknown ref={name}\")\n\n    def select(self: S, *field_names: str) -&gt; S:\n        if \"*\" in self.selected_fields:\n            return self.update(selected_fields=field_names)\n        return self.update(selected_fields=tuple(set(self.selected_fields).intersection(set(field_names))))\n\n    def filter(self: S, expr: Union[str, BooleanExpression]) -&gt; S:\n        return self.update(row_filter=And(self.row_filter, _parse_row_filter(expr)))\n\n    def with_case_sensitive(self: S, case_sensitive: bool = True) -&gt; S:\n        return self.update(case_sensitive=case_sensitive)\n\n    @abstractmethod\n    def count(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableScan.update","title":"<code>update(**overrides)</code>","text":"<p>Create a copy of this table scan with updated fields.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update(self: S, **overrides: Any) -&gt; S:\n    \"\"\"Create a copy of this table scan with updated fields.\"\"\"\n    return type(self)(**{**self.__dict__, **overrides})\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction","title":"<code>Transaction</code>","text":"Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Transaction:\n    _table: Table\n    table_metadata: TableMetadata\n    _autocommit: bool\n    _updates: Tuple[TableUpdate, ...]\n    _requirements: Tuple[TableRequirement, ...]\n\n    def __init__(self, table: Table, autocommit: bool = False):\n        \"\"\"Open a transaction to stage and commit changes to a table.\n\n        Args:\n            table: The table that will be altered.\n            autocommit: Option to automatically commit the changes when they are staged.\n        \"\"\"\n        self.table_metadata = table.metadata\n        self._table = table\n        self._autocommit = autocommit\n        self._updates = ()\n        self._requirements = ()\n\n    def __enter__(self) -&gt; Transaction:\n        \"\"\"Start a transaction to update the table.\"\"\"\n        return self\n\n    def __exit__(\n        self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n    ) -&gt; None:\n        \"\"\"Close and commit the transaction if no exceptions have been raised.\"\"\"\n        if exctype is None and excinst is None and exctb is None:\n            self.commit_transaction()\n\n    def _apply(self, updates: Tuple[TableUpdate, ...], requirements: Tuple[TableRequirement, ...] = ()) -&gt; Transaction:\n        \"\"\"Check if the requirements are met, and applies the updates to the metadata.\"\"\"\n        for requirement in requirements:\n            requirement.validate(self.table_metadata)\n\n        self._updates += updates\n\n        # For the requirements, it does not make sense to add a requirement more than once\n        # For example, you cannot assert that the current schema has two different IDs\n        existing_requirements = {type(requirement) for requirement in self._requirements}\n        for new_requirement in requirements:\n            if type(new_requirement) not in existing_requirements:\n                self._requirements = self._requirements + (new_requirement,)\n\n        self.table_metadata = update_table_metadata(self.table_metadata, updates)\n\n        if self._autocommit:\n            self.commit_transaction()\n            self._updates = ()\n            self._requirements = ()\n\n        return self\n\n    def _scan(self, row_filter: Union[str, BooleanExpression] = ALWAYS_TRUE, case_sensitive: bool = True) -&gt; DataScan:\n        \"\"\"Minimal data scan of the table with the current state of the transaction.\"\"\"\n        return DataScan(\n            table_metadata=self.table_metadata, io=self._table.io, row_filter=row_filter, case_sensitive=case_sensitive\n        )\n\n    def upgrade_table_version(self, format_version: TableVersion) -&gt; Transaction:\n        \"\"\"Set the table to a certain version.\n\n        Args:\n            format_version: The newly set version.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        if format_version not in {1, 2}:\n            raise ValueError(f\"Unsupported table format version: {format_version}\")\n\n        if format_version &lt; self.table_metadata.format_version:\n            raise ValueError(f\"Cannot downgrade v{self.table_metadata.format_version} table to v{format_version}\")\n\n        if format_version &gt; self.table_metadata.format_version:\n            return self._apply((UpgradeFormatVersionUpdate(format_version=format_version),))\n\n        return self\n\n    def set_properties(self, properties: Properties = EMPTY_DICT, **kwargs: Any) -&gt; Transaction:\n        \"\"\"Set properties.\n\n        When a property is already set, it will be overwritten.\n\n        Args:\n            properties: The properties set on the table.\n            kwargs: properties can also be pass as kwargs.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        if properties and kwargs:\n            raise ValueError(\"Cannot pass both properties and kwargs\")\n        updates = properties or kwargs\n        return self._apply((SetPropertiesUpdate(updates=updates),))\n\n    def _set_ref_snapshot(\n        self,\n        snapshot_id: int,\n        ref_name: str,\n        type: str,\n        max_ref_age_ms: Optional[int] = None,\n        max_snapshot_age_ms: Optional[int] = None,\n        min_snapshots_to_keep: Optional[int] = None,\n    ) -&gt; UpdatesAndRequirements:\n        \"\"\"Update a ref to a snapshot.\n\n        Returns:\n            The updates and requirements for the set-snapshot-ref staged\n        \"\"\"\n        updates = (\n            SetSnapshotRefUpdate(\n                snapshot_id=snapshot_id,\n                ref_name=ref_name,\n                type=type,\n                max_ref_age_ms=max_ref_age_ms,\n                max_snapshot_age_ms=max_snapshot_age_ms,\n                min_snapshots_to_keep=min_snapshots_to_keep,\n            ),\n        )\n        requirements = (\n            AssertRefSnapshotId(\n                snapshot_id=self.table_metadata.refs[ref_name].snapshot_id if ref_name in self.table_metadata.refs else None,\n                ref=ref_name,\n            ),\n        )\n\n        return updates, requirements\n\n    def _build_partition_predicate(self, partition_records: Set[Record]) -&gt; BooleanExpression:\n        \"\"\"Build a filter predicate matching any of the input partition records.\n\n        Args:\n            partition_records: A set of partition records to match\n        Returns:\n            A predicate matching any of the input partition records.\n        \"\"\"\n        partition_spec = self.table_metadata.spec()\n        schema = self.table_metadata.schema()\n        partition_fields = [schema.find_field(field.source_id).name for field in partition_spec.fields]\n\n        expr: BooleanExpression = AlwaysFalse()\n        for partition_record in partition_records:\n            match_partition_expression: BooleanExpression = AlwaysTrue()\n\n            for pos, partition_field in enumerate(partition_fields):\n                predicate = (\n                    EqualTo(Reference(partition_field), partition_record[pos])\n                    if partition_record[pos] is not None\n                    else IsNull(Reference(partition_field))\n                )\n                match_partition_expression = And(match_partition_expression, predicate)\n            expr = Or(expr, match_partition_expression)\n        return expr\n\n    def _append_snapshot_producer(self, snapshot_properties: Dict[str, str]) -&gt; _FastAppendFiles:\n        \"\"\"Determine the append type based on table properties.\n\n        Args:\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        Returns:\n            Either a fast-append or a merge-append snapshot producer.\n        \"\"\"\n        manifest_merge_enabled = property_as_bool(\n            self.table_metadata.properties,\n            TableProperties.MANIFEST_MERGE_ENABLED,\n            TableProperties.MANIFEST_MERGE_ENABLED_DEFAULT,\n        )\n        update_snapshot = self.update_snapshot(snapshot_properties=snapshot_properties)\n        return update_snapshot.merge_append() if manifest_merge_enabled else update_snapshot.fast_append()\n\n    def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n        \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n        Args:\n            allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n            case_sensitive: If field names are case-sensitive.\n\n        Returns:\n            A new UpdateSchema.\n        \"\"\"\n        return UpdateSchema(\n            self,\n            allow_incompatible_changes=allow_incompatible_changes,\n            case_sensitive=case_sensitive,\n            name_mapping=self.table_metadata.name_mapping(),\n        )\n\n    def update_snapshot(self, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; UpdateSnapshot:\n        \"\"\"Create a new UpdateSnapshot to produce a new snapshot for the table.\n\n        Returns:\n            A new UpdateSnapshot\n        \"\"\"\n        return UpdateSnapshot(self, io=self._table.io, snapshot_properties=snapshot_properties)\n\n    def append(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n        \"\"\"\n        Shorthand API for appending a PyArrow table to a table transaction.\n\n        Args:\n            df: The Arrow dataframe that will be appended to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        if unsupported_partitions := [\n            field for field in self.table_metadata.spec().fields if not field.transform.supports_pyarrow_transform\n        ]:\n            raise ValueError(\n                f\"Not all partition types are supported for writes. Following partitions cannot be written using pyarrow: {unsupported_partitions}.\"\n            )\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n        )\n\n        with self._append_snapshot_producer(snapshot_properties) as append_files:\n            # skip writing data files if the dataframe is empty\n            if df.shape[0] &gt; 0:\n                data_files = list(\n                    _dataframe_to_data_files(\n                        table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                    )\n                )\n                for data_file in data_files:\n                    append_files.append_data_file(data_file)\n\n    def dynamic_partition_overwrite(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n        \"\"\"\n        Shorthand for overwriting existing partitions with a PyArrow table.\n\n        The function detects partition values in the provided arrow table using the current\n        partition spec, and deletes existing partitions matching these values. Finally, the\n        data in the table is appended to the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        if self.table_metadata.spec().is_unpartitioned():\n            raise ValueError(\"Cannot apply dynamic overwrite on an unpartitioned table.\")\n\n        for field in self.table_metadata.spec().fields:\n            if not isinstance(field.transform, IdentityTransform):\n                raise ValueError(\n                    f\"For now dynamic overwrite does not support a table with non-identity-transform field in the latest partition spec: {field}\"\n                )\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n        )\n\n        # If dataframe does not have data, there is no need to overwrite\n        if df.shape[0] == 0:\n            return\n\n        append_snapshot_commit_uuid = uuid.uuid4()\n        data_files: List[DataFile] = list(\n            _dataframe_to_data_files(\n                table_metadata=self._table.metadata, write_uuid=append_snapshot_commit_uuid, df=df, io=self._table.io\n            )\n        )\n\n        partitions_to_overwrite = {data_file.partition for data_file in data_files}\n        delete_filter = self._build_partition_predicate(partition_records=partitions_to_overwrite)\n        self.delete(delete_filter=delete_filter, snapshot_properties=snapshot_properties)\n\n        with self._append_snapshot_producer(snapshot_properties) as append_files:\n            append_files.commit_uuid = append_snapshot_commit_uuid\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n\n    def overwrite(\n        self,\n        df: pa.Table,\n        overwrite_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for adding a table overwrite with a PyArrow table to the transaction.\n\n        An overwrite may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - REPLACE: In case existing Parquet files need to be rewritten.\n            - APPEND: In case new data is being inserted into the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                              or a boolean expression in case of a partial overwrite\n            case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        if unsupported_partitions := [\n            field for field in self.table_metadata.spec().fields if not field.transform.supports_pyarrow_transform\n        ]:\n            raise ValueError(\n                f\"Not all partition types are supported for writes. Following partitions cannot be written using pyarrow: {unsupported_partitions}.\"\n            )\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n        )\n\n        if overwrite_filter != AlwaysFalse():\n            # Only delete when the filter is != AlwaysFalse\n            self.delete(delete_filter=overwrite_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties)\n\n        with self._append_snapshot_producer(snapshot_properties) as append_files:\n            # skip writing data files if the dataframe is empty\n            if df.shape[0] &gt; 0:\n                data_files = _dataframe_to_data_files(\n                    table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                )\n                for data_file in data_files:\n                    append_files.append_data_file(data_file)\n\n    def delete(\n        self,\n        delete_filter: Union[str, BooleanExpression],\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for deleting record from a table.\n\n        A delete may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - REPLACE: In case existing Parquet files need to be rewritten\n\n        Args:\n            delete_filter: A boolean expression to delete rows from a table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n        \"\"\"\n        from pyiceberg.io.pyarrow import (\n            ArrowScan,\n            _dataframe_to_data_files,\n            _expression_to_complementary_pyarrow,\n        )\n\n        if (\n            self.table_metadata.properties.get(TableProperties.DELETE_MODE, TableProperties.DELETE_MODE_DEFAULT)\n            == TableProperties.DELETE_MODE_MERGE_ON_READ\n        ):\n            warnings.warn(\"Merge on read is not yet supported, falling back to copy-on-write\")\n\n        if isinstance(delete_filter, str):\n            delete_filter = _parse_row_filter(delete_filter)\n\n        with self.update_snapshot(snapshot_properties=snapshot_properties).delete() as delete_snapshot:\n            delete_snapshot.delete_by_predicate(delete_filter, case_sensitive)\n\n        # Check if there are any files that require an actual rewrite of a data file\n        if delete_snapshot.rewrites_needed is True:\n            bound_delete_filter = bind(self.table_metadata.schema(), delete_filter, case_sensitive)\n            preserve_row_filter = _expression_to_complementary_pyarrow(bound_delete_filter)\n\n            files = self._scan(row_filter=delete_filter, case_sensitive=case_sensitive).plan_files()\n\n            commit_uuid = uuid.uuid4()\n            counter = itertools.count(0)\n\n            replaced_files: List[Tuple[DataFile, List[DataFile]]] = []\n            # This will load the Parquet file into memory, including:\n            #   - Filter out the rows based on the delete filter\n            #   - Projecting it to the current schema\n            #   - Applying the positional deletes if they are there\n            # When writing\n            #   - Apply the latest partition-spec\n            #   - And sort order when added\n            for original_file in files:\n                df = ArrowScan(\n                    table_metadata=self.table_metadata,\n                    io=self._table.io,\n                    projected_schema=self.table_metadata.schema(),\n                    row_filter=AlwaysTrue(),\n                ).to_table(tasks=[original_file])\n                filtered_df = df.filter(preserve_row_filter)\n\n                # Only rewrite if there are records being deleted\n                if len(filtered_df) == 0:\n                    replaced_files.append((original_file.file, []))\n                elif len(df) != len(filtered_df):\n                    replaced_files.append(\n                        (\n                            original_file.file,\n                            list(\n                                _dataframe_to_data_files(\n                                    io=self._table.io,\n                                    df=filtered_df,\n                                    table_metadata=self.table_metadata,\n                                    write_uuid=commit_uuid,\n                                    counter=counter,\n                                )\n                            ),\n                        )\n                    )\n\n            if len(replaced_files) &gt; 0:\n                with self.update_snapshot(snapshot_properties=snapshot_properties).overwrite() as overwrite_snapshot:\n                    overwrite_snapshot.commit_uuid = commit_uuid\n                    for original_data_file, replaced_data_files in replaced_files:\n                        overwrite_snapshot.delete_data_file(original_data_file)\n                        for replaced_data_file in replaced_data_files:\n                            overwrite_snapshot.append_data_file(replaced_data_file)\n\n        if not delete_snapshot.files_affected and not delete_snapshot.rewrites_needed:\n            warnings.warn(\"Delete operation did not match any records\")\n\n    def add_files(\n        self, file_paths: List[str], snapshot_properties: Dict[str, str] = EMPTY_DICT, check_duplicate_files: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Shorthand API for adding files as data files to the table transaction.\n\n        Args:\n            file_paths: The list of full file paths to be added as data files to the table\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: Raises a ValueError given file_paths contains duplicate files\n            ValueError: Raises a ValueError given file_paths already referenced by table\n        \"\"\"\n        if len(file_paths) != len(set(file_paths)):\n            raise ValueError(\"File paths must be unique\")\n\n        if check_duplicate_files:\n            import pyarrow.compute as pc\n\n            expr = pc.field(\"file_path\").isin(file_paths)\n            referenced_files = [file[\"file_path\"] for file in self._table.inspect.files().filter(expr).to_pylist()]\n\n            if referenced_files:\n                raise ValueError(f\"Cannot add files that are already referenced by table, files: {', '.join(referenced_files)}\")\n\n        if self.table_metadata.name_mapping() is None:\n            self.set_properties(\n                **{TableProperties.DEFAULT_NAME_MAPPING: self.table_metadata.schema().name_mapping.model_dump_json()}\n            )\n        with self.update_snapshot(snapshot_properties=snapshot_properties).fast_append() as update_snapshot:\n            data_files = _parquet_files_to_data_files(\n                table_metadata=self.table_metadata, file_paths=file_paths, io=self._table.io\n            )\n            for data_file in data_files:\n                update_snapshot.append_data_file(data_file)\n\n    def update_spec(self) -&gt; UpdateSpec:\n        \"\"\"Create a new UpdateSpec to update the partitioning of the table.\n\n        Returns:\n            A new UpdateSpec.\n        \"\"\"\n        return UpdateSpec(self)\n\n    def remove_properties(self, *removals: str) -&gt; Transaction:\n        \"\"\"Remove properties.\n\n        Args:\n            removals: Properties to be removed.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        return self._apply((RemovePropertiesUpdate(removals=removals),))\n\n    def update_location(self, location: str) -&gt; Transaction:\n        \"\"\"Set the new table location.\n\n        Args:\n            location: The new location of the table.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        raise NotImplementedError(\"Not yet implemented\")\n\n    def commit_transaction(self) -&gt; Table:\n        \"\"\"Commit the changes to the catalog.\n\n        Returns:\n            The table with the updates applied.\n        \"\"\"\n        if len(self._updates) &gt; 0:\n            self._requirements += (AssertTableUUID(uuid=self.table_metadata.table_uuid),)\n            self._table._do_commit(  # pylint: disable=W0212\n                updates=self._updates,\n                requirements=self._requirements,\n            )\n            return self._table\n        else:\n            return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__enter__","title":"<code>__enter__()</code>","text":"<p>Start a transaction to update the table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __enter__(self) -&gt; Transaction:\n    \"\"\"Start a transaction to update the table.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Close and commit the transaction if no exceptions have been raised.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __exit__(\n    self, exctype: Optional[Type[BaseException]], excinst: Optional[BaseException], exctb: Optional[TracebackType]\n) -&gt; None:\n    \"\"\"Close and commit the transaction if no exceptions have been raised.\"\"\"\n    if exctype is None and excinst is None and exctb is None:\n        self.commit_transaction()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__init__","title":"<code>__init__(table, autocommit=False)</code>","text":"<p>Open a transaction to stage and commit changes to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table that will be altered.</p> required <code>autocommit</code> <code>bool</code> <p>Option to automatically commit the changes when they are staged.</p> <code>False</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __init__(self, table: Table, autocommit: bool = False):\n    \"\"\"Open a transaction to stage and commit changes to a table.\n\n    Args:\n        table: The table that will be altered.\n        autocommit: Option to automatically commit the changes when they are staged.\n    \"\"\"\n    self.table_metadata = table.metadata\n    self._table = table\n    self._autocommit = autocommit\n    self._updates = ()\n    self._requirements = ()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction._append_snapshot_producer","title":"<code>_append_snapshot_producer(snapshot_properties)</code>","text":"<p>Determine the append type based on table properties.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> required <p>Returns:     Either a fast-append or a merge-append snapshot producer.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _append_snapshot_producer(self, snapshot_properties: Dict[str, str]) -&gt; _FastAppendFiles:\n    \"\"\"Determine the append type based on table properties.\n\n    Args:\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    Returns:\n        Either a fast-append or a merge-append snapshot producer.\n    \"\"\"\n    manifest_merge_enabled = property_as_bool(\n        self.table_metadata.properties,\n        TableProperties.MANIFEST_MERGE_ENABLED,\n        TableProperties.MANIFEST_MERGE_ENABLED_DEFAULT,\n    )\n    update_snapshot = self.update_snapshot(snapshot_properties=snapshot_properties)\n    return update_snapshot.merge_append() if manifest_merge_enabled else update_snapshot.fast_append()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction._apply","title":"<code>_apply(updates, requirements=())</code>","text":"<p>Check if the requirements are met, and applies the updates to the metadata.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _apply(self, updates: Tuple[TableUpdate, ...], requirements: Tuple[TableRequirement, ...] = ()) -&gt; Transaction:\n    \"\"\"Check if the requirements are met, and applies the updates to the metadata.\"\"\"\n    for requirement in requirements:\n        requirement.validate(self.table_metadata)\n\n    self._updates += updates\n\n    # For the requirements, it does not make sense to add a requirement more than once\n    # For example, you cannot assert that the current schema has two different IDs\n    existing_requirements = {type(requirement) for requirement in self._requirements}\n    for new_requirement in requirements:\n        if type(new_requirement) not in existing_requirements:\n            self._requirements = self._requirements + (new_requirement,)\n\n    self.table_metadata = update_table_metadata(self.table_metadata, updates)\n\n    if self._autocommit:\n        self.commit_transaction()\n        self._updates = ()\n        self._requirements = ()\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction._build_partition_predicate","title":"<code>_build_partition_predicate(partition_records)</code>","text":"<p>Build a filter predicate matching any of the input partition records.</p> <p>Parameters:</p> Name Type Description Default <code>partition_records</code> <code>Set[Record]</code> <p>A set of partition records to match</p> required <p>Returns:     A predicate matching any of the input partition records.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _build_partition_predicate(self, partition_records: Set[Record]) -&gt; BooleanExpression:\n    \"\"\"Build a filter predicate matching any of the input partition records.\n\n    Args:\n        partition_records: A set of partition records to match\n    Returns:\n        A predicate matching any of the input partition records.\n    \"\"\"\n    partition_spec = self.table_metadata.spec()\n    schema = self.table_metadata.schema()\n    partition_fields = [schema.find_field(field.source_id).name for field in partition_spec.fields]\n\n    expr: BooleanExpression = AlwaysFalse()\n    for partition_record in partition_records:\n        match_partition_expression: BooleanExpression = AlwaysTrue()\n\n        for pos, partition_field in enumerate(partition_fields):\n            predicate = (\n                EqualTo(Reference(partition_field), partition_record[pos])\n                if partition_record[pos] is not None\n                else IsNull(Reference(partition_field))\n            )\n            match_partition_expression = And(match_partition_expression, predicate)\n        expr = Or(expr, match_partition_expression)\n    return expr\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction._scan","title":"<code>_scan(row_filter=ALWAYS_TRUE, case_sensitive=True)</code>","text":"<p>Minimal data scan of the table with the current state of the transaction.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _scan(self, row_filter: Union[str, BooleanExpression] = ALWAYS_TRUE, case_sensitive: bool = True) -&gt; DataScan:\n    \"\"\"Minimal data scan of the table with the current state of the transaction.\"\"\"\n    return DataScan(\n        table_metadata=self.table_metadata, io=self._table.io, row_filter=row_filter, case_sensitive=case_sensitive\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction._set_ref_snapshot","title":"<code>_set_ref_snapshot(snapshot_id, ref_name, type, max_ref_age_ms=None, max_snapshot_age_ms=None, min_snapshots_to_keep=None)</code>","text":"<p>Update a ref to a snapshot.</p> <p>Returns:</p> Type Description <code>UpdatesAndRequirements</code> <p>The updates and requirements for the set-snapshot-ref staged</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _set_ref_snapshot(\n    self,\n    snapshot_id: int,\n    ref_name: str,\n    type: str,\n    max_ref_age_ms: Optional[int] = None,\n    max_snapshot_age_ms: Optional[int] = None,\n    min_snapshots_to_keep: Optional[int] = None,\n) -&gt; UpdatesAndRequirements:\n    \"\"\"Update a ref to a snapshot.\n\n    Returns:\n        The updates and requirements for the set-snapshot-ref staged\n    \"\"\"\n    updates = (\n        SetSnapshotRefUpdate(\n            snapshot_id=snapshot_id,\n            ref_name=ref_name,\n            type=type,\n            max_ref_age_ms=max_ref_age_ms,\n            max_snapshot_age_ms=max_snapshot_age_ms,\n            min_snapshots_to_keep=min_snapshots_to_keep,\n        ),\n    )\n    requirements = (\n        AssertRefSnapshotId(\n            snapshot_id=self.table_metadata.refs[ref_name].snapshot_id if ref_name in self.table_metadata.refs else None,\n            ref=ref_name,\n        ),\n    )\n\n    return updates, requirements\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.add_files","title":"<code>add_files(file_paths, snapshot_properties=EMPTY_DICT, check_duplicate_files=True)</code>","text":"<p>Shorthand API for adding files as data files to the table transaction.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>The list of full file paths to be added as data files to the table</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>Raises a ValueError given file_paths contains duplicate files</p> <code>ValueError</code> <p>Raises a ValueError given file_paths already referenced by table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def add_files(\n    self, file_paths: List[str], snapshot_properties: Dict[str, str] = EMPTY_DICT, check_duplicate_files: bool = True\n) -&gt; None:\n    \"\"\"\n    Shorthand API for adding files as data files to the table transaction.\n\n    Args:\n        file_paths: The list of full file paths to be added as data files to the table\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: Raises a ValueError given file_paths contains duplicate files\n        ValueError: Raises a ValueError given file_paths already referenced by table\n    \"\"\"\n    if len(file_paths) != len(set(file_paths)):\n        raise ValueError(\"File paths must be unique\")\n\n    if check_duplicate_files:\n        import pyarrow.compute as pc\n\n        expr = pc.field(\"file_path\").isin(file_paths)\n        referenced_files = [file[\"file_path\"] for file in self._table.inspect.files().filter(expr).to_pylist()]\n\n        if referenced_files:\n            raise ValueError(f\"Cannot add files that are already referenced by table, files: {', '.join(referenced_files)}\")\n\n    if self.table_metadata.name_mapping() is None:\n        self.set_properties(\n            **{TableProperties.DEFAULT_NAME_MAPPING: self.table_metadata.schema().name_mapping.model_dump_json()}\n        )\n    with self.update_snapshot(snapshot_properties=snapshot_properties).fast_append() as update_snapshot:\n        data_files = _parquet_files_to_data_files(\n            table_metadata=self.table_metadata, file_paths=file_paths, io=self._table.io\n        )\n        for data_file in data_files:\n            update_snapshot.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.append","title":"<code>append(df, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand API for appending a PyArrow table to a table transaction.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be appended to overwrite the table</p> required <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def append(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n    \"\"\"\n    Shorthand API for appending a PyArrow table to a table transaction.\n\n    Args:\n        df: The Arrow dataframe that will be appended to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    if unsupported_partitions := [\n        field for field in self.table_metadata.spec().fields if not field.transform.supports_pyarrow_transform\n    ]:\n        raise ValueError(\n            f\"Not all partition types are supported for writes. Following partitions cannot be written using pyarrow: {unsupported_partitions}.\"\n        )\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n    )\n\n    with self._append_snapshot_producer(snapshot_properties) as append_files:\n        # skip writing data files if the dataframe is empty\n        if df.shape[0] &gt; 0:\n            data_files = list(\n                _dataframe_to_data_files(\n                    table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                )\n            )\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.commit_transaction","title":"<code>commit_transaction()</code>","text":"<p>Commit the changes to the catalog.</p> <p>Returns:</p> Type Description <code>Table</code> <p>The table with the updates applied.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def commit_transaction(self) -&gt; Table:\n    \"\"\"Commit the changes to the catalog.\n\n    Returns:\n        The table with the updates applied.\n    \"\"\"\n    if len(self._updates) &gt; 0:\n        self._requirements += (AssertTableUUID(uuid=self.table_metadata.table_uuid),)\n        self._table._do_commit(  # pylint: disable=W0212\n            updates=self._updates,\n            requirements=self._requirements,\n        )\n        return self._table\n    else:\n        return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.delete","title":"<code>delete(delete_filter, snapshot_properties=EMPTY_DICT, case_sensitive=True)</code>","text":"<p>Shorthand for deleting record from a table.</p> <p>A delete may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- REPLACE: In case existing Parquet files need to be rewritten\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>delete_filter</code> <code>Union[str, BooleanExpression]</code> <p>A boolean expression to delete rows from a table</p> required <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>delete_filter</code> is case-sensitive</p> <code>True</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def delete(\n    self,\n    delete_filter: Union[str, BooleanExpression],\n    snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n) -&gt; None:\n    \"\"\"\n    Shorthand for deleting record from a table.\n\n    A delete may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - REPLACE: In case existing Parquet files need to be rewritten\n\n    Args:\n        delete_filter: A boolean expression to delete rows from a table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n    \"\"\"\n    from pyiceberg.io.pyarrow import (\n        ArrowScan,\n        _dataframe_to_data_files,\n        _expression_to_complementary_pyarrow,\n    )\n\n    if (\n        self.table_metadata.properties.get(TableProperties.DELETE_MODE, TableProperties.DELETE_MODE_DEFAULT)\n        == TableProperties.DELETE_MODE_MERGE_ON_READ\n    ):\n        warnings.warn(\"Merge on read is not yet supported, falling back to copy-on-write\")\n\n    if isinstance(delete_filter, str):\n        delete_filter = _parse_row_filter(delete_filter)\n\n    with self.update_snapshot(snapshot_properties=snapshot_properties).delete() as delete_snapshot:\n        delete_snapshot.delete_by_predicate(delete_filter, case_sensitive)\n\n    # Check if there are any files that require an actual rewrite of a data file\n    if delete_snapshot.rewrites_needed is True:\n        bound_delete_filter = bind(self.table_metadata.schema(), delete_filter, case_sensitive)\n        preserve_row_filter = _expression_to_complementary_pyarrow(bound_delete_filter)\n\n        files = self._scan(row_filter=delete_filter, case_sensitive=case_sensitive).plan_files()\n\n        commit_uuid = uuid.uuid4()\n        counter = itertools.count(0)\n\n        replaced_files: List[Tuple[DataFile, List[DataFile]]] = []\n        # This will load the Parquet file into memory, including:\n        #   - Filter out the rows based on the delete filter\n        #   - Projecting it to the current schema\n        #   - Applying the positional deletes if they are there\n        # When writing\n        #   - Apply the latest partition-spec\n        #   - And sort order when added\n        for original_file in files:\n            df = ArrowScan(\n                table_metadata=self.table_metadata,\n                io=self._table.io,\n                projected_schema=self.table_metadata.schema(),\n                row_filter=AlwaysTrue(),\n            ).to_table(tasks=[original_file])\n            filtered_df = df.filter(preserve_row_filter)\n\n            # Only rewrite if there are records being deleted\n            if len(filtered_df) == 0:\n                replaced_files.append((original_file.file, []))\n            elif len(df) != len(filtered_df):\n                replaced_files.append(\n                    (\n                        original_file.file,\n                        list(\n                            _dataframe_to_data_files(\n                                io=self._table.io,\n                                df=filtered_df,\n                                table_metadata=self.table_metadata,\n                                write_uuid=commit_uuid,\n                                counter=counter,\n                            )\n                        ),\n                    )\n                )\n\n        if len(replaced_files) &gt; 0:\n            with self.update_snapshot(snapshot_properties=snapshot_properties).overwrite() as overwrite_snapshot:\n                overwrite_snapshot.commit_uuid = commit_uuid\n                for original_data_file, replaced_data_files in replaced_files:\n                    overwrite_snapshot.delete_data_file(original_data_file)\n                    for replaced_data_file in replaced_data_files:\n                        overwrite_snapshot.append_data_file(replaced_data_file)\n\n    if not delete_snapshot.files_affected and not delete_snapshot.rewrites_needed:\n        warnings.warn(\"Delete operation did not match any records\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.dynamic_partition_overwrite","title":"<code>dynamic_partition_overwrite(df, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand for overwriting existing partitions with a PyArrow table.</p> <p>The function detects partition values in the provided arrow table using the current partition spec, and deletes existing partitions matching these values. Finally, the data in the table is appended to the table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def dynamic_partition_overwrite(self, df: pa.Table, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; None:\n    \"\"\"\n    Shorthand for overwriting existing partitions with a PyArrow table.\n\n    The function detects partition values in the provided arrow table using the current\n    partition spec, and deletes existing partitions matching these values. Finally, the\n    data in the table is appended to the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    if self.table_metadata.spec().is_unpartitioned():\n        raise ValueError(\"Cannot apply dynamic overwrite on an unpartitioned table.\")\n\n    for field in self.table_metadata.spec().fields:\n        if not isinstance(field.transform, IdentityTransform):\n            raise ValueError(\n                f\"For now dynamic overwrite does not support a table with non-identity-transform field in the latest partition spec: {field}\"\n            )\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n    )\n\n    # If dataframe does not have data, there is no need to overwrite\n    if df.shape[0] == 0:\n        return\n\n    append_snapshot_commit_uuid = uuid.uuid4()\n    data_files: List[DataFile] = list(\n        _dataframe_to_data_files(\n            table_metadata=self._table.metadata, write_uuid=append_snapshot_commit_uuid, df=df, io=self._table.io\n        )\n    )\n\n    partitions_to_overwrite = {data_file.partition for data_file in data_files}\n    delete_filter = self._build_partition_predicate(partition_records=partitions_to_overwrite)\n    self.delete(delete_filter=delete_filter, snapshot_properties=snapshot_properties)\n\n    with self._append_snapshot_producer(snapshot_properties) as append_files:\n        append_files.commit_uuid = append_snapshot_commit_uuid\n        for data_file in data_files:\n            append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.overwrite","title":"<code>overwrite(df, overwrite_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True)</code>","text":"<p>Shorthand for adding a table overwrite with a PyArrow table to the transaction.</p> <p>An overwrite may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- REPLACE: In case existing Parquet files need to be rewritten.\n- APPEND: In case new data is being inserted into the table.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>overwrite_filter</code> <code>Union[BooleanExpression, str]</code> <p>ALWAYS_TRUE when you overwrite all the data,               or a boolean expression in case of a partial overwrite</p> <code>ALWAYS_TRUE</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>overwrite_filter</code> is case-sensitive</p> <code>True</code> <code>snapshot_properties</code> <code>Dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def overwrite(\n    self,\n    df: pa.Table,\n    overwrite_filter: Union[BooleanExpression, str] = ALWAYS_TRUE,\n    snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n) -&gt; None:\n    \"\"\"\n    Shorthand for adding a table overwrite with a PyArrow table to the transaction.\n\n    An overwrite may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - REPLACE: In case existing Parquet files need to be rewritten.\n        - APPEND: In case new data is being inserted into the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                          or a boolean expression in case of a partial overwrite\n        case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n        snapshot_properties: Custom properties to be added to the snapshot summary\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    if unsupported_partitions := [\n        field for field in self.table_metadata.spec().fields if not field.transform.supports_pyarrow_transform\n    ]:\n        raise ValueError(\n            f\"Not all partition types are supported for writes. Following partitions cannot be written using pyarrow: {unsupported_partitions}.\"\n        )\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(), provided_schema=df.schema, downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us\n    )\n\n    if overwrite_filter != AlwaysFalse():\n        # Only delete when the filter is != AlwaysFalse\n        self.delete(delete_filter=overwrite_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties)\n\n    with self._append_snapshot_producer(snapshot_properties) as append_files:\n        # skip writing data files if the dataframe is empty\n        if df.shape[0] &gt; 0:\n            data_files = _dataframe_to_data_files(\n                table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n            )\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.remove_properties","title":"<code>remove_properties(*removals)</code>","text":"<p>Remove properties.</p> <p>Parameters:</p> Name Type Description Default <code>removals</code> <code>str</code> <p>Properties to be removed.</p> <code>()</code> <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def remove_properties(self, *removals: str) -&gt; Transaction:\n    \"\"\"Remove properties.\n\n    Args:\n        removals: Properties to be removed.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    return self._apply((RemovePropertiesUpdate(removals=removals),))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.set_properties","title":"<code>set_properties(properties=EMPTY_DICT, **kwargs)</code>","text":"<p>Set properties.</p> <p>When a property is already set, it will be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Properties</code> <p>The properties set on the table.</p> <code>EMPTY_DICT</code> <code>kwargs</code> <code>Any</code> <p>properties can also be pass as kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def set_properties(self, properties: Properties = EMPTY_DICT, **kwargs: Any) -&gt; Transaction:\n    \"\"\"Set properties.\n\n    When a property is already set, it will be overwritten.\n\n    Args:\n        properties: The properties set on the table.\n        kwargs: properties can also be pass as kwargs.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    if properties and kwargs:\n        raise ValueError(\"Cannot pass both properties and kwargs\")\n    updates = properties or kwargs\n    return self._apply((SetPropertiesUpdate(updates=updates),))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_location","title":"<code>update_location(location)</code>","text":"<p>Set the new table location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The new location of the table.</p> required <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_location(self, location: str) -&gt; Transaction:\n    \"\"\"Set the new table location.\n\n    Args:\n        location: The new location of the table.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    raise NotImplementedError(\"Not yet implemented\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_schema","title":"<code>update_schema(allow_incompatible_changes=False, case_sensitive=True)</code>","text":"<p>Create a new UpdateSchema to alter the columns of this table.</p> <p>Parameters:</p> Name Type Description Default <code>allow_incompatible_changes</code> <code>bool</code> <p>If changes are allowed that might break downstream consumers.</p> <code>False</code> <code>case_sensitive</code> <code>bool</code> <p>If field names are case-sensitive.</p> <code>True</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>A new UpdateSchema.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n    \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n    Args:\n        allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n        case_sensitive: If field names are case-sensitive.\n\n    Returns:\n        A new UpdateSchema.\n    \"\"\"\n    return UpdateSchema(\n        self,\n        allow_incompatible_changes=allow_incompatible_changes,\n        case_sensitive=case_sensitive,\n        name_mapping=self.table_metadata.name_mapping(),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_snapshot","title":"<code>update_snapshot(snapshot_properties=EMPTY_DICT)</code>","text":"<p>Create a new UpdateSnapshot to produce a new snapshot for the table.</p> <p>Returns:</p> Type Description <code>UpdateSnapshot</code> <p>A new UpdateSnapshot</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_snapshot(self, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; UpdateSnapshot:\n    \"\"\"Create a new UpdateSnapshot to produce a new snapshot for the table.\n\n    Returns:\n        A new UpdateSnapshot\n    \"\"\"\n    return UpdateSnapshot(self, io=self._table.io, snapshot_properties=snapshot_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_spec","title":"<code>update_spec()</code>","text":"<p>Create a new UpdateSpec to update the partitioning of the table.</p> <p>Returns:</p> Type Description <code>UpdateSpec</code> <p>A new UpdateSpec.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_spec(self) -&gt; UpdateSpec:\n    \"\"\"Create a new UpdateSpec to update the partitioning of the table.\n\n    Returns:\n        A new UpdateSpec.\n    \"\"\"\n    return UpdateSpec(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.upgrade_table_version","title":"<code>upgrade_table_version(format_version)</code>","text":"<p>Set the table to a certain version.</p> <p>Parameters:</p> Name Type Description Default <code>format_version</code> <code>TableVersion</code> <p>The newly set version.</p> required <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def upgrade_table_version(self, format_version: TableVersion) -&gt; Transaction:\n    \"\"\"Set the table to a certain version.\n\n    Args:\n        format_version: The newly set version.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    if format_version not in {1, 2}:\n        raise ValueError(f\"Unsupported table format version: {format_version}\")\n\n    if format_version &lt; self.table_metadata.format_version:\n        raise ValueError(f\"Cannot downgrade v{self.table_metadata.format_version} table to v{format_version}\")\n\n    if format_version &gt; self.table_metadata.format_version:\n        return self._apply((UpgradeFormatVersionUpdate(format_version=format_version),))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.UpsertResult","title":"<code>UpsertResult</code>  <code>dataclass</code>","text":"<p>Summary the upsert operation.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass()\nclass UpsertResult:\n    \"\"\"Summary the upsert operation.\"\"\"\n\n    rows_updated: int = 0\n    rows_inserted: int = 0\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.WriteTask","title":"<code>WriteTask</code>  <code>dataclass</code>","text":"<p>Task with the parameters for writing a DataFile.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass(frozen=True)\nclass WriteTask:\n    \"\"\"Task with the parameters for writing a DataFile.\"\"\"\n\n    write_uuid: uuid.UUID\n    task_id: int\n    schema: Schema\n    record_batches: List[pa.RecordBatch]\n    sort_order_id: Optional[int] = None\n    partition_key: Optional[PartitionKey] = None\n\n    def generate_data_file_filename(self, extension: str) -&gt; str:\n        # Mimics the behavior in the Java API:\n        # https://github.com/apache/iceberg/blob/a582968975dd30ff4917fbbe999f1be903efac02/core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java#L92-L101\n        return f\"00000-{self.task_id}-{self.write_uuid}.{extension}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table._match_deletes_to_data_file","title":"<code>_match_deletes_to_data_file(data_entry, positional_delete_entries)</code>","text":"<p>Check if the delete file is relevant for the data file.</p> <p>Using the column metrics to see if the filename is in the lower and upper bound.</p> <p>Parameters:</p> Name Type Description Default <code>data_entry</code> <code>ManifestEntry</code> <p>The manifest entry path of the datafile.</p> required <code>positional_delete_entries</code> <code>List[ManifestEntry]</code> <p>All the candidate positional deletes manifest entries.</p> required <p>Returns:</p> Type Description <code>Set[DataFile]</code> <p>A set of files that are relevant for the data file.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _match_deletes_to_data_file(data_entry: ManifestEntry, positional_delete_entries: SortedList[ManifestEntry]) -&gt; Set[DataFile]:\n    \"\"\"Check if the delete file is relevant for the data file.\n\n    Using the column metrics to see if the filename is in the lower and upper bound.\n\n    Args:\n        data_entry (ManifestEntry): The manifest entry path of the datafile.\n        positional_delete_entries (List[ManifestEntry]): All the candidate positional deletes manifest entries.\n\n    Returns:\n        A set of files that are relevant for the data file.\n    \"\"\"\n    relevant_entries = positional_delete_entries[positional_delete_entries.bisect_right(data_entry) :]\n\n    if len(relevant_entries) &gt; 0:\n        evaluator = _InclusiveMetricsEvaluator(POSITIONAL_DELETE_SCHEMA, EqualTo(\"file_path\", data_entry.data_file.file_path))\n        return {\n            positional_delete_entry.data_file\n            for positional_delete_entry in relevant_entries\n            if evaluator.eval(positional_delete_entry.data_file)\n        }\n    else:\n        return set()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table._open_manifest","title":"<code>_open_manifest(io, manifest, partition_filter, residual_evaluator, metrics_evaluator)</code>","text":"<p>Open a manifest file and return matching manifest entries.</p> <p>Returns:</p> Type Description <code>List[ManifestEntry]</code> <p>A list of ManifestEntry that matches the provided filters.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _open_manifest(\n    io: FileIO,\n    manifest: ManifestFile,\n    partition_filter: Callable[[DataFile], bool],\n    residual_evaluator: Callable[[Record], BooleanExpression],\n    metrics_evaluator: Callable[[DataFile], bool],\n) -&gt; List[ManifestEntry]:\n    \"\"\"Open a manifest file and return matching manifest entries.\n\n    Returns:\n        A list of ManifestEntry that matches the provided filters.\n    \"\"\"\n    return [\n        manifest_entry\n        for manifest_entry in manifest.fetch_manifest_entry(io, discard_deleted=True)\n        if partition_filter(manifest_entry.data_file) and metrics_evaluator(manifest_entry.data_file)\n    ]\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table._parquet_files_to_data_files","title":"<code>_parquet_files_to_data_files(table_metadata, file_paths, io)</code>","text":"<p>Convert a list files into DataFiles.</p> <p>Returns:</p> Type Description <code>Iterable[DataFile]</code> <p>An iterable that supplies DataFiles that describe the parquet files.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _parquet_files_to_data_files(table_metadata: TableMetadata, file_paths: List[str], io: FileIO) -&gt; Iterable[DataFile]:\n    \"\"\"Convert a list files into DataFiles.\n\n    Returns:\n        An iterable that supplies DataFiles that describe the parquet files.\n    \"\"\"\n    from pyiceberg.io.pyarrow import parquet_files_to_data_files\n\n    yield from parquet_files_to_data_files(io=io, table_metadata=table_metadata, file_paths=iter(file_paths))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table._parse_row_filter","title":"<code>_parse_row_filter(expr)</code>","text":"<p>Accept an expression in the form of a BooleanExpression or a string.</p> <p>In the case of a string, it will be converted into a unbound BooleanExpression.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>Union[str, BooleanExpression]</code> <p>Expression as a BooleanExpression or a string.</p> required <p>Returns: An unbound BooleanExpression.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def _parse_row_filter(expr: Union[str, BooleanExpression]) -&gt; BooleanExpression:\n    \"\"\"Accept an expression in the form of a BooleanExpression or a string.\n\n    In the case of a string, it will be converted into a unbound BooleanExpression.\n\n    Args:\n        expr: Expression as a BooleanExpression or a string.\n\n    Returns: An unbound BooleanExpression.\n    \"\"\"\n    return parser.parse(expr) if isinstance(expr, str) else expr\n</code></pre>"},{"location":"reference/pyiceberg/table/inspect/","title":"inspect","text":""},{"location":"reference/pyiceberg/table/locations/","title":"locations","text":""},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider","title":"<code>LocationProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for location providers, that provide file locations for a table's write tasks.</p> <p>Parameters:</p> Name Type Description Default <code>table_location</code> <code>str</code> <p>The table's base storage location.</p> required <code>table_properties</code> <code>Properties</code> <p>The table's properties.</p> required Source code in <code>pyiceberg/table/locations.py</code> <pre><code>class LocationProvider(ABC):\n    \"\"\"A base class for location providers, that provide file locations for a table's write tasks.\n\n    Args:\n        table_location (str): The table's base storage location.\n        table_properties (Properties): The table's properties.\n    \"\"\"\n\n    table_location: str\n    table_properties: Properties\n\n    data_path: str\n    metadata_path: str\n\n    def __init__(self, table_location: str, table_properties: Properties):\n        self.table_location = table_location\n        self.table_properties = table_properties\n\n        from pyiceberg.table import TableProperties\n\n        if path := table_properties.get(TableProperties.WRITE_DATA_PATH):\n            self.data_path = path.rstrip(\"/\")\n        else:\n            self.data_path = f\"{self.table_location.rstrip('/')}/data\"\n\n        if path := table_properties.get(TableProperties.WRITE_METADATA_PATH):\n            self.metadata_path = path.rstrip(\"/\")\n        else:\n            self.metadata_path = f\"{self.table_location.rstrip('/')}/metadata\"\n\n    @abstractmethod\n    def new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -&gt; str:\n        \"\"\"Return a fully-qualified data file location for the given filename.\n\n        Args:\n            data_file_name (str): The name of the data file.\n            partition_key (Optional[PartitionKey]): The data file's partition key. If None, the data is not partitioned.\n\n        Returns:\n            str: A fully-qualified location URI for the data file.\n        \"\"\"\n\n    def new_table_metadata_file_location(self, new_version: int = 0) -&gt; str:\n        \"\"\"Return a fully-qualified metadata file location for a new table version.\n\n        Args:\n            new_version (int): Version number of the metadata file.\n\n        Returns:\n            str: fully-qualified URI for the new table metadata file.\n\n        Raises:\n            ValueError: If the version is negative.\n        \"\"\"\n        if new_version &lt; 0:\n            raise ValueError(f\"Table metadata version: `{new_version}` must be a non-negative integer\")\n\n        file_name = f\"{new_version:05d}-{uuid.uuid4()}.metadata.json\"\n        return self.new_metadata_location(file_name)\n\n    def new_metadata_location(self, metadata_file_name: str) -&gt; str:\n        \"\"\"Return a fully-qualified metadata file location for the given filename.\n\n        Args:\n            metadata_file_name (str): Name of the metadata file.\n\n        Returns:\n            str: A fully-qualified location URI for the metadata file.\n        \"\"\"\n        return f\"{self.metadata_path}/{metadata_file_name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_data_location","title":"<code>new_data_location(data_file_name, partition_key=None)</code>  <code>abstractmethod</code>","text":"<p>Return a fully-qualified data file location for the given filename.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>The name of the data file.</p> required <code>partition_key</code> <code>Optional[PartitionKey]</code> <p>The data file's partition key. If None, the data is not partitioned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A fully-qualified location URI for the data file.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>@abstractmethod\ndef new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -&gt; str:\n    \"\"\"Return a fully-qualified data file location for the given filename.\n\n    Args:\n        data_file_name (str): The name of the data file.\n        partition_key (Optional[PartitionKey]): The data file's partition key. If None, the data is not partitioned.\n\n    Returns:\n        str: A fully-qualified location URI for the data file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_metadata_location","title":"<code>new_metadata_location(metadata_file_name)</code>","text":"<p>Return a fully-qualified metadata file location for the given filename.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_file_name</code> <code>str</code> <p>Name of the metadata file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A fully-qualified location URI for the metadata file.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>def new_metadata_location(self, metadata_file_name: str) -&gt; str:\n    \"\"\"Return a fully-qualified metadata file location for the given filename.\n\n    Args:\n        metadata_file_name (str): Name of the metadata file.\n\n    Returns:\n        str: A fully-qualified location URI for the metadata file.\n    \"\"\"\n    return f\"{self.metadata_path}/{metadata_file_name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_table_metadata_file_location","title":"<code>new_table_metadata_file_location(new_version=0)</code>","text":"<p>Return a fully-qualified metadata file location for a new table version.</p> <p>Parameters:</p> Name Type Description Default <code>new_version</code> <code>int</code> <p>Version number of the metadata file.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>fully-qualified URI for the new table metadata file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version is negative.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>def new_table_metadata_file_location(self, new_version: int = 0) -&gt; str:\n    \"\"\"Return a fully-qualified metadata file location for a new table version.\n\n    Args:\n        new_version (int): Version number of the metadata file.\n\n    Returns:\n        str: fully-qualified URI for the new table metadata file.\n\n    Raises:\n        ValueError: If the version is negative.\n    \"\"\"\n    if new_version &lt; 0:\n        raise ValueError(f\"Table metadata version: `{new_version}` must be a non-negative integer\")\n\n    file_name = f\"{new_version:05d}-{uuid.uuid4()}.metadata.json\"\n    return self.new_metadata_location(file_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.ObjectStoreLocationProvider","title":"<code>ObjectStoreLocationProvider</code>","text":"<p>               Bases: <code>LocationProvider</code></p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>class ObjectStoreLocationProvider(LocationProvider):\n    HASH_BINARY_STRING_BITS = 20\n    ENTROPY_DIR_LENGTH = 4\n    ENTROPY_DIR_DEPTH = 3\n\n    _include_partition_paths: bool\n\n    def __init__(self, table_location: str, table_properties: Properties):\n        super().__init__(table_location, table_properties)\n        from pyiceberg.table import TableProperties\n\n        self._include_partition_paths = property_as_bool(\n            self.table_properties,\n            TableProperties.WRITE_OBJECT_STORE_PARTITIONED_PATHS,\n            TableProperties.WRITE_OBJECT_STORE_PARTITIONED_PATHS_DEFAULT,\n        )\n\n    def new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -&gt; str:\n        if self._include_partition_paths and partition_key:\n            return self.new_data_location(f\"{partition_key.to_path()}/{data_file_name}\")\n\n        hashed_path = self._compute_hash(data_file_name)\n\n        return (\n            f\"{self.data_path}/{hashed_path}/{data_file_name}\"\n            if self._include_partition_paths\n            else f\"{self.data_path}/{hashed_path}-{data_file_name}\"\n        )\n\n    @staticmethod\n    def _compute_hash(data_file_name: str) -&gt; str:\n        # Bitwise AND to combat sign-extension; bitwise OR to preserve leading zeroes that `bin` would otherwise strip.\n        top_mask = 1 &lt;&lt; ObjectStoreLocationProvider.HASH_BINARY_STRING_BITS\n        hash_code = mmh3.hash(data_file_name) &amp; (top_mask - 1) | top_mask\n        return ObjectStoreLocationProvider._dirs_from_hash(bin(hash_code)[-ObjectStoreLocationProvider.HASH_BINARY_STRING_BITS :])\n\n    @staticmethod\n    def _dirs_from_hash(file_hash: str) -&gt; str:\n        \"\"\"Divides hash into directories for optimized orphan removal operation using ENTROPY_DIR_DEPTH and ENTROPY_DIR_LENGTH.\"\"\"\n        total_entropy_length = ObjectStoreLocationProvider.ENTROPY_DIR_DEPTH * ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH\n\n        hash_with_dirs = []\n        for i in range(0, total_entropy_length, ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH):\n            hash_with_dirs.append(file_hash[i : i + ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH])\n\n        if len(file_hash) &gt; total_entropy_length:\n            hash_with_dirs.append(file_hash[total_entropy_length:])\n\n        return \"/\".join(hash_with_dirs)\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.ObjectStoreLocationProvider._dirs_from_hash","title":"<code>_dirs_from_hash(file_hash)</code>  <code>staticmethod</code>","text":"<p>Divides hash into directories for optimized orphan removal operation using ENTROPY_DIR_DEPTH and ENTROPY_DIR_LENGTH.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>@staticmethod\ndef _dirs_from_hash(file_hash: str) -&gt; str:\n    \"\"\"Divides hash into directories for optimized orphan removal operation using ENTROPY_DIR_DEPTH and ENTROPY_DIR_LENGTH.\"\"\"\n    total_entropy_length = ObjectStoreLocationProvider.ENTROPY_DIR_DEPTH * ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH\n\n    hash_with_dirs = []\n    for i in range(0, total_entropy_length, ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH):\n        hash_with_dirs.append(file_hash[i : i + ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH])\n\n    if len(file_hash) &gt; total_entropy_length:\n        hash_with_dirs.append(file_hash[total_entropy_length:])\n\n    return \"/\".join(hash_with_dirs)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/","title":"metadata","text":""},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields","title":"<code>TableMetadataCommonFields</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Metadata for an Iceberg table as specified in the Apache Iceberg spec.</p> <p>https://iceberg.apache.org/spec/#iceberg-table-spec</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataCommonFields(IcebergBaseModel):\n    \"\"\"Metadata for an Iceberg table as specified in the Apache Iceberg spec.\n\n    https://iceberg.apache.org/spec/#iceberg-table-spec\n    \"\"\"\n\n    location: str = Field()\n    \"\"\"The table\u2019s base location. This is used by writers to determine where\n    to store data files, manifest files, and table metadata files.\"\"\"\n\n    table_uuid: uuid.UUID = Field(alias=\"table-uuid\", default_factory=uuid.uuid4)\n    \"\"\"A UUID that identifies the table, generated when the table is created.\n    Implementations must throw an exception if a table\u2019s UUID does not match\n    the expected UUID after refreshing metadata.\"\"\"\n\n    last_updated_ms: int = Field(\n        alias=\"last-updated-ms\", default_factory=lambda: datetime_to_millis(datetime.datetime.now().astimezone())\n    )\n    \"\"\"Timestamp in milliseconds from the unix epoch when the table\n    was last updated. Each table metadata file should update this\n    field just before writing.\"\"\"\n\n    last_column_id: int = Field(alias=\"last-column-id\")\n    \"\"\"An integer; the highest assigned column ID for the table.\n    This is used to ensure fields are always assigned an unused ID\n    when evolving schemas.\"\"\"\n\n    schemas: List[Schema] = Field(default_factory=list)\n    \"\"\"A list of schemas, stored as objects with schema-id.\"\"\"\n\n    current_schema_id: int = Field(alias=\"current-schema-id\", default=DEFAULT_SCHEMA_ID)\n    \"\"\"ID of the table\u2019s current schema.\"\"\"\n\n    partition_specs: List[PartitionSpec] = Field(alias=\"partition-specs\", default_factory=list)\n    \"\"\"A list of partition specs, stored as full partition spec objects.\"\"\"\n\n    default_spec_id: int = Field(alias=\"default-spec-id\", default=INITIAL_SPEC_ID)\n    \"\"\"ID of the \u201ccurrent\u201d spec that writers should use by default.\"\"\"\n\n    last_partition_id: Optional[int] = Field(alias=\"last-partition-id\", default=None)\n    \"\"\"An integer; the highest assigned partition field ID across all\n    partition specs for the table. This is used to ensure partition fields\n    are always assigned an unused ID when evolving specs.\"\"\"\n\n    properties: Dict[str, str] = Field(default_factory=dict)\n    \"\"\"A string to string map of table properties. This is used to\n    control settings that affect reading and writing and is not intended\n    to be used for arbitrary metadata. For example, commit.retry.num-retries\n    is used to control the number of commit retries.\"\"\"\n\n    current_snapshot_id: Optional[int] = Field(alias=\"current-snapshot-id\", default=None)\n    \"\"\"ID of the current table snapshot.\"\"\"\n\n    snapshots: List[Snapshot] = Field(default_factory=list)\n    \"\"\"A list of valid snapshots. Valid snapshots are snapshots for which\n    all data files exist in the file system. A data file must not be\n    deleted from the file system until the last snapshot in which it was\n    listed is garbage collected.\"\"\"\n\n    snapshot_log: List[SnapshotLogEntry] = Field(alias=\"snapshot-log\", default_factory=list)\n    \"\"\"A list (optional) of timestamp and snapshot ID pairs that encodes\n    changes to the current snapshot for the table. Each time the\n    current-snapshot-id is changed, a new entry should be added with the\n    last-updated-ms and the new current-snapshot-id. When snapshots are\n    expired from the list of valid snapshots, all entries before a snapshot\n    that has expired should be removed.\"\"\"\n\n    metadata_log: List[MetadataLogEntry] = Field(alias=\"metadata-log\", default_factory=list)\n    \"\"\"A list (optional) of timestamp and metadata file location pairs that\n    encodes changes to the previous metadata files for the table. Each time\n    a new metadata file is created, a new entry of the previous metadata\n    file location should be added to the list. Tables can be configured to\n    remove oldest metadata log entries and keep a fixed-size log of the most\n    recent entries after a commit.\"\"\"\n\n    sort_orders: List[SortOrder] = Field(alias=\"sort-orders\", default_factory=list)\n    \"\"\"A list of sort orders, stored as full sort order objects.\"\"\"\n\n    default_sort_order_id: int = Field(alias=\"default-sort-order-id\", default=UNSORTED_SORT_ORDER_ID)\n    \"\"\"Default sort order id of the table. Note that this could be used by\n    writers, but is not used when reading because reads use the specs stored\n     in manifest files.\"\"\"\n\n    refs: Dict[str, SnapshotRef] = Field(default_factory=dict)\n    \"\"\"A map of snapshot references.\n    The map keys are the unique snapshot reference names in the table,\n    and the map values are snapshot reference objects.\n    There is always a main branch reference pointing to the\n    current-snapshot-id even if the refs map is null.\"\"\"\n\n    statistics: List[StatisticsFile] = Field(default_factory=list)\n    \"\"\"A optional list of table statistics files.\n    Table statistics files are valid Puffin files. Statistics are\n    informational. A reader can choose to ignore statistics\n    information. Statistics support is not required to read the\n    table correctly. A table can contain many statistics files\n    associated with different table snapshots.\"\"\"\n\n    # validators\n    @field_validator(\"properties\", mode=\"before\")\n    def transform_properties_dict_value_to_str(cls, properties: Properties) -&gt; Dict[str, str]:\n        return transform_dict_value_to_str(properties)\n\n    def snapshot_by_id(self, snapshot_id: int) -&gt; Optional[Snapshot]:\n        \"\"\"Get the snapshot by snapshot_id.\"\"\"\n        return next((snapshot for snapshot in self.snapshots if snapshot.snapshot_id == snapshot_id), None)\n\n    def schema_by_id(self, schema_id: int) -&gt; Optional[Schema]:\n        \"\"\"Get the schema by schema_id.\"\"\"\n        return next((schema for schema in self.schemas if schema.schema_id == schema_id), None)\n\n    def schema(self) -&gt; Schema:\n        \"\"\"Return the schema for this table.\"\"\"\n        return next(schema for schema in self.schemas if schema.schema_id == self.current_schema_id)\n\n    def name_mapping(self) -&gt; Optional[NameMapping]:\n        \"\"\"Return the table's field-id NameMapping.\"\"\"\n        if name_mapping_json := self.properties.get(\"schema.name-mapping.default\"):\n            return parse_mapping_from_json(name_mapping_json)\n        else:\n            return None\n\n    def spec(self) -&gt; PartitionSpec:\n        \"\"\"Return the partition spec of this table.\"\"\"\n        return next(spec for spec in self.partition_specs if spec.spec_id == self.default_spec_id)\n\n    def specs(self) -&gt; Dict[int, PartitionSpec]:\n        \"\"\"Return a dict the partition specs this table.\"\"\"\n        return {spec.spec_id: spec for spec in self.partition_specs}\n\n    def specs_struct(self) -&gt; StructType:\n        \"\"\"Produce a struct of all the combined PartitionSpecs.\n\n        The partition fields should be optional: Partition fields may be added later,\n        in which case not all files would have the result field, and it may be null.\n\n        :return: A StructType that represents all the combined PartitionSpecs of the table\n        \"\"\"\n        specs = self.specs()\n\n        # Collect all the fields\n        struct_fields = {field.field_id: field for spec in specs.values() for field in spec.fields}\n\n        schema = self.schema()\n\n        nested_fields = []\n        # Sort them by field_id in order to get a deterministic output\n        for field_id in sorted(struct_fields):\n            field = struct_fields[field_id]\n            source_type = schema.find_type(field.source_id)\n            result_type = field.transform.result_type(source_type)\n            nested_fields.append(NestedField(field_id=field.field_id, name=field.name, type=result_type, required=False))\n\n        return StructType(*nested_fields)\n\n    def new_snapshot_id(self) -&gt; int:\n        \"\"\"Generate a new snapshot-id that's not in use.\"\"\"\n        snapshot_id = _generate_snapshot_id()\n        while self.snapshot_by_id(snapshot_id) is not None:\n            snapshot_id = _generate_snapshot_id()\n\n        return snapshot_id\n\n    def snapshot_by_name(self, name: str) -&gt; Optional[Snapshot]:\n        \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n        if ref := self.refs.get(name):\n            return self.snapshot_by_id(ref.snapshot_id)\n        return None\n\n    def current_snapshot(self) -&gt; Optional[Snapshot]:\n        \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n        if self.current_snapshot_id is not None:\n            return self.snapshot_by_id(self.current_snapshot_id)\n        return None\n\n    def next_sequence_number(self) -&gt; int:\n        return self.last_sequence_number + 1 if self.format_version &gt; 1 else INITIAL_SEQUENCE_NUMBER\n\n    def sort_order_by_id(self, sort_order_id: int) -&gt; Optional[SortOrder]:\n        \"\"\"Get the sort order by sort_order_id.\"\"\"\n        return next((sort_order for sort_order in self.sort_orders if sort_order.order_id == sort_order_id), None)\n\n    @field_serializer(\"current_snapshot_id\")\n    def serialize_current_snapshot_id(self, current_snapshot_id: Optional[int]) -&gt; Optional[int]:\n        if current_snapshot_id is None and Config().get_bool(\"legacy-current-snapshot-id\"):\n            return -1\n        return current_snapshot_id\n\n    @field_serializer(\"snapshots\")\n    def serialize_snapshots(self, snapshots: List[Snapshot]) -&gt; List[Snapshot]:\n        # Snapshot field `sequence-number` should not be written for v1 metadata\n        if self.format_version == 1:\n            return [snapshot.model_copy(update={\"sequence_number\": None}) for snapshot in snapshots]\n        return snapshots\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_schema_id","title":"<code>current_schema_id = Field(alias='current-schema-id', default=DEFAULT_SCHEMA_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the table\u2019s current schema.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_snapshot_id","title":"<code>current_snapshot_id = Field(alias='current-snapshot-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the current table snapshot.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.default_sort_order_id","title":"<code>default_sort_order_id = Field(alias='default-sort-order-id', default=UNSORTED_SORT_ORDER_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default sort order id of the table. Note that this could be used by writers, but is not used when reading because reads use the specs stored  in manifest files.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.default_spec_id","title":"<code>default_spec_id = Field(alias='default-spec-id', default=INITIAL_SPEC_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the \u201ccurrent\u201d spec that writers should use by default.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_column_id","title":"<code>last_column_id = Field(alias='last-column-id')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer; the highest assigned column ID for the table. This is used to ensure fields are always assigned an unused ID when evolving schemas.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_partition_id","title":"<code>last_partition_id = Field(alias='last-partition-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer; the highest assigned partition field ID across all partition specs for the table. This is used to ensure partition fields are always assigned an unused ID when evolving specs.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_updated_ms","title":"<code>last_updated_ms = Field(alias='last-updated-ms', default_factory=lambda: datetime_to_millis(datetime.datetime.now().astimezone()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Timestamp in milliseconds from the unix epoch when the table was last updated. Each table metadata file should update this field just before writing.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.location","title":"<code>location = Field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s base location. This is used by writers to determine where to store data files, manifest files, and table metadata files.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.metadata_log","title":"<code>metadata_log = Field(alias='metadata-log', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list (optional) of timestamp and metadata file location pairs that encodes changes to the previous metadata files for the table. Each time a new metadata file is created, a new entry of the previous metadata file location should be added to the list. Tables can be configured to remove oldest metadata log entries and keep a fixed-size log of the most recent entries after a commit.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.partition_specs","title":"<code>partition_specs = Field(alias='partition-specs', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of partition specs, stored as full partition spec objects.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.properties","title":"<code>properties = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A string to string map of table properties. This is used to control settings that affect reading and writing and is not intended to be used for arbitrary metadata. For example, commit.retry.num-retries is used to control the number of commit retries.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.refs","title":"<code>refs = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A map of snapshot references. The map keys are the unique snapshot reference names in the table, and the map values are snapshot reference objects. There is always a main branch reference pointing to the current-snapshot-id even if the refs map is null.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schemas","title":"<code>schemas = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of schemas, stored as objects with schema-id.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_log","title":"<code>snapshot_log = Field(alias='snapshot-log', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list (optional) of timestamp and snapshot ID pairs that encodes changes to the current snapshot for the table. Each time the current-snapshot-id is changed, a new entry should be added with the last-updated-ms and the new current-snapshot-id. When snapshots are expired from the list of valid snapshots, all entries before a snapshot that has expired should be removed.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshots","title":"<code>snapshots = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of valid snapshots. Valid snapshots are snapshots for which all data files exist in the file system. A data file must not be deleted from the file system until the last snapshot in which it was listed is garbage collected.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.sort_orders","title":"<code>sort_orders = Field(alias='sort-orders', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of sort orders, stored as full sort order objects.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.statistics","title":"<code>statistics = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A optional list of table statistics files. Table statistics files are valid Puffin files. Statistics are informational. A reader can choose to ignore statistics information. Statistics support is not required to read the table correctly. A table can contain many statistics files associated with different table snapshots.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.table_uuid","title":"<code>table_uuid = Field(alias='table-uuid', default_factory=uuid.uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A UUID that identifies the table, generated when the table is created. Implementations must throw an exception if a table\u2019s UUID does not match the expected UUID after refreshing metadata.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_snapshot","title":"<code>current_snapshot()</code>","text":"<p>Get the current snapshot for this table, or None if there is no current snapshot.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def current_snapshot(self) -&gt; Optional[Snapshot]:\n    \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n    if self.current_snapshot_id is not None:\n        return self.snapshot_by_id(self.current_snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.name_mapping","title":"<code>name_mapping()</code>","text":"<p>Return the table's field-id NameMapping.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def name_mapping(self) -&gt; Optional[NameMapping]:\n    \"\"\"Return the table's field-id NameMapping.\"\"\"\n    if name_mapping_json := self.properties.get(\"schema.name-mapping.default\"):\n        return parse_mapping_from_json(name_mapping_json)\n    else:\n        return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.new_snapshot_id","title":"<code>new_snapshot_id()</code>","text":"<p>Generate a new snapshot-id that's not in use.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def new_snapshot_id(self) -&gt; int:\n    \"\"\"Generate a new snapshot-id that's not in use.\"\"\"\n    snapshot_id = _generate_snapshot_id()\n    while self.snapshot_by_id(snapshot_id) is not None:\n        snapshot_id = _generate_snapshot_id()\n\n    return snapshot_id\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schema","title":"<code>schema()</code>","text":"<p>Return the schema for this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def schema(self) -&gt; Schema:\n    \"\"\"Return the schema for this table.\"\"\"\n    return next(schema for schema in self.schemas if schema.schema_id == self.current_schema_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schema_by_id","title":"<code>schema_by_id(schema_id)</code>","text":"<p>Get the schema by schema_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def schema_by_id(self, schema_id: int) -&gt; Optional[Schema]:\n    \"\"\"Get the schema by schema_id.\"\"\"\n    return next((schema for schema in self.schemas if schema.schema_id == schema_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_by_id","title":"<code>snapshot_by_id(snapshot_id)</code>","text":"<p>Get the snapshot by snapshot_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def snapshot_by_id(self, snapshot_id: int) -&gt; Optional[Snapshot]:\n    \"\"\"Get the snapshot by snapshot_id.\"\"\"\n    return next((snapshot for snapshot in self.snapshots if snapshot.snapshot_id == snapshot_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_by_name","title":"<code>snapshot_by_name(name)</code>","text":"<p>Return the snapshot referenced by the given name or null if no such reference exists.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def snapshot_by_name(self, name: str) -&gt; Optional[Snapshot]:\n    \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n    if ref := self.refs.get(name):\n        return self.snapshot_by_id(ref.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.sort_order_by_id","title":"<code>sort_order_by_id(sort_order_id)</code>","text":"<p>Get the sort order by sort_order_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def sort_order_by_id(self, sort_order_id: int) -&gt; Optional[SortOrder]:\n    \"\"\"Get the sort order by sort_order_id.\"\"\"\n    return next((sort_order for sort_order in self.sort_orders if sort_order.order_id == sort_order_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.spec","title":"<code>spec()</code>","text":"<p>Return the partition spec of this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def spec(self) -&gt; PartitionSpec:\n    \"\"\"Return the partition spec of this table.\"\"\"\n    return next(spec for spec in self.partition_specs if spec.spec_id == self.default_spec_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.specs","title":"<code>specs()</code>","text":"<p>Return a dict the partition specs this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def specs(self) -&gt; Dict[int, PartitionSpec]:\n    \"\"\"Return a dict the partition specs this table.\"\"\"\n    return {spec.spec_id: spec for spec in self.partition_specs}\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.specs_struct","title":"<code>specs_struct()</code>","text":"<p>Produce a struct of all the combined PartitionSpecs.</p> <p>The partition fields should be optional: Partition fields may be added later, in which case not all files would have the result field, and it may be null.</p> <p>:return: A StructType that represents all the combined PartitionSpecs of the table</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def specs_struct(self) -&gt; StructType:\n    \"\"\"Produce a struct of all the combined PartitionSpecs.\n\n    The partition fields should be optional: Partition fields may be added later,\n    in which case not all files would have the result field, and it may be null.\n\n    :return: A StructType that represents all the combined PartitionSpecs of the table\n    \"\"\"\n    specs = self.specs()\n\n    # Collect all the fields\n    struct_fields = {field.field_id: field for spec in specs.values() for field in spec.fields}\n\n    schema = self.schema()\n\n    nested_fields = []\n    # Sort them by field_id in order to get a deterministic output\n    for field_id in sorted(struct_fields):\n        field = struct_fields[field_id]\n        source_type = schema.find_type(field.source_id)\n        result_type = field.transform.result_type(source_type)\n        nested_fields.append(NestedField(field_id=field.field_id, name=field.name, type=result_type, required=False))\n\n    return StructType(*nested_fields)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataUtil","title":"<code>TableMetadataUtil</code>","text":"<p>Helper class for parsing TableMetadata.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataUtil:\n    \"\"\"Helper class for parsing TableMetadata.\"\"\"\n\n    @staticmethod\n    def parse_raw(data: str) -&gt; TableMetadata:\n        try:\n            return TableMetadataWrapper.model_validate_json(data).root\n        except PydanticValidationError as e:\n            raise ValidationError(e) from e\n\n    @staticmethod\n    def parse_obj(data: Dict[str, Any]) -&gt; TableMetadata:\n        if \"format-version\" not in data:\n            raise ValidationError(f\"Missing format-version in TableMetadata: {data}\")\n        format_version = data[\"format-version\"]\n\n        if format_version == 1:\n            return TableMetadataV1(**data)\n        elif format_version == 2:\n            return TableMetadataV2(**data)\n        elif format_version == 3:\n            return TableMetadataV3(**data)\n        else:\n            raise ValidationError(f\"Unknown format version: {format_version}\")\n\n    @staticmethod\n    def _construct_without_validation(table_metadata: TableMetadata) -&gt; TableMetadata:\n        \"\"\"Construct table metadata from an existing table without performing validation.\n\n        This method is useful during a sequence of table updates when the model needs to be re-constructed but is not yet ready for validation.\n        \"\"\"\n        if table_metadata.format_version is None:\n            raise ValidationError(f\"Missing format-version in TableMetadata: {table_metadata}\")\n\n        if table_metadata.format_version == 1:\n            return TableMetadataV1.model_construct(**dict(table_metadata))\n        elif table_metadata.format_version == 2:\n            return TableMetadataV2.model_construct(**dict(table_metadata))\n        elif table_metadata.format_version == 3:\n            return TableMetadataV3.model_construct(**dict(table_metadata))\n        else:\n            raise ValidationError(f\"Unknown format version: {table_metadata.format_version}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataUtil._construct_without_validation","title":"<code>_construct_without_validation(table_metadata)</code>  <code>staticmethod</code>","text":"<p>Construct table metadata from an existing table without performing validation.</p> <p>This method is useful during a sequence of table updates when the model needs to be re-constructed but is not yet ready for validation.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@staticmethod\ndef _construct_without_validation(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Construct table metadata from an existing table without performing validation.\n\n    This method is useful during a sequence of table updates when the model needs to be re-constructed but is not yet ready for validation.\n    \"\"\"\n    if table_metadata.format_version is None:\n        raise ValidationError(f\"Missing format-version in TableMetadata: {table_metadata}\")\n\n    if table_metadata.format_version == 1:\n        return TableMetadataV1.model_construct(**dict(table_metadata))\n    elif table_metadata.format_version == 2:\n        return TableMetadataV2.model_construct(**dict(table_metadata))\n    elif table_metadata.format_version == 3:\n        return TableMetadataV3.model_construct(**dict(table_metadata))\n    else:\n        raise ValidationError(f\"Unknown format version: {table_metadata.format_version}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1","title":"<code>TableMetadataV1</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 1 of the Table Metadata.</p> <p>More information about the specification: https://iceberg.apache.org/spec/#version-1-analytic-data-tables</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV1(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 1 of the Table Metadata.\n\n    More information about the specification:\n    https://iceberg.apache.org/spec/#version-1-analytic-data-tables\n    \"\"\"\n\n    # When we read a V1 format-version, we'll make sure to populate the fields\n    # for V2 as well. This makes it easier downstream because we can just\n    # assume that everything is a TableMetadataV2.\n    # When writing, we should stick to the same version that it was,\n    # because bumping the version should be an explicit operation that is up\n    # to the owner of the table.\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(cls, data: TableMetadataV1) -&gt; TableMetadataV1:\n        return construct_refs(data)\n\n    @model_validator(mode=\"before\")\n    def set_v2_compatible_defaults(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Set default values to be compatible with the format v2.\n\n        Args:\n            data: The raw arguments when initializing a V1 TableMetadata.\n\n        Returns:\n            The TableMetadata with the defaults applied.\n        \"\"\"\n        # When the schema doesn't have an ID\n        schema = data.get(\"schema\")\n        if isinstance(schema, dict):\n            if \"schema_id\" not in schema and \"schema-id\" not in schema:\n                schema[\"schema_id\"] = DEFAULT_SCHEMA_ID\n\n        return data\n\n    @model_validator(mode=\"before\")\n    def construct_schemas(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Convert the schema into schemas.\n\n        For V1 schemas is optional, and if they aren't set, we'll set them\n        in this validator. This was we can always use the schemas when reading\n        table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the schemas set, if not provided.\n        \"\"\"\n        if not data.get(\"schemas\"):\n            schema = data[\"schema\"]\n            data[\"schemas\"] = [schema]\n        return data\n\n    @model_validator(mode=\"before\")\n    def construct_partition_specs(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Convert the partition_spec into partition_specs.\n\n        For V1 partition_specs is optional, and if they aren't set, we'll set them\n        in this validator. This was we can always use the partition_specs when reading\n        table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the partition_specs set, if not provided.\n        \"\"\"\n        if not data.get(PARTITION_SPECS):\n            if data.get(PARTITION_SPEC) is not None:\n                # Promote the spec from partition-spec to partition-specs\n                fields = data[PARTITION_SPEC]\n                data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n                data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n            elif data.get(\"partition_spec\") is not None:\n                # Promote the spec from partition_spec to partition-specs\n                fields = data[\"partition_spec\"]\n                data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n                data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n            else:\n                data[PARTITION_SPECS] = [{\"field-id\": 0, \"fields\": ()}]\n\n        data[LAST_PARTITION_ID] = max(\n            [field.get(FIELD_ID) for spec in data[PARTITION_SPECS] for field in spec[FIELDS]],\n            default=PARTITION_FIELD_ID_START - 1,\n        )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    def set_sort_orders(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Set the sort_orders if not provided.\n\n        For V1 sort_orders is optional, and if they aren't set, we'll set them\n        in this validator.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the sort_orders set, if not provided.\n        \"\"\"\n        if not data.get(SORT_ORDERS) and not data.get(\"sort_orders\"):\n            data[SORT_ORDERS] = [UNSORTED_SORT_ORDER]\n        return data\n\n    def to_v2(self) -&gt; TableMetadataV2:\n        metadata = copy(self.model_dump())\n        metadata[\"format-version\"] = 2\n        return TableMetadataV2.model_validate(metadata)\n\n    format_version: Literal[1] = Field(alias=\"format-version\", default=1)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    schema_: Schema = Field(alias=\"schema\")\n    \"\"\"The table\u2019s current schema. (Deprecated: use schemas and\n    current-schema-id instead).\"\"\"\n\n    partition_spec: List[Dict[str, Any]] = Field(alias=\"partition-spec\", default_factory=list)\n    \"\"\"The table\u2019s current partition spec, stored as only fields.\n    Note that this is used by writers to partition data, but is\n    not used when reading because reads use the specs stored in\n    manifest files. (Deprecated: use partition-specs and default-spec-id\n    instead).\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.format_version","title":"<code>format_version = Field(alias='format-version', default=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.partition_spec","title":"<code>partition_spec = Field(alias='partition-spec', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s current partition spec, stored as only fields. Note that this is used by writers to partition data, but is not used when reading because reads use the specs stored in manifest files. (Deprecated: use partition-specs and default-spec-id instead).</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.schema_","title":"<code>schema_ = Field(alias='schema')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s current schema. (Deprecated: use schemas and current-schema-id instead).</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.construct_partition_specs","title":"<code>construct_partition_specs(data)</code>","text":"<p>Convert the partition_spec into partition_specs.</p> <p>For V1 partition_specs is optional, and if they aren't set, we'll set them in this validator. This was we can always use the partition_specs when reading table metadata, and we don't have to worry if it is a v1 or v2 format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The TableMetadata with the partition_specs set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef construct_partition_specs(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Convert the partition_spec into partition_specs.\n\n    For V1 partition_specs is optional, and if they aren't set, we'll set them\n    in this validator. This was we can always use the partition_specs when reading\n    table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the partition_specs set, if not provided.\n    \"\"\"\n    if not data.get(PARTITION_SPECS):\n        if data.get(PARTITION_SPEC) is not None:\n            # Promote the spec from partition-spec to partition-specs\n            fields = data[PARTITION_SPEC]\n            data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n            data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n        elif data.get(\"partition_spec\") is not None:\n            # Promote the spec from partition_spec to partition-specs\n            fields = data[\"partition_spec\"]\n            data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n            data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n        else:\n            data[PARTITION_SPECS] = [{\"field-id\": 0, \"fields\": ()}]\n\n    data[LAST_PARTITION_ID] = max(\n        [field.get(FIELD_ID) for spec in data[PARTITION_SPECS] for field in spec[FIELDS]],\n        default=PARTITION_FIELD_ID_START - 1,\n    )\n\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.construct_schemas","title":"<code>construct_schemas(data)</code>","text":"<p>Convert the schema into schemas.</p> <p>For V1 schemas is optional, and if they aren't set, we'll set them in this validator. This was we can always use the schemas when reading table metadata, and we don't have to worry if it is a v1 or v2 format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The TableMetadata with the schemas set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef construct_schemas(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Convert the schema into schemas.\n\n    For V1 schemas is optional, and if they aren't set, we'll set them\n    in this validator. This was we can always use the schemas when reading\n    table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the schemas set, if not provided.\n    \"\"\"\n    if not data.get(\"schemas\"):\n        schema = data[\"schema\"]\n        data[\"schemas\"] = [schema]\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.set_sort_orders","title":"<code>set_sort_orders(data)</code>","text":"<p>Set the sort_orders if not provided.</p> <p>For V1 sort_orders is optional, and if they aren't set, we'll set them in this validator.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The TableMetadata with the sort_orders set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef set_sort_orders(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Set the sort_orders if not provided.\n\n    For V1 sort_orders is optional, and if they aren't set, we'll set them\n    in this validator.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the sort_orders set, if not provided.\n    \"\"\"\n    if not data.get(SORT_ORDERS) and not data.get(\"sort_orders\"):\n        data[SORT_ORDERS] = [UNSORTED_SORT_ORDER]\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.set_v2_compatible_defaults","title":"<code>set_v2_compatible_defaults(data)</code>","text":"<p>Set default values to be compatible with the format v2.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The raw arguments when initializing a V1 TableMetadata.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The TableMetadata with the defaults applied.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef set_v2_compatible_defaults(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Set default values to be compatible with the format v2.\n\n    Args:\n        data: The raw arguments when initializing a V1 TableMetadata.\n\n    Returns:\n        The TableMetadata with the defaults applied.\n    \"\"\"\n    # When the schema doesn't have an ID\n    schema = data.get(\"schema\")\n    if isinstance(schema, dict):\n        if \"schema_id\" not in schema and \"schema-id\" not in schema:\n            schema[\"schema_id\"] = DEFAULT_SCHEMA_ID\n\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2","title":"<code>TableMetadataV2</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 2 of the Table Metadata.</p> <p>This extends Version 1 with row-level deletes, and adds some additional information to the schema, such as all the historical schemas, partition-specs, sort-orders.</p> <p>For more information: https://iceberg.apache.org/spec/#version-2-row-level-deletes</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV2(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 2 of the Table Metadata.\n\n    This extends Version 1 with row-level deletes, and adds some additional\n    information to the schema, such as all the historical schemas, partition-specs,\n    sort-orders.\n\n    For more information:\n    https://iceberg.apache.org/spec/#version-2-row-level-deletes\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def check_schemas(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_schemas(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def check_partition_specs(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_partition_specs(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def check_sort_orders(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_sort_orders(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return construct_refs(table_metadata)\n\n    format_version: Literal[2] = Field(alias=\"format-version\", default=2)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    last_sequence_number: int = Field(alias=\"last-sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    \"\"\"The table\u2019s highest assigned sequence number, a monotonically\n    increasing long that tracks the order of snapshots in a table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2.format_version","title":"<code>format_version = Field(alias='format-version', default=2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2.last_sequence_number","title":"<code>last_sequence_number = Field(alias='last-sequence-number', default=INITIAL_SEQUENCE_NUMBER)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3","title":"<code>TableMetadataV3</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 3 of the Table Metadata.</p> <p>Version 3 of the Iceberg spec extends data types and existing metadata structures to add new capabilities:</p> <pre><code>- New data types: nanosecond timestamp(tz), unknown\n- Default value support for columns\n- Multi-argument transforms for partitioning and sorting\n- Row Lineage tracking\n- Binary deletion vectors\n</code></pre> <p>For more information: https://iceberg.apache.org/spec/?column-projection#version-3-extended-types-and-capabilities</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV3(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 3 of the Table Metadata.\n\n    Version 3 of the Iceberg spec extends data types and existing metadata structures to add new capabilities:\n\n        - New data types: nanosecond timestamp(tz), unknown\n        - Default value support for columns\n        - Multi-argument transforms for partitioning and sorting\n        - Row Lineage tracking\n        - Binary deletion vectors\n\n    For more information:\n    https://iceberg.apache.org/spec/?column-projection#version-3-extended-types-and-capabilities\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def check_schemas(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_schemas(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def check_partition_specs(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_partition_specs(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def check_sort_orders(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return check_sort_orders(table_metadata)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(cls, table_metadata: TableMetadata) -&gt; TableMetadata:\n        return construct_refs(table_metadata)\n\n    format_version: Literal[3] = Field(alias=\"format-version\", default=3)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    last_sequence_number: int = Field(alias=\"last-sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    \"\"\"The table\u2019s highest assigned sequence number, a monotonically\n    increasing long that tracks the order of snapshots in a table.\"\"\"\n\n    row_lineage: bool = Field(alias=\"row-lineage\", default=False)\n    \"\"\"Indicates that row-lineage is enabled on the table\n\n    For more information:\n    https://iceberg.apache.org/spec/?column-projection#row-lineage\n    \"\"\"\n\n    next_row_id: Optional[int] = Field(alias=\"next-row-id\", default=None)\n    \"\"\"A long higher than all assigned row IDs; the next snapshot's `first-row-id`.\"\"\"\n\n    def model_dump_json(\n        self, exclude_none: bool = True, exclude: Optional[Any] = None, by_alias: bool = True, **kwargs: Any\n    ) -&gt; str:\n        raise NotImplementedError(\"Writing V3 is not yet supported, see: https://github.com/apache/iceberg-python/issues/1551\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.format_version","title":"<code>format_version = Field(alias='format-version', default=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.last_sequence_number","title":"<code>last_sequence_number = Field(alias='last-sequence-number', default=INITIAL_SEQUENCE_NUMBER)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.next_row_id","title":"<code>next_row_id = Field(alias='next-row-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A long higher than all assigned row IDs; the next snapshot's <code>first-row-id</code>.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.row_lineage","title":"<code>row_lineage = Field(alias='row-lineage', default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indicates that row-lineage is enabled on the table</p> <p>For more information: https://iceberg.apache.org/spec/?column-projection#row-lineage</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata._generate_snapshot_id","title":"<code>_generate_snapshot_id()</code>","text":"<p>Generate a new Snapshot ID from a UUID.</p> <p>Returns: An 64 bit long</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def _generate_snapshot_id() -&gt; int:\n    \"\"\"Generate a new Snapshot ID from a UUID.\n\n    Returns: An 64 bit long\n    \"\"\"\n    rnd_uuid = uuid.uuid4()\n    snapshot_id = int.from_bytes(\n        bytes(lhs ^ rhs for lhs, rhs in zip(rnd_uuid.bytes[0:8], rnd_uuid.bytes[8:16])), byteorder=\"little\", signed=True\n    )\n    snapshot_id = snapshot_id if snapshot_id &gt;= 0 else snapshot_id * -1\n\n    return snapshot_id\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_partition_specs","title":"<code>check_partition_specs(table_metadata)</code>","text":"<p>Check if the default-spec-id is present in partition-specs.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_partition_specs(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the default-spec-id is present in partition-specs.\"\"\"\n    default_spec_id = table_metadata.default_spec_id\n\n    partition_specs: List[PartitionSpec] = table_metadata.partition_specs\n    for spec in partition_specs:\n        if spec.spec_id == default_spec_id:\n            return table_metadata\n\n    raise ValidationError(f\"default-spec-id {default_spec_id} can't be found\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_schemas","title":"<code>check_schemas(table_metadata)</code>","text":"<p>Check if the current-schema-id is actually present in schemas.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_schemas(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the current-schema-id is actually present in schemas.\"\"\"\n    current_schema_id = table_metadata.current_schema_id\n\n    for schema in table_metadata.schemas:\n        if schema.schema_id == current_schema_id:\n            return table_metadata\n\n    raise ValidationError(f\"current-schema-id {current_schema_id} can't be found in the schemas\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_sort_orders","title":"<code>check_sort_orders(table_metadata)</code>","text":"<p>Check if the default_sort_order_id is present in sort-orders.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_sort_orders(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the default_sort_order_id is present in sort-orders.\"\"\"\n    default_sort_order_id: int = table_metadata.default_sort_order_id\n\n    if default_sort_order_id != UNSORTED_SORT_ORDER_ID:\n        sort_orders: List[SortOrder] = table_metadata.sort_orders\n        for sort_order in sort_orders:\n            if sort_order.order_id == default_sort_order_id:\n                return table_metadata\n\n        raise ValidationError(f\"default-sort-order-id {default_sort_order_id} can't be found in {sort_orders}\")\n    return table_metadata\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.cleanup_snapshot_id","title":"<code>cleanup_snapshot_id(data)</code>","text":"<p>Run before validation.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def cleanup_snapshot_id(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Run before validation.\"\"\"\n    if CURRENT_SNAPSHOT_ID in data and data[CURRENT_SNAPSHOT_ID] == -1:\n        # We treat -1 and None the same, by cleaning this up\n        # in a pre-validator, we can simplify the logic later on\n        data[CURRENT_SNAPSHOT_ID] = None\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.construct_refs","title":"<code>construct_refs(table_metadata)</code>","text":"<p>Set the main branch if missing.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def construct_refs(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Set the main branch if missing.\"\"\"\n    if table_metadata.current_snapshot_id is not None:\n        if MAIN_BRANCH not in table_metadata.refs:\n            table_metadata.refs[MAIN_BRANCH] = SnapshotRef(\n                snapshot_id=table_metadata.current_snapshot_id, snapshot_ref_type=SnapshotRefType.BRANCH\n            )\n    return table_metadata\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/","title":"name_mapping","text":"<p>Contains everything around the name mapping.</p> <p>More information can be found on here: https://iceberg.apache.org/spec/#name-mapping-serialization</p>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField","title":"<code>MappedField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class MappedField(IcebergBaseModel):\n    field_id: Optional[int] = Field(alias=\"field-id\", default=None)\n    names: List[str] = conlist(str)\n    fields: List[MappedField] = Field(default_factory=list)\n\n    @field_validator(\"fields\", mode=\"before\")\n    @classmethod\n    def convert_null_to_empty_List(cls, v: Any) -&gt; Any:\n        return v or []\n\n    @model_serializer\n    def ser_model(self) -&gt; Dict[str, Any]:\n        \"\"\"Set custom serializer to leave out the field when it is empty.\"\"\"\n        serialized: Dict[str, Any] = {\"names\": self.names}\n        if self.field_id is not None:\n            serialized[\"field-id\"] = self.field_id\n        if len(self.fields) &gt; 0:\n            serialized[\"fields\"] = self.fields\n        return serialized\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of fields.\"\"\"\n        return len(self.fields)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Convert the mapped-field into a nicely formatted string.\"\"\"\n        # Otherwise the UTs fail because the order of the set can change\n        fields_str = \", \".join([str(e) for e in self.fields]) or \"\"\n        fields_str = \" \" + fields_str if fields_str else \"\"\n        field_id = \"?\" if self.field_id is None else (str(self.field_id) or \"?\")\n        return \"([\" + \", \".join(self.names) + \"] -&gt; \" + field_id + fields_str + \")\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of fields.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of fields.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.__str__","title":"<code>__str__()</code>","text":"<p>Convert the mapped-field into a nicely formatted string.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Convert the mapped-field into a nicely formatted string.\"\"\"\n    # Otherwise the UTs fail because the order of the set can change\n    fields_str = \", \".join([str(e) for e in self.fields]) or \"\"\n    fields_str = \" \" + fields_str if fields_str else \"\"\n    field_id = \"?\" if self.field_id is None else (str(self.field_id) or \"?\")\n    return \"([\" + \", \".join(self.names) + \"] -&gt; \" + field_id + fields_str + \")\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.ser_model","title":"<code>ser_model()</code>","text":"<p>Set custom serializer to leave out the field when it is empty.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@model_serializer\ndef ser_model(self) -&gt; Dict[str, Any]:\n    \"\"\"Set custom serializer to leave out the field when it is empty.\"\"\"\n    serialized: Dict[str, Any] = {\"names\": self.names}\n    if self.field_id is not None:\n        serialized[\"field-id\"] = self.field_id\n    if len(self.fields) &gt; 0:\n        serialized[\"fields\"] = self.fields\n    return serialized\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping","title":"<code>NameMapping</code>","text":"<p>               Bases: <code>IcebergRootModel[List[MappedField]]</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class NameMapping(IcebergRootModel[List[MappedField]]):\n    root: List[MappedField]\n\n    @cached_property\n    def _field_by_name(self) -&gt; Dict[str, MappedField]:\n        return visit_name_mapping(self, _IndexByName())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of mappings.\"\"\"\n        return len(self.root)\n\n    def __iter__(self) -&gt; Iterator[MappedField]:\n        \"\"\"Iterate over the mapped fields.\"\"\"\n        return iter(self.root)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Convert the name-mapping into a nicely formatted string.\"\"\"\n        if len(self.root) == 0:\n            return \"[]\"\n        else:\n            return \"[\\n  \" + \"\\n  \".join([str(e) for e in self.root]) + \"\\n]\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the mapped fields.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __iter__(self) -&gt; Iterator[MappedField]:\n    \"\"\"Iterate over the mapped fields.\"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of mappings.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of mappings.\"\"\"\n    return len(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__str__","title":"<code>__str__()</code>","text":"<p>Convert the name-mapping into a nicely formatted string.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Convert the name-mapping into a nicely formatted string.\"\"\"\n    if len(self.root) == 0:\n        return \"[]\"\n    else:\n        return \"[\\n  \" + \"\\n  \".join([str(e) for e in self.root]) + \"\\n]\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor","title":"<code>NameMappingVisitor</code>","text":"<p>               Bases: <code>Generic[S, T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class NameMappingVisitor(Generic[S, T], ABC):\n    @abstractmethod\n    def mapping(self, nm: NameMapping, field_results: S) -&gt; S:\n        \"\"\"Visit a NameMapping.\"\"\"\n\n    @abstractmethod\n    def fields(self, struct: List[MappedField], field_results: List[T]) -&gt; S:\n        \"\"\"Visit a List[MappedField].\"\"\"\n\n    @abstractmethod\n    def field(self, field: MappedField, field_result: S) -&gt; T:\n        \"\"\"Visit a MappedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MappedField.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef field(self, field: MappedField, field_result: S) -&gt; T:\n    \"\"\"Visit a MappedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.fields","title":"<code>fields(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a List[MappedField].</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef fields(self, struct: List[MappedField], field_results: List[T]) -&gt; S:\n    \"\"\"Visit a List[MappedField].\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.mapping","title":"<code>mapping(nm, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a NameMapping.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef mapping(self, nm: NameMapping, field_results: S) -&gt; S:\n    \"\"\"Visit a NameMapping.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.visit_name_mapping","title":"<code>visit_name_mapping(obj, visitor)</code>","text":"<p>Traverse the name mapping in post-order traversal.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@singledispatch\ndef visit_name_mapping(obj: Union[NameMapping, List[MappedField], MappedField], visitor: NameMappingVisitor[S, T]) -&gt; S:\n    \"\"\"Traverse the name mapping in post-order traversal.\"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/refs/","title":"refs","text":""},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType","title":"<code>SnapshotRefType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>class SnapshotRefType(str, Enum):\n    BRANCH = \"branch\"\n    TAG = \"tag\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n        return f\"SnapshotRefType.{self.name}\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n        return self.value\n</code></pre>"},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SnapshotRefType class.</p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n    return f\"SnapshotRefType.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SnapshotRefType class.</p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n    return self.value\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/","title":"snapshots","text":""},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Operation","title":"<code>Operation</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Describes the operation.</p> Possible operation values are <ul> <li>append: Only data files were added and no files were removed.</li> <li>replace: Data and delete files were added and removed without changing table data; i.e., compaction, changing the data file format, or relocating data files.</li> <li>overwrite: Data and delete files were added and removed in a logical overwrite operation.</li> <li>delete: Data files were removed and their contents logically deleted and/or delete files were added to delete rows.</li> </ul> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Operation(Enum):\n    \"\"\"Describes the operation.\n\n    Possible operation values are:\n        - append: Only data files were added and no files were removed.\n        - replace: Data and delete files were added and removed without changing table data; i.e., compaction, changing the data file format, or relocating data files.\n        - overwrite: Data and delete files were added and removed in a logical overwrite operation.\n        - delete: Data files were removed and their contents logically deleted and/or delete files were added to delete rows.\n    \"\"\"\n\n    APPEND = \"append\"\n    REPLACE = \"replace\"\n    OVERWRITE = \"overwrite\"\n    DELETE = \"delete\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Operation class.\"\"\"\n        return f\"Operation.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Operation.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Operation class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Operation class.\"\"\"\n    return f\"Operation.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot","title":"<code>Snapshot</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Snapshot(IcebergBaseModel):\n    snapshot_id: int = Field(alias=\"snapshot-id\")\n    parent_snapshot_id: Optional[int] = Field(alias=\"parent-snapshot-id\", default=None)\n    sequence_number: Optional[int] = Field(alias=\"sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    timestamp_ms: int = Field(alias=\"timestamp-ms\", default_factory=lambda: int(time.time() * 1000))\n    manifest_list: str = Field(alias=\"manifest-list\", description=\"Location of the snapshot's manifest list file\")\n    summary: Optional[Summary] = Field(default=None)\n    schema_id: Optional[int] = Field(alias=\"schema-id\", default=None)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Snapshot class.\"\"\"\n        operation = f\"{self.summary.operation}: \" if self.summary else \"\"\n        parent_id = f\", parent_id={self.parent_snapshot_id}\" if self.parent_snapshot_id else \"\"\n        schema_id = f\", schema_id={self.schema_id}\" if self.schema_id is not None else \"\"\n        result_str = f\"{operation}id={self.snapshot_id}{parent_id}{schema_id}\"\n        return result_str\n\n    def manifests(self, io: FileIO) -&gt; List[ManifestFile]:\n        \"\"\"Return the manifests for the given snapshot.\"\"\"\n        return list(_manifests(io, self.manifest_list))\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Snapshot class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Snapshot class.\"\"\"\n    operation = f\"{self.summary.operation}: \" if self.summary else \"\"\n    parent_id = f\", parent_id={self.parent_snapshot_id}\" if self.parent_snapshot_id else \"\"\n    schema_id = f\", schema_id={self.schema_id}\" if self.schema_id is not None else \"\"\n    result_str = f\"{operation}id={self.snapshot_id}{parent_id}{schema_id}\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot.manifests","title":"<code>manifests(io)</code>","text":"<p>Return the manifests for the given snapshot.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def manifests(self, io: FileIO) -&gt; List[ManifestFile]:\n    \"\"\"Return the manifests for the given snapshot.\"\"\"\n    return list(_manifests(io, self.manifest_list))\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary","title":"<code>Summary</code>","text":"<p>               Bases: <code>IcebergBaseModel</code>, <code>Mapping[str, str]</code></p> <p>A class that stores the summary information for a Snapshot.</p> <p>The snapshot summary\u2019s operation field is used by some operations, like snapshot expiration, to skip processing certain snapshots.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Summary(IcebergBaseModel, Mapping[str, str]):\n    \"\"\"A class that stores the summary information for a Snapshot.\n\n    The snapshot summary\u2019s operation field is used by some operations,\n    like snapshot expiration, to skip processing certain snapshots.\n    \"\"\"\n\n    operation: Operation = Field()\n    _additional_properties: Dict[str, str] = PrivateAttr()\n\n    def __init__(self, operation: Optional[Operation] = None, **data: Any) -&gt; None:\n        if operation is None:\n            warnings.warn(\"Encountered invalid snapshot summary: operation is missing, defaulting to overwrite\")\n            operation = Operation.OVERWRITE\n        super().__init__(operation=operation, **data)\n        self._additional_properties = data\n\n    def __getitem__(self, __key: str) -&gt; Optional[Any]:  # type: ignore\n        \"\"\"Return a key as it is a map.\"\"\"\n        if __key.lower() == \"operation\":\n            return self.operation\n        else:\n            return self._additional_properties.get(__key)\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a key as it is a map.\"\"\"\n        if key.lower() == \"operation\":\n            self.operation = value\n        else:\n            self._additional_properties[key] = value\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of keys in the summary.\"\"\"\n        # Operation is required\n        return 1 + len(self._additional_properties)\n\n    @model_serializer\n    def ser_model(self) -&gt; Dict[str, str]:\n        return {\n            \"operation\": str(self.operation.value),\n            **self._additional_properties,\n        }\n\n    @property\n    def additional_properties(self) -&gt; Dict[str, str]:\n        return self._additional_properties\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Summary class.\"\"\"\n        repr_properties = f\", **{repr(self._additional_properties)}\" if self._additional_properties else \"\"\n        return f\"Summary({repr(self.operation)}{repr_properties})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare if the summary is equal to another summary.\"\"\"\n        return (\n            self.operation == other.operation and self.additional_properties == other.additional_properties\n            if isinstance(other, Summary)\n            else False\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare if the summary is equal to another summary.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare if the summary is equal to another summary.\"\"\"\n    return (\n        self.operation == other.operation and self.additional_properties == other.additional_properties\n        if isinstance(other, Summary)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__getitem__","title":"<code>__getitem__(__key)</code>","text":"<p>Return a key as it is a map.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __getitem__(self, __key: str) -&gt; Optional[Any]:  # type: ignore\n    \"\"\"Return a key as it is a map.\"\"\"\n    if __key.lower() == \"operation\":\n        return self.operation\n    else:\n        return self._additional_properties.get(__key)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of keys in the summary.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of keys in the summary.\"\"\"\n    # Operation is required\n    return 1 + len(self._additional_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Summary class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Summary class.\"\"\"\n    repr_properties = f\", **{repr(self._additional_properties)}\" if self._additional_properties else \"\"\n    return f\"Summary({repr(self.operation)}{repr_properties})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a key as it is a map.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a key as it is a map.\"\"\"\n    if key.lower() == \"operation\":\n        self.operation = value\n    else:\n        self._additional_properties[key] = value\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.ancestors_of","title":"<code>ancestors_of(current_snapshot, table_metadata)</code>","text":"<p>Get the ancestors of and including the given snapshot.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def ancestors_of(current_snapshot: Optional[Snapshot], table_metadata: TableMetadata) -&gt; Iterable[Snapshot]:\n    \"\"\"Get the ancestors of and including the given snapshot.\"\"\"\n    snapshot = current_snapshot\n    while snapshot is not None:\n        yield snapshot\n        if snapshot.parent_snapshot_id is None:\n            break\n        snapshot = table_metadata.snapshot_by_id(snapshot.parent_snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/","title":"sorting","text":""},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder","title":"<code>NullOrder</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class NullOrder(Enum):\n    NULLS_FIRST = \"nulls-first\"\n    NULLS_LAST = \"nulls-last\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the NullOrder class.\"\"\"\n        return self.name.replace(\"_\", \" \")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the NullOrder class.\"\"\"\n        return f\"NullOrder.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the NullOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the NullOrder class.\"\"\"\n    return f\"NullOrder.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the NullOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the NullOrder class.\"\"\"\n    return self.name.replace(\"_\", \" \")\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection","title":"<code>SortDirection</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortDirection(Enum):\n    ASC = \"asc\"\n    DESC = \"desc\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortDirection class.\"\"\"\n        return self.name\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortDirection class.\"\"\"\n        return f\"SortDirection.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SortDirection class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortDirection class.\"\"\"\n    return f\"SortDirection.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortDirection class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortDirection class.\"\"\"\n    return self.name\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortField","title":"<code>SortField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Sort order field.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>int</code> <p>Source column id from the table\u2019s schema.</p> <code>None</code> <code>transform</code> <code>str</code> <p>Transform that is used to produce values to be sorted on from the source column.                This is the same transform as described in partition transforms.</p> <code>None</code> <code>direction</code> <code>SortDirection</code> <p>Sort direction, that can only be either asc or desc.</p> <code>None</code> <code>null_order</code> <code>NullOrder</code> <p>Null order that describes the order of null values when sorted. Can only be either nulls-first or nulls-last.</p> <code>None</code> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortField(IcebergBaseModel):\n    \"\"\"Sort order field.\n\n    Args:\n      source_id (int): Source column id from the table\u2019s schema.\n      transform (str): Transform that is used to produce values to be sorted on from the source column.\n                       This is the same transform as described in partition transforms.\n      direction (SortDirection): Sort direction, that can only be either asc or desc.\n      null_order (NullOrder): Null order that describes the order of null values when sorted. Can only be either nulls-first or nulls-last.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_id: Optional[int] = None,\n        transform: Optional[Union[Transform[Any, Any], Callable[[IcebergType], Transform[Any, Any]]]] = None,\n        direction: Optional[SortDirection] = None,\n        null_order: Optional[NullOrder] = None,\n        **data: Any,\n    ):\n        if source_id is not None:\n            data[\"source-id\"] = source_id\n        if transform is not None:\n            data[\"transform\"] = transform\n        if direction is not None:\n            data[\"direction\"] = direction\n        if null_order is not None:\n            data[\"null-order\"] = null_order\n        super().__init__(**data)\n\n    @model_validator(mode=\"before\")\n    def set_null_order(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n        values[\"direction\"] = values[\"direction\"] if values.get(\"direction\") else SortDirection.ASC\n        if not values.get(\"null-order\"):\n            values[\"null-order\"] = NullOrder.NULLS_FIRST if values[\"direction\"] == SortDirection.ASC else NullOrder.NULLS_LAST\n        return values\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def map_source_ids_onto_source_id(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"source-id\" not in data and (source_ids := data[\"source-ids\"]):\n                if isinstance(source_ids, list):\n                    if len(source_ids) == 0:\n                        raise ValueError(\"Empty source-ids is not allowed\")\n                    if len(source_ids) &gt; 1:\n                        raise ValueError(\"Multi argument transforms are not yet supported\")\n                    data[\"source-id\"] = source_ids[0]\n        return data\n\n    source_id: int = Field(alias=\"source-id\")\n    transform: Annotated[  # type: ignore\n        Transform,\n        BeforeValidator(parse_transform),\n        PlainSerializer(lambda c: str(c), return_type=str),  # pylint: disable=W0108\n        WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    ] = Field(default=IdentityTransform())\n    direction: SortDirection = Field()\n    null_order: NullOrder = Field(alias=\"null-order\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortField class.\"\"\"\n        if isinstance(self.transform, IdentityTransform):\n            # In the case of an identity transform, we can omit the transform\n            return f\"{self.source_id} {self.direction} {self.null_order}\"\n        else:\n            return f\"{self.transform}({self.source_id}) {self.direction} {self.null_order}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortField class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortField class.\"\"\"\n    if isinstance(self.transform, IdentityTransform):\n        # In the case of an identity transform, we can omit the transform\n        return f\"{self.source_id} {self.direction} {self.null_order}\"\n    else:\n        return f\"{self.transform}({self.source_id}) {self.direction} {self.null_order}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder","title":"<code>SortOrder</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Describes how the data is sorted within the table.</p> <p>Users can sort their data within partitions by columns to gain performance.</p> <p>The order of the sort fields within the list defines the order in which the sort is applied to the data.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>List[SortField]</code> <p>The fields how the table is sorted.</p> <code>()</code> <p>Other Parameters:</p> Name Type Description <code>order_id</code> <code>int</code> <p>An unique id of the sort-order of a table.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortOrder(IcebergBaseModel):\n    \"\"\"Describes how the data is sorted within the table.\n\n    Users can sort their data within partitions by columns to gain performance.\n\n    The order of the sort fields within the list defines the order in which the sort is applied to the data.\n\n    Args:\n      fields (List[SortField]): The fields how the table is sorted.\n\n    Keyword Args:\n      order_id (int): An unique id of the sort-order of a table.\n    \"\"\"\n\n    order_id: int = Field(alias=\"order-id\", default=INITIAL_SORT_ORDER_ID)\n    fields: List[SortField] = Field(default_factory=list)\n\n    def __init__(self, *fields: SortField, **data: Any):\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n\n    @property\n    def is_unsorted(self) -&gt; bool:\n        return len(self.fields) == 0\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortOrder class.\"\"\"\n        result_str = \"[\"\n        if self.fields:\n            result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n        result_str += \"]\"\n        return result_str\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortOrder class.\"\"\"\n        fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n        return f\"SortOrder({fields}order_id={self.order_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SortOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortOrder class.\"\"\"\n    fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n    return f\"SortOrder({fields}order_id={self.order_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortOrder class.\"\"\"\n    result_str = \"[\"\n    if self.fields:\n        result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n    result_str += \"]\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/statistics/","title":"statistics","text":""},{"location":"reference/pyiceberg/table/upsert_util/","title":"upsert_util","text":""},{"location":"reference/pyiceberg/table/upsert_util/#pyiceberg.table.upsert_util.get_rows_to_update","title":"<code>get_rows_to_update(source_table, target_table, join_cols)</code>","text":"<p>Return a table with rows that need to be updated in the target table based on the join columns.</p> <p>The table is joined on the identifier columns, and then checked if there are any updated rows. Those are selected and everything is renamed correctly.</p> Source code in <code>pyiceberg/table/upsert_util.py</code> <pre><code>def get_rows_to_update(source_table: pa.Table, target_table: pa.Table, join_cols: list[str]) -&gt; pa.Table:\n    \"\"\"\n    Return a table with rows that need to be updated in the target table based on the join columns.\n\n    The table is joined on the identifier columns, and then checked if there are any updated rows.\n    Those are selected and everything is renamed correctly.\n    \"\"\"\n    all_columns = set(source_table.column_names)\n    join_cols_set = set(join_cols)\n    non_key_cols = all_columns - join_cols_set\n\n    if has_duplicate_rows(target_table, join_cols):\n        raise ValueError(\"Target table has duplicate rows, aborting upsert\")\n\n    if len(target_table) == 0:\n        # When the target table is empty, there is nothing to update :)\n        return source_table.schema.empty_table()\n\n    diff_expr = functools.reduce(operator.or_, [pc.field(f\"{col}-lhs\") != pc.field(f\"{col}-rhs\") for col in non_key_cols])\n\n    return (\n        source_table\n        # We already know that the schema is compatible, this is to fix large_ types\n        .cast(target_table.schema)\n        .join(target_table, keys=list(join_cols_set), join_type=\"inner\", left_suffix=\"-lhs\", right_suffix=\"-rhs\")\n        .filter(diff_expr)\n        .drop_columns([f\"{col}-rhs\" for col in non_key_cols])\n        .rename_columns({f\"{col}-lhs\" if col not in join_cols else col: col for col in source_table.column_names})\n        # Finally cast to the original schema since it doesn't carry nullability:\n        # https://github.com/apache/arrow/issues/45557\n    ).cast(target_table.schema)\n</code></pre>"},{"location":"reference/pyiceberg/table/upsert_util/#pyiceberg.table.upsert_util.has_duplicate_rows","title":"<code>has_duplicate_rows(df, join_cols)</code>","text":"<p>Check for duplicate rows in a PyArrow table based on the join columns.</p> Source code in <code>pyiceberg/table/upsert_util.py</code> <pre><code>def has_duplicate_rows(df: pyarrow_table, join_cols: list[str]) -&gt; bool:\n    \"\"\"Check for duplicate rows in a PyArrow table based on the join columns.\"\"\"\n    return len(df.select(join_cols).group_by(join_cols).aggregate([([], \"count_all\")]).filter(pc.field(\"count_all\") &gt; 1)) &gt; 0\n</code></pre>"},{"location":"reference/pyiceberg/table/update/","title":"update","text":""},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertCreate","title":"<code>AssertCreate</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table must not already exist; used for create transactions.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertCreate(ValidatableTableRequirement):\n    \"\"\"The table must not already exist; used for create transactions.\"\"\"\n\n    type: Literal[\"assert-create\"] = Field(default=\"assert-create\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is not None:\n            raise CommitFailedException(\"Table already exists\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertCurrentSchemaId","title":"<code>AssertCurrentSchemaId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's current schema id must match the requirement's <code>current-schema-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertCurrentSchemaId(ValidatableTableRequirement):\n    \"\"\"The table's current schema id must match the requirement's `current-schema-id`.\"\"\"\n\n    type: Literal[\"assert-current-schema-id\"] = Field(default=\"assert-current-schema-id\")\n    current_schema_id: int = Field(..., alias=\"current-schema-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.current_schema_id != base_metadata.current_schema_id:\n            raise CommitFailedException(\n                f\"Requirement failed: current schema id has changed: expected {self.current_schema_id}, found {base_metadata.current_schema_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertDefaultSortOrderId","title":"<code>AssertDefaultSortOrderId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's default sort order id must match the requirement's <code>default-sort-order-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertDefaultSortOrderId(ValidatableTableRequirement):\n    \"\"\"The table's default sort order id must match the requirement's `default-sort-order-id`.\"\"\"\n\n    type: Literal[\"assert-default-sort-order-id\"] = Field(default=\"assert-default-sort-order-id\")\n    default_sort_order_id: int = Field(..., alias=\"default-sort-order-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.default_sort_order_id != base_metadata.default_sort_order_id:\n            raise CommitFailedException(\n                f\"Requirement failed: default sort order id has changed: expected {self.default_sort_order_id}, found {base_metadata.default_sort_order_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertDefaultSpecId","title":"<code>AssertDefaultSpecId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's default spec id must match the requirement's <code>default-spec-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertDefaultSpecId(ValidatableTableRequirement):\n    \"\"\"The table's default spec id must match the requirement's `default-spec-id`.\"\"\"\n\n    type: Literal[\"assert-default-spec-id\"] = Field(default=\"assert-default-spec-id\")\n    default_spec_id: int = Field(..., alias=\"default-spec-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.default_spec_id != base_metadata.default_spec_id:\n            raise CommitFailedException(\n                f\"Requirement failed: default spec id has changed: expected {self.default_spec_id}, found {base_metadata.default_spec_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertLastAssignedFieldId","title":"<code>AssertLastAssignedFieldId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's last assigned column id must match the requirement's <code>last-assigned-field-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertLastAssignedFieldId(ValidatableTableRequirement):\n    \"\"\"The table's last assigned column id must match the requirement's `last-assigned-field-id`.\"\"\"\n\n    type: Literal[\"assert-last-assigned-field-id\"] = Field(default=\"assert-last-assigned-field-id\")\n    last_assigned_field_id: int = Field(..., alias=\"last-assigned-field-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif base_metadata.last_column_id != self.last_assigned_field_id:\n            raise CommitFailedException(\n                f\"Requirement failed: last assigned field id has changed: expected {self.last_assigned_field_id}, found {base_metadata.last_column_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertLastAssignedPartitionId","title":"<code>AssertLastAssignedPartitionId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's last assigned partition id must match the requirement's <code>last-assigned-partition-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertLastAssignedPartitionId(ValidatableTableRequirement):\n    \"\"\"The table's last assigned partition id must match the requirement's `last-assigned-partition-id`.\"\"\"\n\n    type: Literal[\"assert-last-assigned-partition-id\"] = Field(default=\"assert-last-assigned-partition-id\")\n    last_assigned_partition_id: Optional[int] = Field(..., alias=\"last-assigned-partition-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif base_metadata.last_partition_id != self.last_assigned_partition_id:\n            raise CommitFailedException(\n                f\"Requirement failed: last assigned partition id has changed: expected {self.last_assigned_partition_id}, found {base_metadata.last_partition_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertRefSnapshotId","title":"<code>AssertRefSnapshotId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table branch or tag identified by the requirement's <code>ref</code> must reference the requirement's <code>snapshot-id</code>.</p> <p>if <code>snapshot-id</code> is <code>null</code> or missing, the ref must not already exist.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertRefSnapshotId(ValidatableTableRequirement):\n    \"\"\"The table branch or tag identified by the requirement's `ref` must reference the requirement's `snapshot-id`.\n\n    if `snapshot-id` is `null` or missing, the ref must not already exist.\n    \"\"\"\n\n    type: Literal[\"assert-ref-snapshot-id\"] = Field(default=\"assert-ref-snapshot-id\")\n    ref: str = Field(...)\n    snapshot_id: Optional[int] = Field(default=None, alias=\"snapshot-id\")\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif snapshot_ref := base_metadata.refs.get(self.ref):\n            ref_type = snapshot_ref.snapshot_ref_type\n            if self.snapshot_id is None:\n                raise CommitFailedException(f\"Requirement failed: {ref_type} {self.ref} was created concurrently\")\n            elif self.snapshot_id != snapshot_ref.snapshot_id:\n                raise CommitFailedException(\n                    f\"Requirement failed: {ref_type} {self.ref} has changed: expected id {self.snapshot_id}, found {snapshot_ref.snapshot_id}\"\n                )\n        elif self.snapshot_id is not None:\n            raise CommitFailedException(f\"Requirement failed: branch or tag {self.ref} is missing, expected {self.snapshot_id}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertTableUUID","title":"<code>AssertTableUUID</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table UUID must match the requirement's <code>uuid</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertTableUUID(ValidatableTableRequirement):\n    \"\"\"The table UUID must match the requirement's `uuid`.\"\"\"\n\n    type: Literal[\"assert-table-uuid\"] = Field(default=\"assert-table-uuid\")\n    uuid: uuid.UUID\n\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.uuid != base_metadata.table_uuid:\n            raise CommitFailedException(f\"Table UUID does not match: {self.uuid} != {base_metadata.table_uuid}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata","title":"<code>UpdateTableMetadata</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[U]</code></p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class UpdateTableMetadata(ABC, Generic[U]):\n    _transaction: Transaction\n\n    def __init__(self, transaction: Transaction) -&gt; None:\n        self._transaction = transaction\n\n    @abstractmethod\n    def _commit(self) -&gt; UpdatesAndRequirements: ...\n\n    def commit(self) -&gt; None:\n        self._transaction._apply(*self._commit())\n\n    def __exit__(self, _: Any, value: Any, traceback: Any) -&gt; None:\n        \"\"\"Close and commit the change.\"\"\"\n        self.commit()\n\n    def __enter__(self) -&gt; U:\n        \"\"\"Update the table.\"\"\"\n        return self  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata.__enter__","title":"<code>__enter__()</code>","text":"<p>Update the table.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def __enter__(self) -&gt; U:\n    \"\"\"Update the table.\"\"\"\n    return self  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata.__exit__","title":"<code>__exit__(_, value, traceback)</code>","text":"<p>Close and commit the change.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def __exit__(self, _: Any, value: Any, traceback: Any) -&gt; None:\n    \"\"\"Close and commit the change.\"\"\"\n    self.commit()\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.ValidatableTableRequirement","title":"<code>ValidatableTableRequirement</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class ValidatableTableRequirement(IcebergBaseModel):\n    type: str\n\n    @abstractmethod\n    def validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n        \"\"\"Validate the requirement against the base metadata.\n\n        Args:\n            base_metadata: The base metadata to be validated against.\n\n        Raises:\n            CommitFailedException: When the requirement is not met.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.ValidatableTableRequirement.validate","title":"<code>validate(base_metadata)</code>  <code>abstractmethod</code>","text":"<p>Validate the requirement against the base metadata.</p> <p>Parameters:</p> Name Type Description Default <code>base_metadata</code> <code>Optional[TableMetadata]</code> <p>The base metadata to be validated against.</p> required <p>Raises:</p> Type Description <code>CommitFailedException</code> <p>When the requirement is not met.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>@abstractmethod\ndef validate(self, base_metadata: Optional[TableMetadata]) -&gt; None:\n    \"\"\"Validate the requirement against the base metadata.\n\n    Args:\n        base_metadata: The base metadata to be validated against.\n\n    Raises:\n        CommitFailedException: When the requirement is not met.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update._apply_table_update","title":"<code>_apply_table_update(update, base_metadata, context)</code>","text":"<p>Apply a table update to the table metadata.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>TableUpdate</code> <p>The update to be applied.</p> required <code>base_metadata</code> <code>TableMetadata</code> <p>The base metadata to be updated.</p> required <code>context</code> <code>_TableMetadataUpdateContext</code> <p>Contains previous updates and other change tracking information in the current transaction.</p> required <p>Returns:</p> Type Description <code>TableMetadata</code> <p>The updated metadata.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>@singledispatch\ndef _apply_table_update(update: TableUpdate, base_metadata: TableMetadata, context: _TableMetadataUpdateContext) -&gt; TableMetadata:\n    \"\"\"Apply a table update to the table metadata.\n\n    Args:\n        update: The update to be applied.\n        base_metadata: The base metadata to be updated.\n        context: Contains previous updates and other change tracking information in the current transaction.\n\n    Returns:\n        The updated metadata.\n\n    \"\"\"\n    raise NotImplementedError(f\"Unsupported table update: {update}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.update_table_metadata","title":"<code>update_table_metadata(base_metadata, updates, enforce_validation=False, metadata_location=None)</code>","text":"<p>Update the table metadata with the given updates in one transaction.</p> <p>Parameters:</p> Name Type Description Default <code>base_metadata</code> <code>TableMetadata</code> <p>The base metadata to be updated.</p> required <code>updates</code> <code>Tuple[TableUpdate, ...]</code> <p>The updates in one transaction.</p> required <code>enforce_validation</code> <code>bool</code> <p>Whether to trigger validation after applying the updates.</p> <code>False</code> <code>metadata_location</code> <code>Optional[str]</code> <p>Current metadata location of the table</p> <code>None</code> <p>Returns:</p> Type Description <code>TableMetadata</code> <p>The metadata with the updates applied.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def update_table_metadata(\n    base_metadata: TableMetadata,\n    updates: Tuple[TableUpdate, ...],\n    enforce_validation: bool = False,\n    metadata_location: Optional[str] = None,\n) -&gt; TableMetadata:\n    \"\"\"Update the table metadata with the given updates in one transaction.\n\n    Args:\n        base_metadata: The base metadata to be updated.\n        updates: The updates in one transaction.\n        enforce_validation: Whether to trigger validation after applying the updates.\n        metadata_location: Current metadata location of the table\n\n    Returns:\n        The metadata with the updates applied.\n    \"\"\"\n    context = _TableMetadataUpdateContext()\n    new_metadata = base_metadata\n\n    for update in updates:\n        new_metadata = _apply_table_update(update, new_metadata, context)\n\n    # Update last_updated_ms if it was not updated by update operations\n    if context.has_changes():\n        if metadata_location:\n            new_metadata = _update_table_metadata_log(new_metadata, metadata_location, base_metadata.last_updated_ms)\n        if base_metadata.last_updated_ms == new_metadata.last_updated_ms:\n            new_metadata = new_metadata.model_copy(update={\"last_updated_ms\": datetime_to_millis(datetime.now().astimezone())})\n\n    if enforce_validation:\n        return TableMetadataUtil.parse_obj(new_metadata.model_dump())\n    else:\n        return new_metadata.model_copy(deep=True)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/","title":"schema","text":""},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema","title":"<code>UpdateSchema</code>","text":"<p>               Bases: <code>UpdateTableMetadata['UpdateSchema']</code></p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>class UpdateSchema(UpdateTableMetadata[\"UpdateSchema\"]):\n    _schema: Schema\n    _last_column_id: itertools.count[int]\n    _identifier_field_names: Set[str]\n\n    _adds: Dict[int, List[NestedField]] = {}\n    _updates: Dict[int, NestedField] = {}\n    _deletes: Set[int] = set()\n    _moves: Dict[int, List[_Move]] = {}\n\n    _added_name_to_id: Dict[str, int] = {}\n    # Part of https://github.com/apache/iceberg/pull/8393\n    _id_to_parent: Dict[int, str] = {}\n    _allow_incompatible_changes: bool\n    _case_sensitive: bool\n\n    def __init__(\n        self,\n        transaction: Transaction,\n        allow_incompatible_changes: bool = False,\n        case_sensitive: bool = True,\n        schema: Optional[Schema] = None,\n        name_mapping: Optional[NameMapping] = None,\n    ) -&gt; None:\n        super().__init__(transaction)\n\n        if isinstance(schema, Schema):\n            self._schema = schema\n            self._last_column_id = itertools.count(1 + schema.highest_field_id)\n        else:\n            self._schema = self._transaction.table_metadata.schema()\n            self._last_column_id = itertools.count(1 + self._transaction.table_metadata.last_column_id)\n\n        self._name_mapping = name_mapping\n        self._identifier_field_names = self._schema.identifier_field_names()\n\n        self._adds = {}\n        self._updates = {}\n        self._deletes = set()\n        self._moves = {}\n\n        self._added_name_to_id = {}\n\n        def get_column_name(field_id: int) -&gt; str:\n            column_name = self._schema.find_column_name(column_id=field_id)\n            if column_name is None:\n                raise ValueError(f\"Could not find field-id: {field_id}\")\n            return column_name\n\n        self._id_to_parent = {\n            field_id: get_column_name(parent_field_id) for field_id, parent_field_id in self._schema._lazy_id_to_parent.items()\n        }\n\n        self._allow_incompatible_changes = allow_incompatible_changes\n        self._case_sensitive = case_sensitive\n        self._transaction = transaction\n\n    def case_sensitive(self, case_sensitive: bool) -&gt; UpdateSchema:\n        \"\"\"Determine if the case of schema needs to be considered when comparing column names.\n\n        Args:\n            case_sensitive: When false case is not considered in column name comparisons.\n\n        Returns:\n            This for method chaining\n        \"\"\"\n        self._case_sensitive = case_sensitive\n        return self\n\n    def union_by_name(self, new_schema: Union[Schema, \"pa.Schema\"]) -&gt; UpdateSchema:\n        from pyiceberg.catalog import Catalog\n\n        visit_with_partner(\n            Catalog._convert_schema_if_needed(new_schema),\n            -1,\n            _UnionByNameVisitor(update_schema=self, existing_schema=self._schema, case_sensitive=self._case_sensitive),\n            # type: ignore\n            PartnerIdByNameAccessor(partner_schema=self._schema, case_sensitive=self._case_sensitive),\n        )\n        return self\n\n    def add_column(\n        self, path: Union[str, Tuple[str, ...]], field_type: IcebergType, doc: Optional[str] = None, required: bool = False\n    ) -&gt; UpdateSchema:\n        \"\"\"Add a new column to a nested struct or Add a new top-level column.\n\n        Because \".\" may be interpreted as a column path separator or may be used in field names, it\n        is not allowed to add nested column by passing in a string. To add to nested structures or\n        to add fields with names that contain \".\" use a tuple instead to indicate the path.\n\n        If type is a nested type, its field IDs are reassigned when added to the existing schema.\n\n        Args:\n            path: Name for the new column.\n            field_type: Type for the new column.\n            doc: Documentation string for the new column.\n            required: Whether the new column is required.\n\n        Returns:\n            This for method chaining.\n        \"\"\"\n        if isinstance(path, str):\n            if \".\" in path:\n                raise ValueError(f\"Cannot add column with ambiguous name: {path}, provide a tuple instead\")\n            path = (path,)\n\n        if required and not self._allow_incompatible_changes:\n            # Table format version 1 and 2 cannot add required column because there is no initial value\n            raise ValueError(f\"Incompatible change: cannot add required column: {'.'.join(path)}\")\n\n        name = path[-1]\n        parent = path[:-1]\n\n        full_name = \".\".join(path)\n        parent_full_path = \".\".join(parent)\n        parent_id: int = TABLE_ROOT_ID\n\n        if len(parent) &gt; 0:\n            parent_field = self._schema.find_field(parent_full_path, self._case_sensitive)\n            parent_type = parent_field.field_type\n            if isinstance(parent_type, MapType):\n                parent_field = parent_type.value_field\n            elif isinstance(parent_type, ListType):\n                parent_field = parent_type.element_field\n\n            if not parent_field.field_type.is_struct:\n                raise ValueError(f\"Cannot add column '{name}' to non-struct type: {parent_full_path}\")\n\n            parent_id = parent_field.field_id\n\n        existing_field = None\n        try:\n            existing_field = self._schema.find_field(full_name, self._case_sensitive)\n        except ValueError:\n            pass\n\n        if existing_field is not None and existing_field.field_id not in self._deletes:\n            raise ValueError(f\"Cannot add column, name already exists: {full_name}\")\n\n        # assign new IDs in order\n        new_id = self.assign_new_column_id()\n\n        # update tracking for moves\n        self._added_name_to_id[full_name] = new_id\n        self._id_to_parent[new_id] = parent_full_path\n\n        new_type = assign_fresh_schema_ids(field_type, self.assign_new_column_id)\n        field = NestedField(field_id=new_id, name=name, field_type=new_type, required=required, doc=doc)\n\n        if parent_id in self._adds:\n            self._adds[parent_id].append(field)\n        else:\n            self._adds[parent_id] = [field]\n\n        return self\n\n    def delete_column(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n        \"\"\"Delete a column from a table.\n\n        Args:\n            path: The path to the column.\n\n        Returns:\n            The UpdateSchema with the delete operation staged.\n        \"\"\"\n        name = (path,) if isinstance(path, str) else path\n        full_name = \".\".join(name)\n\n        field = self._schema.find_field(full_name, case_sensitive=self._case_sensitive)\n\n        if field.field_id in self._adds:\n            raise ValueError(f\"Cannot delete a column that has additions: {full_name}\")\n        if field.field_id in self._updates:\n            raise ValueError(f\"Cannot delete a column that has updates: {full_name}\")\n\n        self._deletes.add(field.field_id)\n\n        return self\n\n    def rename_column(self, path_from: Union[str, Tuple[str, ...]], new_name: str) -&gt; UpdateSchema:\n        \"\"\"Update the name of a column.\n\n        Args:\n            path_from: The path to the column to be renamed.\n            new_name: The new path of the column.\n\n        Returns:\n            The UpdateSchema with the rename operation staged.\n        \"\"\"\n        path_from = \".\".join(path_from) if isinstance(path_from, tuple) else path_from\n        field_from = self._schema.find_field(path_from, self._case_sensitive)\n\n        if field_from.field_id in self._deletes:\n            raise ValueError(f\"Cannot rename a column that will be deleted: {path_from}\")\n\n        if updated := self._updates.get(field_from.field_id):\n            self._updates[field_from.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=new_name,\n                field_type=updated.field_type,\n                doc=updated.doc,\n                required=updated.required,\n            )\n        else:\n            self._updates[field_from.field_id] = NestedField(\n                field_id=field_from.field_id,\n                name=new_name,\n                field_type=field_from.field_type,\n                doc=field_from.doc,\n                required=field_from.required,\n            )\n\n        # Lookup the field because of casing\n        from_field_correct_casing = self._schema.find_column_name(field_from.field_id)\n        if from_field_correct_casing in self._identifier_field_names:\n            self._identifier_field_names.remove(from_field_correct_casing)\n            new_identifier_path = f\"{from_field_correct_casing[: -len(field_from.name)]}{new_name}\"\n            self._identifier_field_names.add(new_identifier_path)\n\n        return self\n\n    def make_column_optional(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n        \"\"\"Make a column optional.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the requirement change staged.\n        \"\"\"\n        self._set_column_requirement(path, required=False)\n        return self\n\n    def set_identifier_fields(self, *fields: str) -&gt; None:\n        self._identifier_field_names = set(fields)\n\n    def _set_column_requirement(self, path: Union[str, Tuple[str, ...]], required: bool) -&gt; None:\n        path = (path,) if isinstance(path, str) else path\n        name = \".\".join(path)\n\n        field = self._schema.find_field(name, self._case_sensitive)\n\n        if (field.required and required) or (field.optional and not required):\n            # if the change is a noop, allow it even if allowIncompatibleChanges is false\n            return\n\n        if not self._allow_incompatible_changes and required:\n            raise ValueError(f\"Cannot change column nullability: {name}: optional -&gt; required\")\n\n        if field.field_id in self._deletes:\n            raise ValueError(f\"Cannot update a column that will be deleted: {name}\")\n\n        if updated := self._updates.get(field.field_id):\n            self._updates[field.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=updated.name,\n                field_type=updated.field_type,\n                doc=updated.doc,\n                required=required,\n            )\n        else:\n            self._updates[field.field_id] = NestedField(\n                field_id=field.field_id,\n                name=field.name,\n                field_type=field.field_type,\n                doc=field.doc,\n                required=required,\n            )\n\n    def update_column(\n        self,\n        path: Union[str, Tuple[str, ...]],\n        field_type: Optional[IcebergType] = None,\n        required: Optional[bool] = None,\n        doc: Optional[str] = None,\n    ) -&gt; UpdateSchema:\n        \"\"\"Update the type of column.\n\n        Args:\n            path: The path to the field.\n            field_type: The new type\n            required: If the field should be required\n            doc: Documentation describing the column\n\n        Returns:\n            The UpdateSchema with the type update staged.\n        \"\"\"\n        path = (path,) if isinstance(path, str) else path\n        full_name = \".\".join(path)\n\n        if field_type is None and required is None and doc is None:\n            return self\n\n        field = self._schema.find_field(full_name, self._case_sensitive)\n\n        if field.field_id in self._deletes:\n            raise ValueError(f\"Cannot update a column that will be deleted: {full_name}\")\n\n        if field_type is not None:\n            if not field.field_type.is_primitive:\n                raise ValidationError(f\"Cannot change column type: {field.field_type} is not a primitive\")\n\n            if not self._allow_incompatible_changes and field.field_type != field_type:\n                try:\n                    promote(field.field_type, field_type)\n                except ResolveError as e:\n                    raise ValidationError(f\"Cannot change column type: {full_name}: {field.field_type} -&gt; {field_type}\") from e\n\n        # if other updates for the same field exist in one transaction:\n        if updated := self._updates.get(field.field_id):\n            self._updates[field.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=updated.name,\n                field_type=field_type or updated.field_type,\n                doc=doc if doc is not None else updated.doc,\n                required=updated.required,\n            )\n        else:\n            self._updates[field.field_id] = NestedField(\n                field_id=field.field_id,\n                name=field.name,\n                field_type=field_type or field.field_type,\n                doc=doc if doc is not None else field.doc,\n                required=field.required,\n            )\n\n        if required is not None:\n            self._set_column_requirement(path, required=required)\n\n        return self\n\n    def _find_for_move(self, name: str) -&gt; Optional[int]:\n        try:\n            return self._schema.find_field(name, self._case_sensitive).field_id\n        except ValueError:\n            pass\n\n        return self._added_name_to_id.get(name)\n\n    def _move(self, move: _Move) -&gt; None:\n        if parent_name := self._id_to_parent.get(move.field_id):\n            parent_field = self._schema.find_field(parent_name, case_sensitive=self._case_sensitive)\n            if not parent_field.field_type.is_struct:\n                raise ValueError(f\"Cannot move fields in non-struct type: {parent_field.field_type}\")\n\n            if move.op == _MoveOperation.After or move.op == _MoveOperation.Before:\n                if move.other_field_id is None:\n                    raise ValueError(\"Expected other field when performing before/after move\")\n\n                if self._id_to_parent.get(move.field_id) != self._id_to_parent.get(move.other_field_id):\n                    raise ValueError(f\"Cannot move field {move.full_name} to a different struct\")\n\n            self._moves[parent_field.field_id] = self._moves.get(parent_field.field_id, []) + [move]\n        else:\n            # In the top level field\n            if move.op == _MoveOperation.After or move.op == _MoveOperation.Before:\n                if move.other_field_id is None:\n                    raise ValueError(\"Expected other field when performing before/after move\")\n\n                if other_struct := self._id_to_parent.get(move.other_field_id):\n                    raise ValueError(f\"Cannot move field {move.full_name} to a different struct: {other_struct}\")\n\n            self._moves[TABLE_ROOT_ID] = self._moves.get(TABLE_ROOT_ID, []) + [move]\n\n    def move_first(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n        \"\"\"Move the field to the first position of the parent struct.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, op=_MoveOperation.First))\n\n        return self\n\n    def move_before(self, path: Union[str, Tuple[str, ...]], before_path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n        \"\"\"Move the field to before another field.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        before_full_name = (\n            \".\".join(\n                before_path,\n            )\n            if isinstance(before_path, tuple)\n            else before_path\n        )\n        before_field_id = self._find_for_move(before_full_name)\n\n        if before_field_id is None:\n            raise ValueError(f\"Cannot move {full_name} before missing column: {before_full_name}\")\n\n        if field_id == before_field_id:\n            raise ValueError(f\"Cannot move {full_name} before itself\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=before_field_id, op=_MoveOperation.Before))\n\n        return self\n\n    def move_after(self, path: Union[str, Tuple[str, ...]], after_name: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n        \"\"\"Move the field to after another field.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        after_path = \".\".join(after_name) if isinstance(after_name, tuple) else after_name\n        after_field_id = self._find_for_move(after_path)\n\n        if after_field_id is None:\n            raise ValueError(f\"Cannot move {full_name} after missing column: {after_path}\")\n\n        if field_id == after_field_id:\n            raise ValueError(f\"Cannot move {full_name} after itself\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=after_field_id, op=_MoveOperation.After))\n\n        return self\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"Apply the pending changes and commit.\"\"\"\n        from pyiceberg.table import TableProperties\n\n        new_schema = self._apply()\n\n        existing_schema_id = next(\n            (schema.schema_id for schema in self._transaction.table_metadata.schemas if schema == new_schema), None\n        )\n\n        requirements: Tuple[TableRequirement, ...] = ()\n        updates: Tuple[TableUpdate, ...] = ()\n\n        # Check if it is different current schema ID\n        if existing_schema_id != self._schema.schema_id:\n            requirements += (AssertCurrentSchemaId(current_schema_id=self._schema.schema_id),)\n            if existing_schema_id is None:\n                last_column_id = max(self._transaction.table_metadata.last_column_id, new_schema.highest_field_id)\n                updates += (\n                    AddSchemaUpdate(schema=new_schema, last_column_id=last_column_id),\n                    SetCurrentSchemaUpdate(schema_id=-1),\n                )\n            else:\n                updates += (SetCurrentSchemaUpdate(schema_id=existing_schema_id),)\n\n            if name_mapping := self._name_mapping:\n                updated_name_mapping = update_mapping(name_mapping, self._updates, self._adds)\n                updates += (\n                    SetPropertiesUpdate(updates={TableProperties.DEFAULT_NAME_MAPPING: updated_name_mapping.model_dump_json()}),\n                )\n\n        return updates, requirements\n\n    def _apply(self) -&gt; Schema:\n        \"\"\"Apply the pending changes to the original schema and returns the result.\n\n        Returns:\n            the result Schema when all pending updates are applied\n        \"\"\"\n        struct = visit(self._schema, _ApplyChanges(self._adds, self._updates, self._deletes, self._moves))\n        if struct is None:\n            # Should never happen\n            raise ValueError(\"Could not apply changes\")\n\n        # Check the field-ids\n        new_schema = Schema(*struct.fields)\n        field_ids = set()\n        for name in self._identifier_field_names:\n            try:\n                field = new_schema.find_field(name, case_sensitive=self._case_sensitive)\n            except ValueError as e:\n                raise ValueError(\n                    f\"Cannot find identifier field {name}. In case of deletion, update the identifier fields first.\"\n                ) from e\n\n            field_ids.add(field.field_id)\n\n        if txn := self._transaction:\n            next_schema_id = 1 + (\n                max(schema.schema_id for schema in txn.table_metadata.schemas) if txn.table_metadata is not None else 0\n            )\n        else:\n            next_schema_id = 0\n\n        return Schema(*struct.fields, schema_id=next_schema_id, identifier_field_ids=field_ids)\n\n    def assign_new_column_id(self) -&gt; int:\n        return next(self._last_column_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema._apply","title":"<code>_apply()</code>","text":"<p>Apply the pending changes to the original schema and returns the result.</p> <p>Returns:</p> Type Description <code>Schema</code> <p>the result Schema when all pending updates are applied</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def _apply(self) -&gt; Schema:\n    \"\"\"Apply the pending changes to the original schema and returns the result.\n\n    Returns:\n        the result Schema when all pending updates are applied\n    \"\"\"\n    struct = visit(self._schema, _ApplyChanges(self._adds, self._updates, self._deletes, self._moves))\n    if struct is None:\n        # Should never happen\n        raise ValueError(\"Could not apply changes\")\n\n    # Check the field-ids\n    new_schema = Schema(*struct.fields)\n    field_ids = set()\n    for name in self._identifier_field_names:\n        try:\n            field = new_schema.find_field(name, case_sensitive=self._case_sensitive)\n        except ValueError as e:\n            raise ValueError(\n                f\"Cannot find identifier field {name}. In case of deletion, update the identifier fields first.\"\n            ) from e\n\n        field_ids.add(field.field_id)\n\n    if txn := self._transaction:\n        next_schema_id = 1 + (\n            max(schema.schema_id for schema in txn.table_metadata.schemas) if txn.table_metadata is not None else 0\n        )\n    else:\n        next_schema_id = 0\n\n    return Schema(*struct.fields, schema_id=next_schema_id, identifier_field_ids=field_ids)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema._commit","title":"<code>_commit()</code>","text":"<p>Apply the pending changes and commit.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def _commit(self) -&gt; UpdatesAndRequirements:\n    \"\"\"Apply the pending changes and commit.\"\"\"\n    from pyiceberg.table import TableProperties\n\n    new_schema = self._apply()\n\n    existing_schema_id = next(\n        (schema.schema_id for schema in self._transaction.table_metadata.schemas if schema == new_schema), None\n    )\n\n    requirements: Tuple[TableRequirement, ...] = ()\n    updates: Tuple[TableUpdate, ...] = ()\n\n    # Check if it is different current schema ID\n    if existing_schema_id != self._schema.schema_id:\n        requirements += (AssertCurrentSchemaId(current_schema_id=self._schema.schema_id),)\n        if existing_schema_id is None:\n            last_column_id = max(self._transaction.table_metadata.last_column_id, new_schema.highest_field_id)\n            updates += (\n                AddSchemaUpdate(schema=new_schema, last_column_id=last_column_id),\n                SetCurrentSchemaUpdate(schema_id=-1),\n            )\n        else:\n            updates += (SetCurrentSchemaUpdate(schema_id=existing_schema_id),)\n\n        if name_mapping := self._name_mapping:\n            updated_name_mapping = update_mapping(name_mapping, self._updates, self._adds)\n            updates += (\n                SetPropertiesUpdate(updates={TableProperties.DEFAULT_NAME_MAPPING: updated_name_mapping.model_dump_json()}),\n            )\n\n    return updates, requirements\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.add_column","title":"<code>add_column(path, field_type, doc=None, required=False)</code>","text":"<p>Add a new column to a nested struct or Add a new top-level column.</p> <p>Because \".\" may be interpreted as a column path separator or may be used in field names, it is not allowed to add nested column by passing in a string. To add to nested structures or to add fields with names that contain \".\" use a tuple instead to indicate the path.</p> <p>If type is a nested type, its field IDs are reassigned when added to the existing schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>Name for the new column.</p> required <code>field_type</code> <code>IcebergType</code> <p>Type for the new column.</p> required <code>doc</code> <code>Optional[str]</code> <p>Documentation string for the new column.</p> <code>None</code> <code>required</code> <code>bool</code> <p>Whether the new column is required.</p> <code>False</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>This for method chaining.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def add_column(\n    self, path: Union[str, Tuple[str, ...]], field_type: IcebergType, doc: Optional[str] = None, required: bool = False\n) -&gt; UpdateSchema:\n    \"\"\"Add a new column to a nested struct or Add a new top-level column.\n\n    Because \".\" may be interpreted as a column path separator or may be used in field names, it\n    is not allowed to add nested column by passing in a string. To add to nested structures or\n    to add fields with names that contain \".\" use a tuple instead to indicate the path.\n\n    If type is a nested type, its field IDs are reassigned when added to the existing schema.\n\n    Args:\n        path: Name for the new column.\n        field_type: Type for the new column.\n        doc: Documentation string for the new column.\n        required: Whether the new column is required.\n\n    Returns:\n        This for method chaining.\n    \"\"\"\n    if isinstance(path, str):\n        if \".\" in path:\n            raise ValueError(f\"Cannot add column with ambiguous name: {path}, provide a tuple instead\")\n        path = (path,)\n\n    if required and not self._allow_incompatible_changes:\n        # Table format version 1 and 2 cannot add required column because there is no initial value\n        raise ValueError(f\"Incompatible change: cannot add required column: {'.'.join(path)}\")\n\n    name = path[-1]\n    parent = path[:-1]\n\n    full_name = \".\".join(path)\n    parent_full_path = \".\".join(parent)\n    parent_id: int = TABLE_ROOT_ID\n\n    if len(parent) &gt; 0:\n        parent_field = self._schema.find_field(parent_full_path, self._case_sensitive)\n        parent_type = parent_field.field_type\n        if isinstance(parent_type, MapType):\n            parent_field = parent_type.value_field\n        elif isinstance(parent_type, ListType):\n            parent_field = parent_type.element_field\n\n        if not parent_field.field_type.is_struct:\n            raise ValueError(f\"Cannot add column '{name}' to non-struct type: {parent_full_path}\")\n\n        parent_id = parent_field.field_id\n\n    existing_field = None\n    try:\n        existing_field = self._schema.find_field(full_name, self._case_sensitive)\n    except ValueError:\n        pass\n\n    if existing_field is not None and existing_field.field_id not in self._deletes:\n        raise ValueError(f\"Cannot add column, name already exists: {full_name}\")\n\n    # assign new IDs in order\n    new_id = self.assign_new_column_id()\n\n    # update tracking for moves\n    self._added_name_to_id[full_name] = new_id\n    self._id_to_parent[new_id] = parent_full_path\n\n    new_type = assign_fresh_schema_ids(field_type, self.assign_new_column_id)\n    field = NestedField(field_id=new_id, name=name, field_type=new_type, required=required, doc=doc)\n\n    if parent_id in self._adds:\n        self._adds[parent_id].append(field)\n    else:\n        self._adds[parent_id] = [field]\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.case_sensitive","title":"<code>case_sensitive(case_sensitive)</code>","text":"<p>Determine if the case of schema needs to be considered when comparing column names.</p> <p>Parameters:</p> Name Type Description Default <code>case_sensitive</code> <code>bool</code> <p>When false case is not considered in column name comparisons.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>This for method chaining</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def case_sensitive(self, case_sensitive: bool) -&gt; UpdateSchema:\n    \"\"\"Determine if the case of schema needs to be considered when comparing column names.\n\n    Args:\n        case_sensitive: When false case is not considered in column name comparisons.\n\n    Returns:\n        This for method chaining\n    \"\"\"\n    self._case_sensitive = case_sensitive\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.delete_column","title":"<code>delete_column(path)</code>","text":"<p>Delete a column from a table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the column.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the delete operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def delete_column(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n    \"\"\"Delete a column from a table.\n\n    Args:\n        path: The path to the column.\n\n    Returns:\n        The UpdateSchema with the delete operation staged.\n    \"\"\"\n    name = (path,) if isinstance(path, str) else path\n    full_name = \".\".join(name)\n\n    field = self._schema.find_field(full_name, case_sensitive=self._case_sensitive)\n\n    if field.field_id in self._adds:\n        raise ValueError(f\"Cannot delete a column that has additions: {full_name}\")\n    if field.field_id in self._updates:\n        raise ValueError(f\"Cannot delete a column that has updates: {full_name}\")\n\n    self._deletes.add(field.field_id)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.make_column_optional","title":"<code>make_column_optional(path)</code>","text":"<p>Make a column optional.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the requirement change staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def make_column_optional(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n    \"\"\"Make a column optional.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the requirement change staged.\n    \"\"\"\n    self._set_column_requirement(path, required=False)\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_after","title":"<code>move_after(path, after_name)</code>","text":"<p>Move the field to after another field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_after(self, path: Union[str, Tuple[str, ...]], after_name: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n    \"\"\"Move the field to after another field.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    after_path = \".\".join(after_name) if isinstance(after_name, tuple) else after_name\n    after_field_id = self._find_for_move(after_path)\n\n    if after_field_id is None:\n        raise ValueError(f\"Cannot move {full_name} after missing column: {after_path}\")\n\n    if field_id == after_field_id:\n        raise ValueError(f\"Cannot move {full_name} after itself\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=after_field_id, op=_MoveOperation.After))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_before","title":"<code>move_before(path, before_path)</code>","text":"<p>Move the field to before another field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_before(self, path: Union[str, Tuple[str, ...]], before_path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n    \"\"\"Move the field to before another field.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    before_full_name = (\n        \".\".join(\n            before_path,\n        )\n        if isinstance(before_path, tuple)\n        else before_path\n    )\n    before_field_id = self._find_for_move(before_full_name)\n\n    if before_field_id is None:\n        raise ValueError(f\"Cannot move {full_name} before missing column: {before_full_name}\")\n\n    if field_id == before_field_id:\n        raise ValueError(f\"Cannot move {full_name} before itself\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=before_field_id, op=_MoveOperation.Before))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_first","title":"<code>move_first(path)</code>","text":"<p>Move the field to the first position of the parent struct.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_first(self, path: Union[str, Tuple[str, ...]]) -&gt; UpdateSchema:\n    \"\"\"Move the field to the first position of the parent struct.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, op=_MoveOperation.First))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.rename_column","title":"<code>rename_column(path_from, new_name)</code>","text":"<p>Update the name of a column.</p> <p>Parameters:</p> Name Type Description Default <code>path_from</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the column to be renamed.</p> required <code>new_name</code> <code>str</code> <p>The new path of the column.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the rename operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def rename_column(self, path_from: Union[str, Tuple[str, ...]], new_name: str) -&gt; UpdateSchema:\n    \"\"\"Update the name of a column.\n\n    Args:\n        path_from: The path to the column to be renamed.\n        new_name: The new path of the column.\n\n    Returns:\n        The UpdateSchema with the rename operation staged.\n    \"\"\"\n    path_from = \".\".join(path_from) if isinstance(path_from, tuple) else path_from\n    field_from = self._schema.find_field(path_from, self._case_sensitive)\n\n    if field_from.field_id in self._deletes:\n        raise ValueError(f\"Cannot rename a column that will be deleted: {path_from}\")\n\n    if updated := self._updates.get(field_from.field_id):\n        self._updates[field_from.field_id] = NestedField(\n            field_id=updated.field_id,\n            name=new_name,\n            field_type=updated.field_type,\n            doc=updated.doc,\n            required=updated.required,\n        )\n    else:\n        self._updates[field_from.field_id] = NestedField(\n            field_id=field_from.field_id,\n            name=new_name,\n            field_type=field_from.field_type,\n            doc=field_from.doc,\n            required=field_from.required,\n        )\n\n    # Lookup the field because of casing\n    from_field_correct_casing = self._schema.find_column_name(field_from.field_id)\n    if from_field_correct_casing in self._identifier_field_names:\n        self._identifier_field_names.remove(from_field_correct_casing)\n        new_identifier_path = f\"{from_field_correct_casing[: -len(field_from.name)]}{new_name}\"\n        self._identifier_field_names.add(new_identifier_path)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.update_column","title":"<code>update_column(path, field_type=None, required=None, doc=None)</code>","text":"<p>Update the type of column.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Tuple[str, ...]]</code> <p>The path to the field.</p> required <code>field_type</code> <code>Optional[IcebergType]</code> <p>The new type</p> <code>None</code> <code>required</code> <code>Optional[bool]</code> <p>If the field should be required</p> <code>None</code> <code>doc</code> <code>Optional[str]</code> <p>Documentation describing the column</p> <code>None</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the type update staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def update_column(\n    self,\n    path: Union[str, Tuple[str, ...]],\n    field_type: Optional[IcebergType] = None,\n    required: Optional[bool] = None,\n    doc: Optional[str] = None,\n) -&gt; UpdateSchema:\n    \"\"\"Update the type of column.\n\n    Args:\n        path: The path to the field.\n        field_type: The new type\n        required: If the field should be required\n        doc: Documentation describing the column\n\n    Returns:\n        The UpdateSchema with the type update staged.\n    \"\"\"\n    path = (path,) if isinstance(path, str) else path\n    full_name = \".\".join(path)\n\n    if field_type is None and required is None and doc is None:\n        return self\n\n    field = self._schema.find_field(full_name, self._case_sensitive)\n\n    if field.field_id in self._deletes:\n        raise ValueError(f\"Cannot update a column that will be deleted: {full_name}\")\n\n    if field_type is not None:\n        if not field.field_type.is_primitive:\n            raise ValidationError(f\"Cannot change column type: {field.field_type} is not a primitive\")\n\n        if not self._allow_incompatible_changes and field.field_type != field_type:\n            try:\n                promote(field.field_type, field_type)\n            except ResolveError as e:\n                raise ValidationError(f\"Cannot change column type: {full_name}: {field.field_type} -&gt; {field_type}\") from e\n\n    # if other updates for the same field exist in one transaction:\n    if updated := self._updates.get(field.field_id):\n        self._updates[field.field_id] = NestedField(\n            field_id=updated.field_id,\n            name=updated.name,\n            field_type=field_type or updated.field_type,\n            doc=doc if doc is not None else updated.doc,\n            required=updated.required,\n        )\n    else:\n        self._updates[field.field_id] = NestedField(\n            field_id=field.field_id,\n            name=field.name,\n            field_type=field_type or field.field_type,\n            doc=doc if doc is not None else field.doc,\n            required=field.required,\n        )\n\n    if required is not None:\n        self._set_column_requirement(path, required=required)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/","title":"snapshot","text":""},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots","title":"<code>ManageSnapshots</code>","text":"<p>               Bases: <code>UpdateTableMetadata['ManageSnapshots']</code></p> <p>Run snapshot management operations using APIs.</p> <p>APIs include create branch, create tag, etc.</p> <p>Use table.manage_snapshots().().commit() to run a specific operation. Use table.manage_snapshots().().().commit() to run multiple operations. Pending changes are applied on commit. <p>We can also use context managers to make more changes. For example,</p> <p>with table.manage_snapshots() as ms:    ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class ManageSnapshots(UpdateTableMetadata[\"ManageSnapshots\"]):\n    \"\"\"\n    Run snapshot management operations using APIs.\n\n    APIs include create branch, create tag, etc.\n\n    Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example,\n\n    with table.manage_snapshots() as ms:\n       ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n    \"\"\"\n\n    _updates: Tuple[TableUpdate, ...] = ()\n    _requirements: Tuple[TableRequirement, ...] = ()\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"Apply the pending changes and commit.\"\"\"\n        return self._updates, self._requirements\n\n    def _remove_ref_snapshot(self, ref_name: str) -&gt; ManageSnapshots:\n        \"\"\"Remove a snapshot ref.\n\n        Args:\n            ref_name: branch / tag name to remove\n        Stages the updates and requirements for the remove-snapshot-ref.\n        Returns\n            This method for chaining\n        \"\"\"\n        updates = (RemoveSnapshotRefUpdate(ref_name=ref_name),)\n        requirements = (\n            AssertRefSnapshotId(\n                snapshot_id=self._transaction.table_metadata.refs[ref_name].snapshot_id\n                if ref_name in self._transaction.table_metadata.refs\n                else None,\n                ref=ref_name,\n            ),\n        )\n        self._updates += updates\n        self._requirements += requirements\n        return self\n\n    def create_tag(self, snapshot_id: int, tag_name: str, max_ref_age_ms: Optional[int] = None) -&gt; ManageSnapshots:\n        \"\"\"\n        Create a new tag pointing to the given snapshot id.\n\n        Args:\n            snapshot_id (int): snapshot id of the existing snapshot to tag\n            tag_name (str): name of the tag\n            max_ref_age_ms (Optional[int]): max ref age in milliseconds\n\n        Returns:\n            This for method chaining\n        \"\"\"\n        update, requirement = self._transaction._set_ref_snapshot(\n            snapshot_id=snapshot_id,\n            ref_name=tag_name,\n            type=\"tag\",\n            max_ref_age_ms=max_ref_age_ms,\n        )\n        self._updates += update\n        self._requirements += requirement\n        return self\n\n    def remove_tag(self, tag_name: str) -&gt; ManageSnapshots:\n        \"\"\"\n        Remove a tag.\n\n        Args:\n            tag_name (str): name of tag to remove\n        Returns:\n            This for method chaining\n        \"\"\"\n        return self._remove_ref_snapshot(ref_name=tag_name)\n\n    def create_branch(\n        self,\n        snapshot_id: int,\n        branch_name: str,\n        max_ref_age_ms: Optional[int] = None,\n        max_snapshot_age_ms: Optional[int] = None,\n        min_snapshots_to_keep: Optional[int] = None,\n    ) -&gt; ManageSnapshots:\n        \"\"\"\n        Create a new branch pointing to the given snapshot id.\n\n        Args:\n            snapshot_id (int): snapshot id of existing snapshot at which the branch is created.\n            branch_name (str): name of the new branch\n            max_ref_age_ms (Optional[int]): max ref age in milliseconds\n            max_snapshot_age_ms (Optional[int]): max age of snapshots to keep in milliseconds\n            min_snapshots_to_keep (Optional[int]): min number of snapshots to keep in milliseconds\n        Returns:\n            This for method chaining\n        \"\"\"\n        update, requirement = self._transaction._set_ref_snapshot(\n            snapshot_id=snapshot_id,\n            ref_name=branch_name,\n            type=\"branch\",\n            max_ref_age_ms=max_ref_age_ms,\n            max_snapshot_age_ms=max_snapshot_age_ms,\n            min_snapshots_to_keep=min_snapshots_to_keep,\n        )\n        self._updates += update\n        self._requirements += requirement\n        return self\n\n    def remove_branch(self, branch_name: str) -&gt; ManageSnapshots:\n        \"\"\"\n        Remove a branch.\n\n        Args:\n            branch_name (str): name of branch to remove\n        Returns:\n            This for method chaining\n        \"\"\"\n        return self._remove_ref_snapshot(ref_name=branch_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots._commit","title":"<code>_commit()</code>","text":"<p>Apply the pending changes and commit.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _commit(self) -&gt; UpdatesAndRequirements:\n    \"\"\"Apply the pending changes and commit.\"\"\"\n    return self._updates, self._requirements\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots._remove_ref_snapshot","title":"<code>_remove_ref_snapshot(ref_name)</code>","text":"<p>Remove a snapshot ref.</p> <p>Parameters:</p> Name Type Description Default <code>ref_name</code> <code>str</code> <p>branch / tag name to remove</p> required <p>Stages the updates and requirements for the remove-snapshot-ref. Returns     This method for chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _remove_ref_snapshot(self, ref_name: str) -&gt; ManageSnapshots:\n    \"\"\"Remove a snapshot ref.\n\n    Args:\n        ref_name: branch / tag name to remove\n    Stages the updates and requirements for the remove-snapshot-ref.\n    Returns\n        This method for chaining\n    \"\"\"\n    updates = (RemoveSnapshotRefUpdate(ref_name=ref_name),)\n    requirements = (\n        AssertRefSnapshotId(\n            snapshot_id=self._transaction.table_metadata.refs[ref_name].snapshot_id\n            if ref_name in self._transaction.table_metadata.refs\n            else None,\n            ref=ref_name,\n        ),\n    )\n    self._updates += updates\n    self._requirements += requirements\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.create_branch","title":"<code>create_branch(snapshot_id, branch_name, max_ref_age_ms=None, max_snapshot_age_ms=None, min_snapshots_to_keep=None)</code>","text":"<p>Create a new branch pointing to the given snapshot id.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>snapshot id of existing snapshot at which the branch is created.</p> required <code>branch_name</code> <code>str</code> <p>name of the new branch</p> required <code>max_ref_age_ms</code> <code>Optional[int]</code> <p>max ref age in milliseconds</p> <code>None</code> <code>max_snapshot_age_ms</code> <code>Optional[int]</code> <p>max age of snapshots to keep in milliseconds</p> <code>None</code> <code>min_snapshots_to_keep</code> <code>Optional[int]</code> <p>min number of snapshots to keep in milliseconds</p> <code>None</code> <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def create_branch(\n    self,\n    snapshot_id: int,\n    branch_name: str,\n    max_ref_age_ms: Optional[int] = None,\n    max_snapshot_age_ms: Optional[int] = None,\n    min_snapshots_to_keep: Optional[int] = None,\n) -&gt; ManageSnapshots:\n    \"\"\"\n    Create a new branch pointing to the given snapshot id.\n\n    Args:\n        snapshot_id (int): snapshot id of existing snapshot at which the branch is created.\n        branch_name (str): name of the new branch\n        max_ref_age_ms (Optional[int]): max ref age in milliseconds\n        max_snapshot_age_ms (Optional[int]): max age of snapshots to keep in milliseconds\n        min_snapshots_to_keep (Optional[int]): min number of snapshots to keep in milliseconds\n    Returns:\n        This for method chaining\n    \"\"\"\n    update, requirement = self._transaction._set_ref_snapshot(\n        snapshot_id=snapshot_id,\n        ref_name=branch_name,\n        type=\"branch\",\n        max_ref_age_ms=max_ref_age_ms,\n        max_snapshot_age_ms=max_snapshot_age_ms,\n        min_snapshots_to_keep=min_snapshots_to_keep,\n    )\n    self._updates += update\n    self._requirements += requirement\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.create_tag","title":"<code>create_tag(snapshot_id, tag_name, max_ref_age_ms=None)</code>","text":"<p>Create a new tag pointing to the given snapshot id.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>snapshot id of the existing snapshot to tag</p> required <code>tag_name</code> <code>str</code> <p>name of the tag</p> required <code>max_ref_age_ms</code> <code>Optional[int]</code> <p>max ref age in milliseconds</p> <code>None</code> <p>Returns:</p> Type Description <code>ManageSnapshots</code> <p>This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def create_tag(self, snapshot_id: int, tag_name: str, max_ref_age_ms: Optional[int] = None) -&gt; ManageSnapshots:\n    \"\"\"\n    Create a new tag pointing to the given snapshot id.\n\n    Args:\n        snapshot_id (int): snapshot id of the existing snapshot to tag\n        tag_name (str): name of the tag\n        max_ref_age_ms (Optional[int]): max ref age in milliseconds\n\n    Returns:\n        This for method chaining\n    \"\"\"\n    update, requirement = self._transaction._set_ref_snapshot(\n        snapshot_id=snapshot_id,\n        ref_name=tag_name,\n        type=\"tag\",\n        max_ref_age_ms=max_ref_age_ms,\n    )\n    self._updates += update\n    self._requirements += requirement\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.remove_branch","title":"<code>remove_branch(branch_name)</code>","text":"<p>Remove a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>name of branch to remove</p> required <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def remove_branch(self, branch_name: str) -&gt; ManageSnapshots:\n    \"\"\"\n    Remove a branch.\n\n    Args:\n        branch_name (str): name of branch to remove\n    Returns:\n        This for method chaining\n    \"\"\"\n    return self._remove_ref_snapshot(ref_name=branch_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.remove_tag","title":"<code>remove_tag(tag_name)</code>","text":"<p>Remove a tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag_name</code> <code>str</code> <p>name of tag to remove</p> required <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def remove_tag(self, tag_name: str) -&gt; ManageSnapshots:\n    \"\"\"\n    Remove a tag.\n\n    Args:\n        tag_name (str): name of tag to remove\n    Returns:\n        This for method chaining\n    \"\"\"\n    return self._remove_ref_snapshot(ref_name=tag_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._DeleteFiles","title":"<code>_DeleteFiles</code>","text":"<p>               Bases: <code>_SnapshotProducer['_DeleteFiles']</code></p> <p>Will delete manifest entries from the current snapshot based on the predicate.</p> This will produce a DELETE snapshot <p>Data files were removed and their contents logically deleted and/or delete files were added to delete rows.</p> <p>From the specification</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class _DeleteFiles(_SnapshotProducer[\"_DeleteFiles\"]):\n    \"\"\"Will delete manifest entries from the current snapshot based on the predicate.\n\n    This will produce a DELETE snapshot:\n        Data files were removed and their contents logically deleted and/or delete\n        files were added to delete rows.\n\n    From the specification\n    \"\"\"\n\n    _predicate: BooleanExpression\n    _case_sensitive: bool\n\n    def __init__(\n        self,\n        operation: Operation,\n        transaction: Transaction,\n        io: FileIO,\n        commit_uuid: Optional[uuid.UUID] = None,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    ):\n        super().__init__(operation, transaction, io, commit_uuid, snapshot_properties)\n        self._predicate = AlwaysFalse()\n        self._case_sensitive = True\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        # Only produce a commit when there is something to delete\n        if self.files_affected:\n            return super()._commit()\n        else:\n            return (), ()\n\n    def _build_partition_projection(self, spec_id: int) -&gt; BooleanExpression:\n        schema = self._transaction.table_metadata.schema()\n        spec = self._transaction.table_metadata.specs()[spec_id]\n        project = inclusive_projection(schema, spec, self._case_sensitive)\n        return project(self._predicate)\n\n    @cached_property\n    def partition_filters(self) -&gt; KeyDefaultDict[int, BooleanExpression]:\n        return KeyDefaultDict(self._build_partition_projection)\n\n    def _build_manifest_evaluator(self, spec_id: int) -&gt; Callable[[ManifestFile], bool]:\n        schema = self._transaction.table_metadata.schema()\n        spec = self._transaction.table_metadata.specs()[spec_id]\n        return manifest_evaluator(spec, schema, self.partition_filters[spec_id], self._case_sensitive)\n\n    def delete_by_predicate(self, predicate: BooleanExpression, case_sensitive: bool = True) -&gt; None:\n        self._predicate = Or(self._predicate, predicate)\n        self._case_sensitive = case_sensitive\n\n    @cached_property\n    def _compute_deletes(self) -&gt; Tuple[List[ManifestFile], List[ManifestEntry], bool]:\n        \"\"\"Computes all the delete operation and cache it when nothing changes.\n\n        Returns:\n            - List of existing manifests that are not affected by the delete operation.\n            - The manifest-entries that are deleted based on the metadata.\n            - Flag indicating that rewrites of data-files are needed.\n        \"\"\"\n        schema = self._transaction.table_metadata.schema()\n\n        def _copy_with_new_status(entry: ManifestEntry, status: ManifestEntryStatus) -&gt; ManifestEntry:\n            return ManifestEntry(\n                status=status,\n                snapshot_id=entry.snapshot_id,\n                sequence_number=entry.sequence_number,\n                file_sequence_number=entry.file_sequence_number,\n                data_file=entry.data_file,\n            )\n\n        manifest_evaluators: Dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)\n        strict_metrics_evaluator = _StrictMetricsEvaluator(schema, self._predicate, case_sensitive=self._case_sensitive).eval\n        inclusive_metrics_evaluator = _InclusiveMetricsEvaluator(\n            schema, self._predicate, case_sensitive=self._case_sensitive\n        ).eval\n\n        existing_manifests = []\n        total_deleted_entries = []\n        partial_rewrites_needed = False\n        self._deleted_data_files = set()\n        if snapshot := self._transaction.table_metadata.current_snapshot():\n            for manifest_file in snapshot.manifests(io=self._io):\n                if manifest_file.content == ManifestContent.DATA:\n                    if not manifest_evaluators[manifest_file.partition_spec_id](manifest_file):\n                        # If the manifest isn't relevant, we can just keep it in the manifest-list\n                        existing_manifests.append(manifest_file)\n                    else:\n                        # It is relevant, let's check out the content\n                        deleted_entries = []\n                        existing_entries = []\n                        for entry in manifest_file.fetch_manifest_entry(io=self._io, discard_deleted=True):\n                            if strict_metrics_evaluator(entry.data_file) == ROWS_MUST_MATCH:\n                                # Based on the metadata, it can be dropped right away\n                                deleted_entries.append(_copy_with_new_status(entry, ManifestEntryStatus.DELETED))\n                                self._deleted_data_files.add(entry.data_file)\n                            else:\n                                # Based on the metadata, we cannot determine if it can be deleted\n                                existing_entries.append(_copy_with_new_status(entry, ManifestEntryStatus.EXISTING))\n                                if inclusive_metrics_evaluator(entry.data_file) != ROWS_MIGHT_NOT_MATCH:\n                                    partial_rewrites_needed = True\n\n                        if len(deleted_entries) &gt; 0:\n                            total_deleted_entries += deleted_entries\n\n                            # Rewrite the manifest\n                            if len(existing_entries) &gt; 0:\n                                with write_manifest(\n                                    format_version=self._transaction.table_metadata.format_version,\n                                    spec=self._transaction.table_metadata.specs()[manifest_file.partition_spec_id],\n                                    schema=self._transaction.table_metadata.schema(),\n                                    output_file=self.new_manifest_output(),\n                                    snapshot_id=self._snapshot_id,\n                                ) as writer:\n                                    for existing_entry in existing_entries:\n                                        writer.add_entry(existing_entry)\n                                existing_manifests.append(writer.to_manifest_file())\n                        else:\n                            existing_manifests.append(manifest_file)\n                else:\n                    existing_manifests.append(manifest_file)\n\n        return existing_manifests, total_deleted_entries, partial_rewrites_needed\n\n    def _existing_manifests(self) -&gt; List[ManifestFile]:\n        return self._compute_deletes[0]\n\n    def _deleted_entries(self) -&gt; List[ManifestEntry]:\n        return self._compute_deletes[1]\n\n    @property\n    def rewrites_needed(self) -&gt; bool:\n        \"\"\"Indicate if data files need to be rewritten.\"\"\"\n        return self._compute_deletes[2]\n\n    @property\n    def files_affected(self) -&gt; bool:\n        \"\"\"Indicate if any manifest-entries can be dropped.\"\"\"\n        return len(self._deleted_entries()) &gt; 0\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._DeleteFiles._compute_deletes","title":"<code>_compute_deletes</code>  <code>cached</code> <code>property</code>","text":"<p>Computes all the delete operation and cache it when nothing changes.</p> <p>Returns:</p> Type Description <code>List[ManifestFile]</code> <ul> <li>List of existing manifests that are not affected by the delete operation.</li> </ul> <code>List[ManifestEntry]</code> <ul> <li>The manifest-entries that are deleted based on the metadata.</li> </ul> <code>bool</code> <ul> <li>Flag indicating that rewrites of data-files are needed.</li> </ul>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._DeleteFiles.files_affected","title":"<code>files_affected</code>  <code>property</code>","text":"<p>Indicate if any manifest-entries can be dropped.</p>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._DeleteFiles.rewrites_needed","title":"<code>rewrites_needed</code>  <code>property</code>","text":"<p>Indicate if data files need to be rewritten.</p>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._FastAppendFiles","title":"<code>_FastAppendFiles</code>","text":"<p>               Bases: <code>_SnapshotProducer['_FastAppendFiles']</code></p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class _FastAppendFiles(_SnapshotProducer[\"_FastAppendFiles\"]):\n    def _existing_manifests(self) -&gt; List[ManifestFile]:\n        \"\"\"To determine if there are any existing manifest files.\n\n        A fast append will add another ManifestFile to the ManifestList.\n        All the existing manifest files are considered existing.\n        \"\"\"\n        existing_manifests = []\n\n        if self._parent_snapshot_id is not None:\n            previous_snapshot = self._transaction.table_metadata.snapshot_by_id(self._parent_snapshot_id)\n\n            if previous_snapshot is None:\n                raise ValueError(f\"Snapshot could not be found: {self._parent_snapshot_id}\")\n\n            for manifest in previous_snapshot.manifests(io=self._io):\n                if manifest.has_added_files() or manifest.has_existing_files() or manifest.added_snapshot_id == self._snapshot_id:\n                    existing_manifests.append(manifest)\n\n        return existing_manifests\n\n    def _deleted_entries(self) -&gt; List[ManifestEntry]:\n        \"\"\"To determine if we need to record any deleted manifest entries.\n\n        In case of an append, nothing is deleted.\n        \"\"\"\n        return []\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._FastAppendFiles._deleted_entries","title":"<code>_deleted_entries()</code>","text":"<p>To determine if we need to record any deleted manifest entries.</p> <p>In case of an append, nothing is deleted.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _deleted_entries(self) -&gt; List[ManifestEntry]:\n    \"\"\"To determine if we need to record any deleted manifest entries.\n\n    In case of an append, nothing is deleted.\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._FastAppendFiles._existing_manifests","title":"<code>_existing_manifests()</code>","text":"<p>To determine if there are any existing manifest files.</p> <p>A fast append will add another ManifestFile to the ManifestList. All the existing manifest files are considered existing.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _existing_manifests(self) -&gt; List[ManifestFile]:\n    \"\"\"To determine if there are any existing manifest files.\n\n    A fast append will add another ManifestFile to the ManifestList.\n    All the existing manifest files are considered existing.\n    \"\"\"\n    existing_manifests = []\n\n    if self._parent_snapshot_id is not None:\n        previous_snapshot = self._transaction.table_metadata.snapshot_by_id(self._parent_snapshot_id)\n\n        if previous_snapshot is None:\n            raise ValueError(f\"Snapshot could not be found: {self._parent_snapshot_id}\")\n\n        for manifest in previous_snapshot.manifests(io=self._io):\n            if manifest.has_added_files() or manifest.has_existing_files() or manifest.added_snapshot_id == self._snapshot_id:\n                existing_manifests.append(manifest)\n\n    return existing_manifests\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._MergeAppendFiles","title":"<code>_MergeAppendFiles</code>","text":"<p>               Bases: <code>_FastAppendFiles</code></p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class _MergeAppendFiles(_FastAppendFiles):\n    _target_size_bytes: int\n    _min_count_to_merge: int\n    _merge_enabled: bool\n\n    def __init__(\n        self,\n        operation: Operation,\n        transaction: Transaction,\n        io: FileIO,\n        commit_uuid: Optional[uuid.UUID] = None,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        from pyiceberg.table import TableProperties\n\n        super().__init__(operation, transaction, io, commit_uuid, snapshot_properties)\n        self._target_size_bytes = property_as_int(\n            self._transaction.table_metadata.properties,\n            TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n            TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT,\n        )  # type: ignore\n        self._min_count_to_merge = property_as_int(\n            self._transaction.table_metadata.properties,\n            TableProperties.MANIFEST_MIN_MERGE_COUNT,\n            TableProperties.MANIFEST_MIN_MERGE_COUNT_DEFAULT,\n        )  # type: ignore\n        self._merge_enabled = property_as_bool(\n            self._transaction.table_metadata.properties,\n            TableProperties.MANIFEST_MERGE_ENABLED,\n            TableProperties.MANIFEST_MERGE_ENABLED_DEFAULT,\n        )\n\n    def _process_manifests(self, manifests: List[ManifestFile]) -&gt; List[ManifestFile]:\n        \"\"\"To perform any post-processing on the manifests before writing them to the new snapshot.\n\n        In _MergeAppendFiles, we merge manifests based on the target size and the minimum count to merge\n        if automatic merge is enabled.\n        \"\"\"\n        unmerged_data_manifests = [manifest for manifest in manifests if manifest.content == ManifestContent.DATA]\n        unmerged_deletes_manifests = [manifest for manifest in manifests if manifest.content == ManifestContent.DELETES]\n\n        data_manifest_merge_manager = _ManifestMergeManager(\n            target_size_bytes=self._target_size_bytes,\n            min_count_to_merge=self._min_count_to_merge,\n            merge_enabled=self._merge_enabled,\n            snapshot_producer=self,\n        )\n\n        return data_manifest_merge_manager.merge_manifests(unmerged_data_manifests) + unmerged_deletes_manifests\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._MergeAppendFiles._process_manifests","title":"<code>_process_manifests(manifests)</code>","text":"<p>To perform any post-processing on the manifests before writing them to the new snapshot.</p> <p>In _MergeAppendFiles, we merge manifests based on the target size and the minimum count to merge if automatic merge is enabled.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _process_manifests(self, manifests: List[ManifestFile]) -&gt; List[ManifestFile]:\n    \"\"\"To perform any post-processing on the manifests before writing them to the new snapshot.\n\n    In _MergeAppendFiles, we merge manifests based on the target size and the minimum count to merge\n    if automatic merge is enabled.\n    \"\"\"\n    unmerged_data_manifests = [manifest for manifest in manifests if manifest.content == ManifestContent.DATA]\n    unmerged_deletes_manifests = [manifest for manifest in manifests if manifest.content == ManifestContent.DELETES]\n\n    data_manifest_merge_manager = _ManifestMergeManager(\n        target_size_bytes=self._target_size_bytes,\n        min_count_to_merge=self._min_count_to_merge,\n        merge_enabled=self._merge_enabled,\n        snapshot_producer=self,\n    )\n\n    return data_manifest_merge_manager.merge_manifests(unmerged_data_manifests) + unmerged_deletes_manifests\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._OverwriteFiles","title":"<code>_OverwriteFiles</code>","text":"<p>               Bases: <code>_SnapshotProducer['_OverwriteFiles']</code></p> <p>Overwrites data from the table. This will produce an OVERWRITE snapshot.</p> <p>Data and delete files were added and removed in a logical overwrite operation.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class _OverwriteFiles(_SnapshotProducer[\"_OverwriteFiles\"]):\n    \"\"\"Overwrites data from the table. This will produce an OVERWRITE snapshot.\n\n    Data and delete files were added and removed in a logical overwrite operation.\n    \"\"\"\n\n    def _existing_manifests(self) -&gt; List[ManifestFile]:\n        \"\"\"Determine if there are any existing manifest files.\"\"\"\n        existing_files = []\n\n        if snapshot := self._transaction.table_metadata.current_snapshot():\n            for manifest_file in snapshot.manifests(io=self._io):\n                entries = manifest_file.fetch_manifest_entry(io=self._io, discard_deleted=True)\n                found_deleted_data_files = [entry.data_file for entry in entries if entry.data_file in self._deleted_data_files]\n\n                if len(found_deleted_data_files) == 0:\n                    existing_files.append(manifest_file)\n                else:\n                    # We have to rewrite the manifest file without the deleted data files\n                    if any(entry.data_file not in found_deleted_data_files for entry in entries):\n                        with write_manifest(\n                            format_version=self._transaction.table_metadata.format_version,\n                            spec=self._transaction.table_metadata.specs()[manifest_file.partition_spec_id],\n                            schema=self._transaction.table_metadata.schema(),\n                            output_file=self.new_manifest_output(),\n                            snapshot_id=self._snapshot_id,\n                        ) as writer:\n                            [\n                                writer.add_entry(\n                                    ManifestEntry(\n                                        status=ManifestEntryStatus.EXISTING,\n                                        snapshot_id=entry.snapshot_id,\n                                        sequence_number=entry.sequence_number,\n                                        file_sequence_number=entry.file_sequence_number,\n                                        data_file=entry.data_file,\n                                    )\n                                )\n                                for entry in entries\n                                if entry.data_file not in found_deleted_data_files\n                            ]\n                        existing_files.append(writer.to_manifest_file())\n        return existing_files\n\n    def _deleted_entries(self) -&gt; List[ManifestEntry]:\n        \"\"\"To determine if we need to record any deleted entries.\n\n        With a full overwrite all the entries are considered deleted.\n        With partial overwrites we have to use the predicate to evaluate\n        which entries are affected.\n        \"\"\"\n        if self._parent_snapshot_id is not None:\n            previous_snapshot = self._transaction.table_metadata.snapshot_by_id(self._parent_snapshot_id)\n            if previous_snapshot is None:\n                # This should never happen since you cannot overwrite an empty table\n                raise ValueError(f\"Could not find the previous snapshot: {self._parent_snapshot_id}\")\n\n            executor = ExecutorFactory.get_or_create()\n\n            def _get_entries(manifest: ManifestFile) -&gt; List[ManifestEntry]:\n                return [\n                    ManifestEntry(\n                        status=ManifestEntryStatus.DELETED,\n                        snapshot_id=entry.snapshot_id,\n                        sequence_number=entry.sequence_number,\n                        file_sequence_number=entry.file_sequence_number,\n                        data_file=entry.data_file,\n                    )\n                    for entry in manifest.fetch_manifest_entry(self._io, discard_deleted=True)\n                    if entry.data_file.content == DataFileContent.DATA and entry.data_file in self._deleted_data_files\n                ]\n\n            list_of_entries = executor.map(_get_entries, previous_snapshot.manifests(self._io))\n            return list(itertools.chain(*list_of_entries))\n        else:\n            return []\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._OverwriteFiles._deleted_entries","title":"<code>_deleted_entries()</code>","text":"<p>To determine if we need to record any deleted entries.</p> <p>With a full overwrite all the entries are considered deleted. With partial overwrites we have to use the predicate to evaluate which entries are affected.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _deleted_entries(self) -&gt; List[ManifestEntry]:\n    \"\"\"To determine if we need to record any deleted entries.\n\n    With a full overwrite all the entries are considered deleted.\n    With partial overwrites we have to use the predicate to evaluate\n    which entries are affected.\n    \"\"\"\n    if self._parent_snapshot_id is not None:\n        previous_snapshot = self._transaction.table_metadata.snapshot_by_id(self._parent_snapshot_id)\n        if previous_snapshot is None:\n            # This should never happen since you cannot overwrite an empty table\n            raise ValueError(f\"Could not find the previous snapshot: {self._parent_snapshot_id}\")\n\n        executor = ExecutorFactory.get_or_create()\n\n        def _get_entries(manifest: ManifestFile) -&gt; List[ManifestEntry]:\n            return [\n                ManifestEntry(\n                    status=ManifestEntryStatus.DELETED,\n                    snapshot_id=entry.snapshot_id,\n                    sequence_number=entry.sequence_number,\n                    file_sequence_number=entry.file_sequence_number,\n                    data_file=entry.data_file,\n                )\n                for entry in manifest.fetch_manifest_entry(self._io, discard_deleted=True)\n                if entry.data_file.content == DataFileContent.DATA and entry.data_file in self._deleted_data_files\n            ]\n\n        list_of_entries = executor.map(_get_entries, previous_snapshot.manifests(self._io))\n        return list(itertools.chain(*list_of_entries))\n    else:\n        return []\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._OverwriteFiles._existing_manifests","title":"<code>_existing_manifests()</code>","text":"<p>Determine if there are any existing manifest files.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _existing_manifests(self) -&gt; List[ManifestFile]:\n    \"\"\"Determine if there are any existing manifest files.\"\"\"\n    existing_files = []\n\n    if snapshot := self._transaction.table_metadata.current_snapshot():\n        for manifest_file in snapshot.manifests(io=self._io):\n            entries = manifest_file.fetch_manifest_entry(io=self._io, discard_deleted=True)\n            found_deleted_data_files = [entry.data_file for entry in entries if entry.data_file in self._deleted_data_files]\n\n            if len(found_deleted_data_files) == 0:\n                existing_files.append(manifest_file)\n            else:\n                # We have to rewrite the manifest file without the deleted data files\n                if any(entry.data_file not in found_deleted_data_files for entry in entries):\n                    with write_manifest(\n                        format_version=self._transaction.table_metadata.format_version,\n                        spec=self._transaction.table_metadata.specs()[manifest_file.partition_spec_id],\n                        schema=self._transaction.table_metadata.schema(),\n                        output_file=self.new_manifest_output(),\n                        snapshot_id=self._snapshot_id,\n                    ) as writer:\n                        [\n                            writer.add_entry(\n                                ManifestEntry(\n                                    status=ManifestEntryStatus.EXISTING,\n                                    snapshot_id=entry.snapshot_id,\n                                    sequence_number=entry.sequence_number,\n                                    file_sequence_number=entry.file_sequence_number,\n                                    data_file=entry.data_file,\n                                )\n                            )\n                            for entry in entries\n                            if entry.data_file not in found_deleted_data_files\n                        ]\n                    existing_files.append(writer.to_manifest_file())\n    return existing_files\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._SnapshotProducer","title":"<code>_SnapshotProducer</code>","text":"<p>               Bases: <code>UpdateTableMetadata[U]</code>, <code>Generic[U]</code></p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class _SnapshotProducer(UpdateTableMetadata[U], Generic[U]):\n    commit_uuid: uuid.UUID\n    _io: FileIO\n    _operation: Operation\n    _snapshot_id: int\n    _parent_snapshot_id: Optional[int]\n    _added_data_files: List[DataFile]\n    _manifest_num_counter: itertools.count[int]\n    _deleted_data_files: Set[DataFile]\n\n    def __init__(\n        self,\n        operation: Operation,\n        transaction: Transaction,\n        io: FileIO,\n        commit_uuid: Optional[uuid.UUID] = None,\n        snapshot_properties: Dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        super().__init__(transaction)\n        self.commit_uuid = commit_uuid or uuid.uuid4()\n        self._io = io\n        self._operation = operation\n        self._snapshot_id = self._transaction.table_metadata.new_snapshot_id()\n        # Since we only support the main branch for now\n        self._parent_snapshot_id = (\n            snapshot.snapshot_id if (snapshot := self._transaction.table_metadata.current_snapshot()) else None\n        )\n        self._added_data_files = []\n        self._deleted_data_files = set()\n        self.snapshot_properties = snapshot_properties\n        self._manifest_num_counter = itertools.count(0)\n\n    def append_data_file(self, data_file: DataFile) -&gt; _SnapshotProducer[U]:\n        self._added_data_files.append(data_file)\n        return self\n\n    def delete_data_file(self, data_file: DataFile) -&gt; _SnapshotProducer[U]:\n        self._deleted_data_files.add(data_file)\n        return self\n\n    @abstractmethod\n    def _deleted_entries(self) -&gt; List[ManifestEntry]: ...\n\n    @abstractmethod\n    def _existing_manifests(self) -&gt; List[ManifestFile]: ...\n\n    def _process_manifests(self, manifests: List[ManifestFile]) -&gt; List[ManifestFile]:\n        \"\"\"To perform any post-processing on the manifests before writing them to the new snapshot.\"\"\"\n        return manifests\n\n    def _manifests(self) -&gt; List[ManifestFile]:\n        def _write_added_manifest() -&gt; List[ManifestFile]:\n            if self._added_data_files:\n                with write_manifest(\n                    format_version=self._transaction.table_metadata.format_version,\n                    spec=self._transaction.table_metadata.spec(),\n                    schema=self._transaction.table_metadata.schema(),\n                    output_file=self.new_manifest_output(),\n                    snapshot_id=self._snapshot_id,\n                ) as writer:\n                    for data_file in self._added_data_files:\n                        writer.add(\n                            ManifestEntry(\n                                status=ManifestEntryStatus.ADDED,\n                                snapshot_id=self._snapshot_id,\n                                sequence_number=None,\n                                file_sequence_number=None,\n                                data_file=data_file,\n                            )\n                        )\n                return [writer.to_manifest_file()]\n            else:\n                return []\n\n        def _write_delete_manifest() -&gt; List[ManifestFile]:\n            # Check if we need to mark the files as deleted\n            deleted_entries = self._deleted_entries()\n            if len(deleted_entries) &gt; 0:\n                deleted_manifests = []\n                partition_groups: Dict[int, List[ManifestEntry]] = defaultdict(list)\n                for deleted_entry in deleted_entries:\n                    partition_groups[deleted_entry.data_file.spec_id].append(deleted_entry)\n                for spec_id, entries in partition_groups.items():\n                    with write_manifest(\n                        format_version=self._transaction.table_metadata.format_version,\n                        spec=self._transaction.table_metadata.specs()[spec_id],\n                        schema=self._transaction.table_metadata.schema(),\n                        output_file=self.new_manifest_output(),\n                        snapshot_id=self._snapshot_id,\n                    ) as writer:\n                        for entry in entries:\n                            writer.add_entry(entry)\n                    deleted_manifests.append(writer.to_manifest_file())\n                return deleted_manifests\n            else:\n                return []\n\n        executor = ExecutorFactory.get_or_create()\n\n        added_manifests = executor.submit(_write_added_manifest)\n        delete_manifests = executor.submit(_write_delete_manifest)\n        existing_manifests = executor.submit(self._existing_manifests)\n\n        return self._process_manifests(added_manifests.result() + delete_manifests.result() + existing_manifests.result())\n\n    def _summary(self, snapshot_properties: Dict[str, str] = EMPTY_DICT) -&gt; Summary:\n        from pyiceberg.table import TableProperties\n\n        ssc = SnapshotSummaryCollector()\n        partition_summary_limit = int(\n            self._transaction.table_metadata.properties.get(\n                TableProperties.WRITE_PARTITION_SUMMARY_LIMIT, TableProperties.WRITE_PARTITION_SUMMARY_LIMIT_DEFAULT\n            )\n        )\n        ssc.set_partition_summary_limit(partition_summary_limit)\n\n        for data_file in self._added_data_files:\n            ssc.add_file(\n                data_file=data_file,\n                partition_spec=self._transaction.table_metadata.spec(),\n                schema=self._transaction.table_metadata.schema(),\n            )\n\n        if len(self._deleted_data_files) &gt; 0:\n            specs = self._transaction.table_metadata.specs()\n            for data_file in self._deleted_data_files:\n                ssc.remove_file(\n                    data_file=data_file,\n                    partition_spec=specs[data_file.spec_id],\n                    schema=self._transaction.table_metadata.schema(),\n                )\n\n        previous_snapshot = (\n            self._transaction.table_metadata.snapshot_by_id(self._parent_snapshot_id)\n            if self._parent_snapshot_id is not None\n            else None\n        )\n\n        return update_snapshot_summaries(\n            summary=Summary(operation=self._operation, **ssc.build(), **snapshot_properties),\n            previous_summary=previous_snapshot.summary if previous_snapshot is not None else None,\n            truncate_full_table=self._operation == Operation.OVERWRITE,\n        )\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        new_manifests = self._manifests()\n        next_sequence_number = self._transaction.table_metadata.next_sequence_number()\n\n        summary = self._summary(self.snapshot_properties)\n        file_name = _new_manifest_list_file_name(\n            snapshot_id=self._snapshot_id,\n            attempt=0,\n            commit_uuid=self.commit_uuid,\n        )\n        location_provider = self._transaction._table.location_provider()\n        manifest_list_file_path = location_provider.new_metadata_location(file_name)\n        with write_manifest_list(\n            format_version=self._transaction.table_metadata.format_version,\n            output_file=self._io.new_output(manifest_list_file_path),\n            snapshot_id=self._snapshot_id,\n            parent_snapshot_id=self._parent_snapshot_id,\n            sequence_number=next_sequence_number,\n        ) as writer:\n            writer.add_manifests(new_manifests)\n\n        snapshot = Snapshot(\n            snapshot_id=self._snapshot_id,\n            parent_snapshot_id=self._parent_snapshot_id,\n            manifest_list=manifest_list_file_path,\n            sequence_number=next_sequence_number,\n            summary=summary,\n            schema_id=self._transaction.table_metadata.current_schema_id,\n        )\n\n        return (\n            (\n                AddSnapshotUpdate(snapshot=snapshot),\n                SetSnapshotRefUpdate(\n                    snapshot_id=self._snapshot_id, parent_snapshot_id=self._parent_snapshot_id, ref_name=\"main\", type=\"branch\"\n                ),\n            ),\n            (AssertRefSnapshotId(snapshot_id=self._transaction.table_metadata.current_snapshot_id, ref=\"main\"),),\n        )\n\n    @property\n    def snapshot_id(self) -&gt; int:\n        return self._snapshot_id\n\n    def spec(self, spec_id: int) -&gt; PartitionSpec:\n        return self._transaction.table_metadata.specs()[spec_id]\n\n    def new_manifest_writer(self, spec: PartitionSpec) -&gt; ManifestWriter:\n        return write_manifest(\n            format_version=self._transaction.table_metadata.format_version,\n            spec=spec,\n            schema=self._transaction.table_metadata.schema(),\n            output_file=self.new_manifest_output(),\n            snapshot_id=self._snapshot_id,\n        )\n\n    def new_manifest_output(self) -&gt; OutputFile:\n        location_provider = self._transaction._table.location_provider()\n        file_name = _new_manifest_file_name(num=next(self._manifest_num_counter), commit_uuid=self.commit_uuid)\n        file_path = location_provider.new_metadata_location(file_name)\n        return self._io.new_output(file_path)\n\n    def fetch_manifest_entry(self, manifest: ManifestFile, discard_deleted: bool = True) -&gt; List[ManifestEntry]:\n        return manifest.fetch_manifest_entry(io=self._io, discard_deleted=discard_deleted)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot._SnapshotProducer._process_manifests","title":"<code>_process_manifests(manifests)</code>","text":"<p>To perform any post-processing on the manifests before writing them to the new snapshot.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def _process_manifests(self, manifests: List[ManifestFile]) -&gt; List[ManifestFile]:\n    \"\"\"To perform any post-processing on the manifests before writing them to the new snapshot.\"\"\"\n    return manifests\n</code></pre>"},{"location":"reference/pyiceberg/table/update/spec/","title":"spec","text":""},{"location":"reference/pyiceberg/table/update/statistics/","title":"statistics","text":""},{"location":"reference/pyiceberg/table/update/statistics/#pyiceberg.table.update.statistics.UpdateStatistics","title":"<code>UpdateStatistics</code>","text":"<p>               Bases: <code>UpdateTableMetadata['UpdateStatistics']</code></p> <p>Run statistics management operations using APIs.</p> <p>APIs include set_statistics and remove statistics operations.</p> <p>Use table.update_statistics().().commit() to run a specific operation. Use table.update_statistics().().().commit() to run multiple operations. <p>Pending changes are applied on commit.</p> <p>We can also use context managers to make more changes. For example:</p> <p>with table.update_statistics() as update:     update.set_statistics(statistics_file=statistics_file)     update.remove_statistics(snapshot_id=2)</p> Source code in <code>pyiceberg/table/update/statistics.py</code> <pre><code>class UpdateStatistics(UpdateTableMetadata[\"UpdateStatistics\"]):\n    \"\"\"\n    Run statistics management operations using APIs.\n\n    APIs include set_statistics and remove statistics operations.\n\n    Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example:\n\n    with table.update_statistics() as update:\n        update.set_statistics(statistics_file=statistics_file)\n        update.remove_statistics(snapshot_id=2)\n    \"\"\"\n\n    _updates: Tuple[TableUpdate, ...] = ()\n\n    def __init__(self, transaction: \"Transaction\") -&gt; None:\n        super().__init__(transaction)\n\n    def set_statistics(self, statistics_file: StatisticsFile) -&gt; \"UpdateStatistics\":\n        self._updates += (\n            SetStatisticsUpdate(\n                statistics=statistics_file,\n            ),\n        )\n\n        return self\n\n    def remove_statistics(self, snapshot_id: int) -&gt; \"UpdateStatistics\":\n        self._updates = (\n            RemoveStatisticsUpdate(\n                snapshot_id=snapshot_id,\n            ),\n        )\n\n        return self\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        return self._updates, ()\n</code></pre>"},{"location":"reference/pyiceberg/utils/","title":"utils","text":""},{"location":"reference/pyiceberg/utils/bin_packing/","title":"bin_packing","text":""},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator","title":"<code>PackingIterator</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>class PackingIterator(Generic[T]):\n    bins: List[Bin[T]]\n\n    def __init__(\n        self,\n        items: Iterable[T],\n        target_weight: int,\n        lookback: int,\n        weight_func: Callable[[T], int],\n        largest_bin_first: bool = False,\n    ) -&gt; None:\n        self.items = iter(items)\n        self.target_weight = target_weight\n        self.lookback = lookback\n        self.weight_func = weight_func\n        self.largest_bin_first = largest_bin_first\n        self.bins = []\n\n    def __iter__(self) -&gt; PackingIterator[T]:\n        \"\"\"Return an iterator for the PackingIterator class.\"\"\"\n        return self\n\n    def __next__(self) -&gt; List[T]:\n        \"\"\"Return the next item when iterating over the PackingIterator class.\"\"\"\n        while True:\n            try:\n                item = next(self.items)\n                weight = self.weight_func(item)\n                bin_ = self.find_bin(weight)\n                if bin_ is not None:\n                    bin_.add(item, weight)\n                else:\n                    bin_ = Bin(self.target_weight)\n                    bin_.add(item, weight)\n                    self.bins.append(bin_)\n\n                    if len(self.bins) &gt; self.lookback:\n                        return self.remove_bin().items\n            except StopIteration:\n                break\n\n        if len(self.bins) == 0:\n            raise StopIteration()\n\n        return self.remove_bin().items\n\n    def find_bin(self, weight: int) -&gt; Optional[Bin[T]]:\n        for bin_ in self.bins:\n            if bin_.can_add(weight):\n                return bin_\n        return None\n\n    def remove_bin(self) -&gt; Bin[T]:\n        if self.largest_bin_first:\n            bin_ = max(self.bins, key=lambda b: b.weight())\n            self.bins.remove(bin_)\n            return bin_\n        else:\n            return self.bins.pop(0)\n</code></pre>"},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the PackingIterator class.</p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>def __iter__(self) -&gt; PackingIterator[T]:\n    \"\"\"Return an iterator for the PackingIterator class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the PackingIterator class.</p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>def __next__(self) -&gt; List[T]:\n    \"\"\"Return the next item when iterating over the PackingIterator class.\"\"\"\n    while True:\n        try:\n            item = next(self.items)\n            weight = self.weight_func(item)\n            bin_ = self.find_bin(weight)\n            if bin_ is not None:\n                bin_.add(item, weight)\n            else:\n                bin_ = Bin(self.target_weight)\n                bin_.add(item, weight)\n                self.bins.append(bin_)\n\n                if len(self.bins) &gt; self.lookback:\n                    return self.remove_bin().items\n        except StopIteration:\n            break\n\n    if len(self.bins) == 0:\n        raise StopIteration()\n\n    return self.remove_bin().items\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/","title":"concurrent","text":"<p>Concurrency concepts that support efficient multi-threading.</p>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory","title":"<code>ExecutorFactory</code>","text":"Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>class ExecutorFactory:\n    _instance: Optional[Executor] = None\n\n    @staticmethod\n    def get_or_create() -&gt; Executor:\n        \"\"\"Return the same executor in each call.\"\"\"\n        if ExecutorFactory._instance is None:\n            max_workers = ExecutorFactory.max_workers()\n            ExecutorFactory._instance = ThreadPoolExecutor(max_workers=max_workers)\n\n        return ExecutorFactory._instance\n\n    @staticmethod\n    def max_workers() -&gt; Optional[int]:\n        \"\"\"Return the max number of workers configured.\"\"\"\n        return Config().get_int(\"max-workers\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory.get_or_create","title":"<code>get_or_create()</code>  <code>staticmethod</code>","text":"<p>Return the same executor in each call.</p> Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>@staticmethod\ndef get_or_create() -&gt; Executor:\n    \"\"\"Return the same executor in each call.\"\"\"\n    if ExecutorFactory._instance is None:\n        max_workers = ExecutorFactory.max_workers()\n        ExecutorFactory._instance = ThreadPoolExecutor(max_workers=max_workers)\n\n    return ExecutorFactory._instance\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory.max_workers","title":"<code>max_workers()</code>  <code>staticmethod</code>","text":"<p>Return the max number of workers configured.</p> Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>@staticmethod\ndef max_workers() -&gt; Optional[int]:\n    \"\"\"Return the max number of workers configured.\"\"\"\n    return Config().get_int(\"max-workers\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/","title":"config","text":""},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config","title":"<code>Config</code>","text":"Source code in <code>pyiceberg/utils/config.py</code> <pre><code>class Config:\n    config: RecursiveDict\n\n    def __init__(self) -&gt; None:\n        config = self._from_configuration_files() or {}\n        config = merge_config(config, self._from_environment_variables(config))\n        self.config = FrozenDict(**config)\n\n    @staticmethod\n    def _from_configuration_files() -&gt; Optional[RecursiveDict]:\n        \"\"\"Load the first configuration file that its finds.\n\n        Will first look in the PYICEBERG_HOME env variable,\n        and then in the home directory.\n        \"\"\"\n\n        def _load_yaml(directory: Optional[str]) -&gt; Optional[RecursiveDict]:\n            if directory:\n                path = os.path.join(directory, PYICEBERG_YML)\n                if os.path.isfile(path):\n                    with open(path, encoding=UTF8) as f:\n                        yml_str = f.read()\n                    file_config = strictyaml.load(yml_str).data\n                    file_config_lowercase = _lowercase_dictionary_keys(file_config)\n                    return file_config_lowercase\n            return None\n\n        # Directories to search for the configuration file\n        # The current search order is: PYICEBERG_HOME, home directory, then current directory\n        search_dirs = [os.environ.get(PYICEBERG_HOME), os.path.expanduser(\"~\"), os.getcwd()]\n\n        for directory in search_dirs:\n            if config := _load_yaml(directory):\n                return config\n\n        # Didn't find a config\n        return None\n\n    @staticmethod\n    def _from_environment_variables(config: RecursiveDict) -&gt; RecursiveDict:\n        \"\"\"Read the environment variables, to check if there are any prepended by PYICEBERG_.\n\n        Args:\n            config: Existing configuration that's being amended with configuration from environment variables.\n\n        Returns:\n            Amended configuration.\n        \"\"\"\n\n        def set_property(_config: RecursiveDict, path: List[str], config_value: str) -&gt; None:\n            while len(path) &gt; 0:\n                element = path.pop(0)\n                if len(path) == 0:\n                    # We're at the end\n                    _config[element] = config_value\n                else:\n                    # We have to go deeper\n                    if element not in _config:\n                        _config[element] = {}\n                    if isinstance(_config[element], dict):\n                        _config = _config[element]  # type: ignore\n                    else:\n                        raise ValueError(\n                            f\"Incompatible configurations, merging dict with a value: {'.'.join(path)}, value: {config_value}\"\n                        )\n\n        for env_var, config_value in os.environ.items():\n            # Make it lowercase to make it case-insensitive\n            env_var_lower = env_var.lower()\n            if env_var_lower.startswith(PYICEBERG.lower()):\n                key = env_var_lower[len(PYICEBERG) :]\n                parts = key.split(\"__\", maxsplit=2)\n                parts_normalized = [part.replace(\"__\", \".\").replace(\"_\", \"-\") for part in parts]\n                set_property(config, parts_normalized, config_value)\n\n        return config\n\n    def get_default_catalog_name(self) -&gt; str:\n        \"\"\"Return the default catalog name.\n\n        Returns: The name of the default catalog in `default-catalog`.\n                 Returns `default` when the key cannot be found in the config file.\n        \"\"\"\n        if default_catalog_name := self.config.get(DEFAULT_CATALOG):\n            if not isinstance(default_catalog_name, str):\n                raise ValueError(f\"Default catalog name should be a str: {default_catalog_name}\")\n            return default_catalog_name\n        return DEFAULT\n\n    def get_catalog_config(self, catalog_name: str) -&gt; Optional[RecursiveDict]:\n        if CATALOG in self.config:\n            catalog_name_lower = catalog_name.lower()\n            catalogs = self.config[CATALOG]\n            if not isinstance(catalogs, dict):\n                raise ValueError(f\"Catalog configurations needs to be an object: {catalog_name}\")\n            if catalog_name_lower in catalogs:\n                catalog_conf = catalogs[catalog_name_lower]\n                assert isinstance(catalog_conf, dict), f\"Configuration path catalogs.{catalog_name_lower} needs to be an object\"\n                return catalog_conf\n        return None\n\n    def get_int(self, key: str) -&gt; Optional[int]:\n        if (val := self.config.get(key)) is not None:\n            try:\n                return int(val)  # type: ignore\n            except ValueError as err:\n                raise ValueError(f\"{key} should be an integer or left unset. Current value: {val}\") from err\n        return None\n\n    def get_bool(self, key: str) -&gt; Optional[bool]:\n        if (val := self.config.get(key)) is not None:\n            try:\n                return strtobool(val)  # type: ignore\n            except ValueError as err:\n                raise ValueError(f\"{key} should be a boolean or left unset. Current value: {val}\") from err\n        return None\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config._from_configuration_files","title":"<code>_from_configuration_files()</code>  <code>staticmethod</code>","text":"<p>Load the first configuration file that its finds.</p> <p>Will first look in the PYICEBERG_HOME env variable, and then in the home directory.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>@staticmethod\ndef _from_configuration_files() -&gt; Optional[RecursiveDict]:\n    \"\"\"Load the first configuration file that its finds.\n\n    Will first look in the PYICEBERG_HOME env variable,\n    and then in the home directory.\n    \"\"\"\n\n    def _load_yaml(directory: Optional[str]) -&gt; Optional[RecursiveDict]:\n        if directory:\n            path = os.path.join(directory, PYICEBERG_YML)\n            if os.path.isfile(path):\n                with open(path, encoding=UTF8) as f:\n                    yml_str = f.read()\n                file_config = strictyaml.load(yml_str).data\n                file_config_lowercase = _lowercase_dictionary_keys(file_config)\n                return file_config_lowercase\n        return None\n\n    # Directories to search for the configuration file\n    # The current search order is: PYICEBERG_HOME, home directory, then current directory\n    search_dirs = [os.environ.get(PYICEBERG_HOME), os.path.expanduser(\"~\"), os.getcwd()]\n\n    for directory in search_dirs:\n        if config := _load_yaml(directory):\n            return config\n\n    # Didn't find a config\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config._from_environment_variables","title":"<code>_from_environment_variables(config)</code>  <code>staticmethod</code>","text":"<p>Read the environment variables, to check if there are any prepended by PYICEBERG_.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RecursiveDict</code> <p>Existing configuration that's being amended with configuration from environment variables.</p> required <p>Returns:</p> Type Description <code>RecursiveDict</code> <p>Amended configuration.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>@staticmethod\ndef _from_environment_variables(config: RecursiveDict) -&gt; RecursiveDict:\n    \"\"\"Read the environment variables, to check if there are any prepended by PYICEBERG_.\n\n    Args:\n        config: Existing configuration that's being amended with configuration from environment variables.\n\n    Returns:\n        Amended configuration.\n    \"\"\"\n\n    def set_property(_config: RecursiveDict, path: List[str], config_value: str) -&gt; None:\n        while len(path) &gt; 0:\n            element = path.pop(0)\n            if len(path) == 0:\n                # We're at the end\n                _config[element] = config_value\n            else:\n                # We have to go deeper\n                if element not in _config:\n                    _config[element] = {}\n                if isinstance(_config[element], dict):\n                    _config = _config[element]  # type: ignore\n                else:\n                    raise ValueError(\n                        f\"Incompatible configurations, merging dict with a value: {'.'.join(path)}, value: {config_value}\"\n                    )\n\n    for env_var, config_value in os.environ.items():\n        # Make it lowercase to make it case-insensitive\n        env_var_lower = env_var.lower()\n        if env_var_lower.startswith(PYICEBERG.lower()):\n            key = env_var_lower[len(PYICEBERG) :]\n            parts = key.split(\"__\", maxsplit=2)\n            parts_normalized = [part.replace(\"__\", \".\").replace(\"_\", \"-\") for part in parts]\n            set_property(config, parts_normalized, config_value)\n\n    return config\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config.get_default_catalog_name","title":"<code>get_default_catalog_name()</code>","text":"<p>Return the default catalog name.</p> <p>The name of the default catalog in `default-catalog`.</p> Type Description <code>str</code> <p>Returns <code>default</code> when the key cannot be found in the config file.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>def get_default_catalog_name(self) -&gt; str:\n    \"\"\"Return the default catalog name.\n\n    Returns: The name of the default catalog in `default-catalog`.\n             Returns `default` when the key cannot be found in the config file.\n    \"\"\"\n    if default_catalog_name := self.config.get(DEFAULT_CATALOG):\n        if not isinstance(default_catalog_name, str):\n            raise ValueError(f\"Default catalog name should be a str: {default_catalog_name}\")\n        return default_catalog_name\n    return DEFAULT\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config._lowercase_dictionary_keys","title":"<code>_lowercase_dictionary_keys(input_dict)</code>","text":"<p>Lowers all the keys of a dictionary in a recursive manner, to make the lookup case-insensitive.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>def _lowercase_dictionary_keys(input_dict: RecursiveDict) -&gt; RecursiveDict:\n    \"\"\"Lowers all the keys of a dictionary in a recursive manner, to make the lookup case-insensitive.\"\"\"\n    return {k.lower(): _lowercase_dictionary_keys(v) if isinstance(v, dict) else v for k, v in input_dict.items()}\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.merge_config","title":"<code>merge_config(lhs, rhs)</code>","text":"<p>Merge right-hand side into the left-hand side.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>def merge_config(lhs: RecursiveDict, rhs: RecursiveDict) -&gt; RecursiveDict:\n    \"\"\"Merge right-hand side into the left-hand side.\"\"\"\n    new_config = lhs.copy()\n    for rhs_key, rhs_value in rhs.items():\n        if rhs_key in new_config:\n            lhs_value = new_config[rhs_key]\n            if isinstance(lhs_value, dict) and isinstance(rhs_value, dict):\n                # If they are both dicts, then we have to go deeper\n                new_config[rhs_key] = merge_config(lhs_value, rhs_value)\n            else:\n                # Take the non-null value, with precedence on rhs\n                new_config[rhs_key] = rhs_value or lhs_value\n        else:\n            # New key\n            new_config[rhs_key] = rhs_value\n\n    return new_config\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/","title":"datetime","text":"<p>Helper methods for working with date/time representations.</p>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.date_str_to_days","title":"<code>date_str_to_days(date_str)</code>","text":"<p>Convert an ISO-8601 formatted date to days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def date_str_to_days(date_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted date to days from 1970-01-01.\"\"\"\n    return (date.fromisoformat(date_str) - EPOCH_DATE).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.date_to_days","title":"<code>date_to_days(date_val)</code>","text":"<p>Convert a Python date object to days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def date_to_days(date_val: date) -&gt; int:\n    \"\"\"Convert a Python date object to days from 1970-01-01.\"\"\"\n    return (date_val - EPOCH_DATE).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.datetime_to_micros","title":"<code>datetime_to_micros(dt)</code>","text":"<p>Convert a datetime to microseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def datetime_to_micros(dt: datetime) -&gt; int:\n    \"\"\"Convert a datetime to microseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if dt.tzinfo:\n        delta = dt - EPOCH_TIMESTAMPTZ\n    else:\n        delta = dt - EPOCH_TIMESTAMP\n    return (delta.days * 86400 + delta.seconds) * 1_000_000 + delta.microseconds\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.datetime_to_millis","title":"<code>datetime_to_millis(dt)</code>","text":"<p>Convert a datetime to milliseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def datetime_to_millis(dt: datetime) -&gt; int:\n    \"\"\"Convert a datetime to milliseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if dt.tzinfo:\n        delta = dt - EPOCH_TIMESTAMPTZ\n    else:\n        delta = dt - EPOCH_TIMESTAMP\n    return (delta.days * 86400 + delta.seconds) * 1_000 + delta.microseconds // 1_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.days_to_date","title":"<code>days_to_date(days)</code>","text":"<p>Create a date from the number of days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def days_to_date(days: int) -&gt; date:\n    \"\"\"Create a date from the number of days from 1970-01-01.\"\"\"\n    return EPOCH_DATE + timedelta(days)\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_days","title":"<code>micros_to_days(timestamp)</code>","text":"<p>Convert a timestamp in microseconds to a date in days.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_days(timestamp: int) -&gt; int:\n    \"\"\"Convert a timestamp in microseconds to a date in days.\"\"\"\n    return timedelta(microseconds=timestamp).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_hours","title":"<code>micros_to_hours(micros)</code>","text":"<p>Convert a timestamp in microseconds to hours from 1970-01-01T00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_hours(micros: int) -&gt; int:\n    \"\"\"Convert a timestamp in microseconds to hours from 1970-01-01T00:00.\"\"\"\n    return micros // 3_600_000_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_time","title":"<code>micros_to_time(micros)</code>","text":"<p>Convert a timestamp in microseconds to a time.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_time(micros: int) -&gt; time:\n    \"\"\"Convert a timestamp in microseconds to a time.\"\"\"\n    micros, microseconds = divmod(micros, 1000000)\n    micros, seconds = divmod(micros, 60)\n    micros, minutes = divmod(micros, 60)\n    hours = micros\n    return time(hour=hours, minute=minutes, second=seconds, microsecond=microseconds)\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_timestamp","title":"<code>micros_to_timestamp(micros)</code>","text":"<p>Convert microseconds from epoch to a timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_timestamp(micros: int) -&gt; datetime:\n    \"\"\"Convert microseconds from epoch to a timestamp.\"\"\"\n    dt = timedelta(microseconds=micros)\n    return EPOCH_TIMESTAMP + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_timestamptz","title":"<code>micros_to_timestamptz(micros)</code>","text":"<p>Convert microseconds from epoch to an utc timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_timestamptz(micros: int) -&gt; datetime:\n    \"\"\"Convert microseconds from epoch to an utc timestamp.\"\"\"\n    dt = timedelta(microseconds=micros)\n    return EPOCH_TIMESTAMPTZ + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.millis_to_datetime","title":"<code>millis_to_datetime(millis)</code>","text":"<p>Convert milliseconds from epoch to a timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def millis_to_datetime(millis: int) -&gt; datetime:\n    \"\"\"Convert milliseconds from epoch to a timestamp.\"\"\"\n    dt = timedelta(milliseconds=millis)\n    return EPOCH_TIMESTAMP + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_str_to_micros","title":"<code>time_str_to_micros(time_str)</code>","text":"<p>Convert an ISO-8601 formatted time to microseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_str_to_micros(time_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted time to microseconds from midnight.\"\"\"\n    return time_to_micros(time.fromisoformat(time_str))\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_to_micros","title":"<code>time_to_micros(t)</code>","text":"<p>Convert a datetime.time object to microseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_to_micros(t: time) -&gt; int:\n    \"\"\"Convert a datetime.time object to microseconds from midnight.\"\"\"\n    return (((t.hour * 60 + t.minute) * 60) + t.second) * 1_000_000 + t.microsecond\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamp_to_micros","title":"<code>timestamp_to_micros(timestamp_str)</code>","text":"<p>Convert an ISO-9601 formatted timestamp without zone to microseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamp_to_micros(timestamp_str: str) -&gt; int:\n    \"\"\"Convert an ISO-9601 formatted timestamp without zone to microseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if ISO_TIMESTAMP.fullmatch(timestamp_str):\n        return datetime_to_micros(datetime.fromisoformat(timestamp_str))\n    if ISO_TIMESTAMPTZ.fullmatch(timestamp_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Zone offset provided, but not expected: {timestamp_str}\")\n    raise ValueError(f\"Invalid timestamp without zone: {timestamp_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamptz_to_micros","title":"<code>timestamptz_to_micros(timestamptz_str)</code>","text":"<p>Convert an ISO-8601 formatted timestamp with zone to microseconds from 1970-01-01T00:00:00.000000+00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamptz_to_micros(timestamptz_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted timestamp with zone to microseconds from 1970-01-01T00:00:00.000000+00:00.\"\"\"\n    if ISO_TIMESTAMPTZ.fullmatch(timestamptz_str):\n        return datetime_to_micros(datetime.fromisoformat(timestamptz_str))\n    if ISO_TIMESTAMP.fullmatch(timestamptz_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Missing zone offset: {timestamptz_str} (must be ISO-8601)\")\n    raise ValueError(f\"Invalid timestamp with zone: {timestamptz_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_day","title":"<code>to_human_day(day_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_day(day_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return (EPOCH_DATE + timedelta(days=day_ordinal)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_hour","title":"<code>to_human_hour(hour_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_hour(hour_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMP + timedelta(hours=hour_ordinal)).isoformat(\"-\", \"hours\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_month","title":"<code>to_human_month(month_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_month(month_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return f\"{EPOCH_TIMESTAMP.year + month_ordinal // 12:0=4d}-{1 + month_ordinal % 12:0=2d}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_time","title":"<code>to_human_time(micros_from_midnight)</code>","text":"<p>Convert a TimeType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_time(micros_from_midnight: int) -&gt; str:\n    \"\"\"Convert a TimeType value to human string.\"\"\"\n    return micros_to_time(micros_from_midnight).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_timestamp","title":"<code>to_human_timestamp(timestamp_micros)</code>","text":"<p>Convert a TimestampType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_timestamp(timestamp_micros: int) -&gt; str:\n    \"\"\"Convert a TimestampType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMP + timedelta(microseconds=timestamp_micros)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_timestamptz","title":"<code>to_human_timestamptz(timestamp_micros)</code>","text":"<p>Convert a TimestamptzType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_timestamptz(timestamp_micros: int) -&gt; str:\n    \"\"\"Convert a TimestamptzType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMPTZ + timedelta(microseconds=timestamp_micros)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_year","title":"<code>to_human_year(year_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_year(year_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return f\"{EPOCH_TIMESTAMP.year + year_ordinal:0=4d}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/","title":"decimal","text":"<p>Helper methods for working with Python Decimals.</p>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.bytes_required","title":"<code>bytes_required(value)</code>","text":"<p>Return the minimum number of bytes needed to serialize a decimal or unscaled value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int | Decimal</code> <p>a Decimal value or unscaled int value.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the minimum number of bytes needed to serialize the value.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def bytes_required(value: Union[int, Decimal]) -&gt; int:\n    \"\"\"Return the minimum number of bytes needed to serialize a decimal or unscaled value.\n\n    Args:\n        value (int | Decimal): a Decimal value or unscaled int value.\n\n    Returns:\n        int: the minimum number of bytes needed to serialize the value.\n    \"\"\"\n    if isinstance(value, int):\n        return (value.bit_length() + 8) // 8\n    elif isinstance(value, Decimal):\n        return (decimal_to_unscaled(value).bit_length() + 8) // 8\n\n    raise ValueError(f\"Unsupported value: {value}\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.bytes_to_decimal","title":"<code>bytes_to_decimal(value, scale)</code>","text":"<p>Return a decimal from the bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>the bytes to be converted into a decimal.</p> required <code>scale</code> <code>int</code> <p>the scale of the decimal.</p> required <p>Returns:</p> Name Type Description <code>Decimal</code> <code>Decimal</code> <p>the scaled decimal.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def bytes_to_decimal(value: bytes, scale: int) -&gt; Decimal:\n    \"\"\"Return a decimal from the bytes.\n\n    Args:\n        value (bytes): the bytes to be converted into a decimal.\n        scale (int): the scale of the decimal.\n\n    Returns:\n        Decimal: the scaled decimal.\n    \"\"\"\n    unscaled_datum = int.from_bytes(value, byteorder=\"big\", signed=True)\n    return unscaled_to_decimal(unscaled_datum, scale)\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_required_bytes","title":"<code>decimal_required_bytes(precision)</code>","text":"<p>Compute the number of bytes required to store a precision.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>int</code> <p>The number of digits to store.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes required to store a decimal with a certain precision.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_required_bytes(precision: int) -&gt; int:\n    \"\"\"Compute the number of bytes required to store a precision.\n\n    Args:\n        precision: The number of digits to store.\n\n    Returns:\n        The number of bytes required to store a decimal with a certain precision.\n    \"\"\"\n    if precision &lt;= 0 or precision &gt;= 40:\n        raise ValueError(f\"Unsupported precision, outside of (0, 40]: {precision}\")\n\n    return REQUIRED_LENGTH[precision]\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_to_bytes","title":"<code>decimal_to_bytes(value, byte_length=None)</code>","text":"<p>Return a byte representation of a decimal.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>a decimal value.</p> required <code>byte_length</code> <code>int</code> <p>The number of bytes.</p> <code>None</code> <p>Returns:     bytes: the unscaled value of the Decimal as bytes.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_to_bytes(value: Decimal, byte_length: Optional[int] = None) -&gt; bytes:\n    \"\"\"Return a byte representation of a decimal.\n\n    Args:\n        value (Decimal): a decimal value.\n        byte_length (int): The number of bytes.\n    Returns:\n        bytes: the unscaled value of the Decimal as bytes.\n    \"\"\"\n    unscaled_value = decimal_to_unscaled(value)\n    if byte_length is None:\n        byte_length = bytes_required(unscaled_value)\n    return unscaled_value.to_bytes(byte_length, byteorder=\"big\", signed=True)\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_to_unscaled","title":"<code>decimal_to_unscaled(value)</code>","text":"<p>Get an unscaled value given a Decimal value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>A Decimal instance.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The unscaled value.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_to_unscaled(value: Decimal) -&gt; int:\n    \"\"\"Get an unscaled value given a Decimal value.\n\n    Args:\n        value (Decimal): A Decimal instance.\n\n    Returns:\n        int: The unscaled value.\n    \"\"\"\n    sign, digits, _ = value.as_tuple()\n    return int(Decimal((sign, digits, 0)).to_integral_value())\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.truncate_decimal","title":"<code>truncate_decimal(value, width)</code>","text":"<p>Get a truncated Decimal value given a decimal value and a width.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>a decimal value.</p> required <code>width</code> <code>int</code> <p>A width for the returned Decimal instance.</p> required <p>Returns:     Decimal: A truncated Decimal instance.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def truncate_decimal(value: Decimal, width: int) -&gt; Decimal:\n    \"\"\"Get a truncated Decimal value given a decimal value and a width.\n\n    Args:\n        value (Decimal): a decimal value.\n        width (int): A width for the returned Decimal instance.\n    Returns:\n        Decimal: A truncated Decimal instance.\n    \"\"\"\n    unscaled_value = decimal_to_unscaled(value)\n    applied_value = unscaled_value - (((unscaled_value % width) + width) % width)\n    return unscaled_to_decimal(applied_value, abs(int(value.as_tuple().exponent)))\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.unscaled_to_decimal","title":"<code>unscaled_to_decimal(unscaled, scale)</code>","text":"<p>Get a scaled Decimal value given an unscaled value and a scale.</p> <p>Parameters:</p> Name Type Description Default <code>unscaled</code> <code>int</code> <p>An unscaled value.</p> required <code>scale</code> <code>int</code> <p>A scale to set for the returned Decimal instance.</p> required <p>Returns:</p> Name Type Description <code>Decimal</code> <code>Decimal</code> <p>A scaled Decimal instance.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def unscaled_to_decimal(unscaled: int, scale: int) -&gt; Decimal:\n    \"\"\"Get a scaled Decimal value given an unscaled value and a scale.\n\n    Args:\n        unscaled (int): An unscaled value.\n        scale (int): A scale to set for the returned Decimal instance.\n\n    Returns:\n        Decimal: A scaled Decimal instance.\n    \"\"\"\n    sign, digits, _ = Decimal(unscaled).as_tuple()\n    return Decimal((sign, digits, -scale))\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/","title":"deprecated","text":""},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecated","title":"<code>deprecated(deprecated_in, removed_in, help_message=None)</code>","text":"<p>Mark functions as deprecated.</p> <p>Adding this will result in a warning being emitted when the function is used.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecated(deprecated_in: str, removed_in: str, help_message: Optional[str] = None) -&gt; Callable:  # type: ignore\n    \"\"\"Mark functions as deprecated.\n\n    Adding this will result in a warning being emitted when the function is used.\n    \"\"\"\n    if help_message is not None:\n        help_message = f\" {help_message}.\"\n\n    def decorator(func: Callable):  # type: ignore\n        @functools.wraps(func)\n        def new_func(*args: Any, **kwargs: Any) -&gt; Any:\n            message = f\"Call to {func.__name__}, deprecated in {deprecated_in}, will be removed in {removed_in}.{help_message}\"\n\n            _deprecation_warning(message)\n\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return decorator\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecation_message","title":"<code>deprecation_message(deprecated_in, removed_in, help_message)</code>","text":"<p>Mark properties or behaviors as deprecated.</p> <p>Adding this will result in a warning being emitted.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecation_message(deprecated_in: str, removed_in: str, help_message: Optional[str]) -&gt; None:\n    \"\"\"Mark properties or behaviors as deprecated.\n\n    Adding this will result in a warning being emitted.\n    \"\"\"\n    _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecation_notice","title":"<code>deprecation_notice(deprecated_in, removed_in, help_message)</code>","text":"<p>Return a deprecation notice.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecation_notice(deprecated_in: str, removed_in: str, help_message: Optional[str]) -&gt; str:\n    \"\"\"Return a deprecation notice.\"\"\"\n    return f\"Deprecated in {deprecated_in}, will be removed in {removed_in}. {help_message}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/","title":"lazydict","text":""},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict","title":"<code>LazyDict</code>","text":"<p>               Bases: <code>Mapping[K, V]</code></p> <p>Lazily build a dictionary from an array of items.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>class LazyDict(Mapping[K, V]):\n    \"\"\"Lazily build a dictionary from an array of items.\"\"\"\n\n    __slots__ = (\"_contents\", \"_dict\")\n\n    # Since Python's type system is not powerful enough to express the type of the\n    # contents of the dictionary, we use specify the type as a sequence of either K or V\n    # values.\n    #\n    # Rather than spending the runtime cost of checking the type of each item, we presume\n    # that the developer has correctly used the class and that the contents are valid.\n    def __init__(self, contents: Sequence[Sequence[Union[K, V]]]):\n        self._contents = contents\n        self._dict: Optional[Dict[K, V]] = None\n\n    def _build_dict(self) -&gt; Dict[K, V]:\n        self._dict = {}\n        for item in self._contents:\n            self._dict.update(dict(zip(cast(Sequence[K], item[::2]), cast(Sequence[V], item[1::2]))))\n\n        return self._dict\n\n    def __getitem__(self, key: K, /) -&gt; V:\n        \"\"\"Return the value for the given key.\"\"\"\n        source = self._dict or self._build_dict()\n        return source[key]\n\n    def __iter__(self) -&gt; Iterator[K]:\n        \"\"\"Return an iterator over the keys of the dictionary.\"\"\"\n        source = self._dict or self._build_dict()\n        return iter(source)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of items in the dictionary.\"\"\"\n        source = self._dict or self._build_dict()\n        return len(source)\n\n    def __dict__(self) -&gt; Dict[K, V]:  # type: ignore\n        \"\"\"Convert the lazy dict in a dict.\"\"\"\n        return self._dict or self._build_dict()\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__dict__","title":"<code>__dict__()</code>","text":"<p>Convert the lazy dict in a dict.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __dict__(self) -&gt; Dict[K, V]:  # type: ignore\n    \"\"\"Convert the lazy dict in a dict.\"\"\"\n    return self._dict or self._build_dict()\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Return the value for the given key.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __getitem__(self, key: K, /) -&gt; V:\n    \"\"\"Return the value for the given key.\"\"\"\n    source = self._dict or self._build_dict()\n    return source[key]\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator over the keys of the dictionary.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __iter__(self) -&gt; Iterator[K]:\n    \"\"\"Return an iterator over the keys of the dictionary.\"\"\"\n    source = self._dict or self._build_dict()\n    return iter(source)\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of items in the dictionary.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of items in the dictionary.\"\"\"\n    source = self._dict or self._build_dict()\n    return len(source)\n</code></pre>"},{"location":"reference/pyiceberg/utils/parsing/","title":"parsing","text":""},{"location":"reference/pyiceberg/utils/parsing/#pyiceberg.utils.parsing.ParseNumberFromBrackets","title":"<code>ParseNumberFromBrackets</code>","text":"<p>Extracts the size from a string in the form of prefix[22].</p> Source code in <code>pyiceberg/utils/parsing.py</code> <pre><code>class ParseNumberFromBrackets:\n    \"\"\"Extracts the size from a string in the form of prefix[22].\"\"\"\n\n    regex: Pattern  # type: ignore\n    prefix: str\n\n    def __init__(self, prefix: str):\n        self.prefix = prefix\n        self.regex = re.compile(rf\"{prefix}\\[(\\d+)\\]\")\n\n    def match(self, str_repr: str) -&gt; int:\n        matches = self.regex.search(str_repr)\n        if matches:\n            return int(matches.group(1))\n        raise ValidationError(f\"Could not match {str_repr}, expected format {self.prefix}[22]\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/properties/","title":"properties","text":""},{"location":"reference/pyiceberg/utils/schema_conversion/","title":"schema_conversion","text":"<p>Utility class for converting between Avro and Iceberg schemas.</p>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion","title":"<code>AvroSchemaConversion</code>","text":"Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>class AvroSchemaConversion:\n    def avro_to_iceberg(self, avro_schema: Dict[str, Any]) -&gt; Schema:\n        \"\"\"Convert an Apache Avro into an Apache Iceberg schema equivalent.\n\n        This expects to have field id's to be encoded in the Avro schema:\n\n            {\n                \"type\": \"record\",\n                \"name\": \"manifest_file\",\n                \"fields\": [\n                    {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n                    {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n                ]\n            }\n\n        Example:\n            This converts an Avro schema into an Iceberg schema:\n\n            &gt;&gt;&gt; avro_schema = AvroSchemaConversion().avro_to_iceberg({\n            ...     \"type\": \"record\",\n            ...     \"name\": \"manifest_file\",\n            ...     \"fields\": [\n            ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n            ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n            ...     ]\n            ... })\n            &gt;&gt;&gt; iceberg_schema = Schema(\n            ...     NestedField(\n            ...         field_id=500, name=\"manifest_path\", field_type=StringType(), required=False, doc=\"Location URI with FS scheme\"\n            ...     ),\n            ...     NestedField(\n            ...         field_id=501, name=\"manifest_length\", field_type=LongType(), required=False, doc=\"Total file size in bytes\"\n            ...     ),\n            ...     schema_id=1\n            ... )\n            &gt;&gt;&gt; avro_schema == iceberg_schema\n            True\n\n        Args:\n            avro_schema (Dict[str, Any]): The JSON decoded Avro schema.\n\n        Returns:\n            Equivalent Iceberg schema.\n        \"\"\"\n        return Schema(*[self._convert_field(field) for field in avro_schema[\"fields\"]], schema_id=1)\n\n    def iceberg_to_avro(self, schema: Schema, schema_name: Optional[str] = None) -&gt; AvroType:\n        \"\"\"Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.\"\"\"\n        return visit(schema, ConvertSchemaToAvro(schema_name))\n\n    def _resolve_union(\n        self, type_union: Union[Dict[str, str], List[Union[str, Dict[str, str]]], str]\n    ) -&gt; Tuple[Union[str, Dict[str, Any]], bool]:\n        \"\"\"\n        Convert Unions into their type and resolves if the field is required.\n\n        Examples:\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union('str')\n            ('str', True)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', 'str'])\n            ('str', False)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union([{'type': 'str'}])\n            ({'type': 'str'}, True)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', {'type': 'str'}])\n            ({'type': 'str'}, False)\n\n        Args:\n            type_union: The field, can be a string 'str', list ['null', 'str'], or dict {\"type\": 'str'}.\n\n        Returns:\n            A tuple containing the type and if required.\n\n        Raises:\n            TypeError: In the case non-optional union types are encountered.\n        \"\"\"\n        avro_types: Union[Dict[str, str], List[Union[Dict[str, str], str]]]\n        if isinstance(type_union, str):\n            # It is a primitive and required\n            return type_union, True\n        elif isinstance(type_union, dict):\n            # It is a context and required\n            return type_union, True\n        else:\n            avro_types = type_union\n\n        if len(avro_types) &gt; 2:\n            raise TypeError(f\"Non-optional types aren't part of the Iceberg specification: {avro_types}\")\n\n        # For the Iceberg spec it is required to set the default value to null\n        # From https://iceberg.apache.org/spec/#avro\n        # Optional fields must always set the Avro field default value to null.\n        #\n        # This means that null has to come first:\n        # https://avro.apache.org/docs/current/spec.html\n        # type of the default value must match the first element of the union.\n        if \"null\" != avro_types[0]:\n            raise TypeError(\"Only null-unions are supported\")\n\n        # Filter the null value and return the type\n        return list(filter(lambda t: t != \"null\", avro_types))[0], False\n\n    def _convert_schema(self, avro_type: Union[str, Dict[str, Any]]) -&gt; IcebergType:\n        \"\"\"\n        Resolve the Avro type.\n\n        Args:\n            avro_type: The Avro type, can be simple or complex.\n\n        Returns:\n            The equivalent IcebergType.\n\n        Raises:\n            ValueError: When there are unknown types\n        \"\"\"\n        if isinstance(avro_type, str) and avro_type in PRIMITIVE_FIELD_TYPE_MAPPING:\n            return PRIMITIVE_FIELD_TYPE_MAPPING[avro_type]\n        elif isinstance(avro_type, dict):\n            if \"logicalType\" in avro_type:\n                return self._convert_logical_type(avro_type)\n            else:\n                # Resolve potential nested types\n                while \"type\" in avro_type and isinstance(avro_type[\"type\"], dict):\n                    avro_type = avro_type[\"type\"]\n                type_identifier = avro_type[\"type\"]\n                if type_identifier == \"record\":\n                    return self._convert_record_type(avro_type)\n                elif type_identifier == \"array\":\n                    return self._convert_array_type(avro_type)\n                elif type_identifier == \"map\":\n                    return self._convert_map_type(avro_type)\n                elif type_identifier == \"fixed\":\n                    return self._convert_fixed_type(avro_type)\n                elif isinstance(type_identifier, str) and type_identifier in PRIMITIVE_FIELD_TYPE_MAPPING:\n                    return PRIMITIVE_FIELD_TYPE_MAPPING[type_identifier]\n                else:\n                    raise TypeError(f\"Unknown type: {avro_type}\")\n        else:\n            raise TypeError(f\"Unknown type: {avro_type}\")\n\n    def _convert_field(self, field: Dict[str, Any]) -&gt; NestedField:\n        \"\"\"Convert an Avro field into an Iceberg equivalent field.\n\n        Args:\n            field: The Avro field.\n\n        Returns:\n            The Iceberg equivalent field.\n        \"\"\"\n        if \"field-id\" not in field:\n            raise ValueError(f\"Cannot convert field, missing field-id: {field}\")\n\n        plain_type, required = self._resolve_union(field[\"type\"])\n\n        return NestedField(\n            field_id=field[\"field-id\"],\n            name=field[\"name\"],\n            field_type=self._convert_schema(plain_type),\n            required=required,\n            doc=field.get(\"doc\"),\n        )\n\n    def _convert_record_type(self, record_type: Dict[str, Any]) -&gt; StructType:\n        \"\"\"\n        Convert the fields from a record into an Iceberg struct.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; record_type = {\n            ...     \"type\": \"record\",\n            ...     \"name\": \"r508\",\n            ...     \"fields\": [{\n            ...         \"name\": \"contains_null\",\n            ...         \"type\": \"boolean\",\n            ...         \"doc\": \"True if any file has a null partition value\",\n            ...         \"field-id\": 509,\n            ...      }, {\n            ...          \"name\": \"contains_nan\",\n            ...          \"type\": [\"null\", \"boolean\"],\n            ...          \"doc\": \"True if any file has a nan partition value\",\n            ...          \"default\": None,\n            ...          \"field-id\": 518,\n            ...      }],\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_record_type(record_type)\n            &gt;&gt;&gt; expected = StructType(\n            ...     fields=(\n            ...         NestedField(\n            ...             field_id=509,\n            ...             name=\"contains_null\",\n            ...             field_type=BooleanType(),\n            ...             required=False,\n            ...             doc=\"True if any file has a null partition value\",\n            ...         ),\n            ...         NestedField(\n            ...             field_id=518,\n            ...             name=\"contains_nan\",\n            ...             field_type=BooleanType(),\n            ...             required=True,\n            ...             doc=\"True if any file has a nan partition value\",\n            ...         ),\n            ...     )\n            ... )\n            &gt;&gt;&gt; expected == actual\n            True\n\n        Args:\n            record_type: The record type itself.\n\n        Returns: A StructType.\n        \"\"\"\n        if record_type[\"type\"] != \"record\":\n            raise ValueError(f\"Expected record type, got: {record_type}\")\n\n        return StructType(*[self._convert_field(field) for field in record_type[\"fields\"]])\n\n    def _convert_array_type(self, array_type: Dict[str, Any]) -&gt; ListType:\n        if \"element-id\" not in array_type:\n            raise ValueError(f\"Cannot convert array-type, missing element-id: {array_type}\")\n\n        plain_type, element_required = self._resolve_union(array_type[\"items\"])\n\n        return ListType(\n            element_id=array_type[\"element-id\"],\n            element_type=self._convert_schema(plain_type),\n            element_required=element_required,\n        )\n\n    def _convert_map_type(self, map_type: Dict[str, Any]) -&gt; MapType:\n        \"\"\"Convert an avro map type into an Iceberg MapType.\n\n        Args:\n            map_type: The dict that describes the Avro map type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_field = {\n            ...     \"type\": \"map\",\n            ...     \"values\": [\"null\", \"long\"],\n            ...     \"key-id\": 101,\n            ...     \"value-id\": 102,\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_map_type(avro_field)\n            &gt;&gt;&gt; expected = MapType(\n            ...     key_id=101,\n            ...     key_type=StringType(),\n            ...     value_id=102,\n            ...     value_type=LongType(),\n            ...     value_required=True\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        Returns: A MapType.\n        \"\"\"\n        value_type, value_required = self._resolve_union(map_type[\"values\"])\n        return MapType(\n            key_id=map_type[\"key-id\"],\n            # Avro only supports string keys\n            key_type=StringType(),\n            value_id=map_type[\"value-id\"],\n            value_type=self._convert_schema(value_type),\n            value_required=value_required,\n        )\n\n    def _convert_logical_type(self, avro_logical_type: Dict[str, Any]) -&gt; IcebergType:\n        \"\"\"Convert a schema with a logical type annotation into an IcebergType.\n\n        For the decimal and map we need to fetch more keys from the dict, and for\n        the simple ones we can just look it up in the mapping.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_logical_type = {\n            ...     \"type\": \"int\",\n            ...     \"logicalType\": \"date\"\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_type(avro_logical_type)\n            &gt;&gt;&gt; actual == DateType()\n            True\n\n        Args:\n            avro_logical_type: The logical type.\n\n        Returns:\n            The converted logical type.\n\n        Raises:\n            ValueError: When the logical type is unknown.\n        \"\"\"\n        logical_type = avro_logical_type[\"logicalType\"]\n        physical_type = avro_logical_type[\"type\"]\n        if logical_type == \"decimal\":\n            return self._convert_logical_decimal_type(avro_logical_type)\n        elif logical_type == \"map\":\n            return self._convert_logical_map_type(avro_logical_type)\n        elif logical_type == \"timestamp-micros\":\n            if avro_logical_type.get(\"adjust-to-utc\", False) is True:\n                return TimestamptzType()\n            else:\n                return TimestampType()\n        elif (logical_type, physical_type) in LOGICAL_FIELD_TYPE_MAPPING:\n            return LOGICAL_FIELD_TYPE_MAPPING[(logical_type, physical_type)]\n        else:\n            raise ValueError(f\"Unknown logical/physical type combination: {avro_logical_type}\")\n\n    def _convert_logical_decimal_type(self, avro_type: Dict[str, Any]) -&gt; DecimalType:\n        \"\"\"Convert an avro type to an Iceberg DecimalType.\n\n        Args:\n            avro_type: The Avro type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_decimal_type = {\n            ...     \"type\": \"bytes\",\n            ...     \"logicalType\": \"decimal\",\n            ...     \"precision\": 19,\n            ...     \"scale\": 25\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_decimal_type(avro_decimal_type)\n            &gt;&gt;&gt; expected = DecimalType(\n            ...     precision=19,\n            ...     scale=25\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        Returns:\n            A Iceberg DecimalType.\n        \"\"\"\n        return DecimalType(precision=avro_type[\"precision\"], scale=avro_type[\"scale\"])\n\n    def _convert_logical_map_type(self, avro_type: Dict[str, Any]) -&gt; MapType:\n        \"\"\"Convert an avro map type to an Iceberg MapType.\n\n        In the case where a map hasn't a key as a type you can use a logical map to still encode this in Avro.\n\n        Args:\n            avro_type: The Avro Type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_type = {\n            ...     \"type\": \"array\",\n            ...     \"logicalType\": \"map\",\n            ...     \"items\": {\n            ...         \"type\": \"record\",\n            ...         \"name\": \"k101_v102\",\n            ...         \"fields\": [\n            ...             {\"name\": \"key\", \"type\": \"int\", \"field-id\": 101},\n            ...             {\"name\": \"value\", \"type\": \"string\", \"field-id\": 102},\n            ...         ],\n            ...     },\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_map_type(avro_type)\n            &gt;&gt;&gt; expected = MapType(\n            ...         key_id=101,\n            ...         key_type=IntegerType(),\n            ...         value_id=102,\n            ...         value_type=StringType(),\n            ...         value_required=False\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        .. _Apache Iceberg specification:\n            https://iceberg.apache.org/spec/#appendix-a-format-specific-requirements\n\n        Returns:\n            The logical map.\n        \"\"\"\n        fields = avro_type[\"items\"][\"fields\"]\n        if len(fields) != 2:\n            raise ValueError(f\"Invalid key-value pair schema: {avro_type['items']}\")\n        key = self._convert_field(list(filter(lambda f: f[\"name\"] == \"key\", fields))[0])\n        value = self._convert_field(list(filter(lambda f: f[\"name\"] == \"value\", fields))[0])\n        return MapType(\n            key_id=key.field_id,\n            key_type=key.field_type,\n            value_id=value.field_id,\n            value_type=value.field_type,\n            value_required=value.required,\n        )\n\n    def _convert_fixed_type(self, avro_type: Dict[str, Any]) -&gt; FixedType:\n        \"\"\"\n        Convert Avro Type to the equivalent Iceberg fixed type.\n\n        - https://avro.apache.org/docs/current/spec.html#Fixed\n\n        Args:\n            avro_type: The Avro type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_fixed_type = {\n            ...     \"name\": \"md5\",\n            ...     \"type\": \"fixed\",\n            ...     \"size\": 16\n            ... }\n            &gt;&gt;&gt; FixedType(length=16) == AvroSchemaConversion()._convert_fixed_type(avro_fixed_type)\n            True\n\n        Returns:\n            An Iceberg equivalent fixed type.\n        \"\"\"\n        return FixedType(length=avro_type[\"size\"])\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_field","title":"<code>_convert_field(field)</code>","text":"<p>Convert an Avro field into an Iceberg equivalent field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Dict[str, Any]</code> <p>The Avro field.</p> required <p>Returns:</p> Type Description <code>NestedField</code> <p>The Iceberg equivalent field.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_field(self, field: Dict[str, Any]) -&gt; NestedField:\n    \"\"\"Convert an Avro field into an Iceberg equivalent field.\n\n    Args:\n        field: The Avro field.\n\n    Returns:\n        The Iceberg equivalent field.\n    \"\"\"\n    if \"field-id\" not in field:\n        raise ValueError(f\"Cannot convert field, missing field-id: {field}\")\n\n    plain_type, required = self._resolve_union(field[\"type\"])\n\n    return NestedField(\n        field_id=field[\"field-id\"],\n        name=field[\"name\"],\n        field_type=self._convert_schema(plain_type),\n        required=required,\n        doc=field.get(\"doc\"),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_fixed_type","title":"<code>_convert_fixed_type(avro_type)</code>","text":"<p>Convert Avro Type to the equivalent Iceberg fixed type.</p> <ul> <li>https://avro.apache.org/docs/current/spec.html#Fixed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>avro_type</code> <code>Dict[str, Any]</code> <p>The Avro type.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; avro_fixed_type = {\n...     \"name\": \"md5\",\n...     \"type\": \"fixed\",\n...     \"size\": 16\n... }\n&gt;&gt;&gt; FixedType(length=16) == AvroSchemaConversion()._convert_fixed_type(avro_fixed_type)\nTrue\n</code></pre> <p>Returns:</p> Type Description <code>FixedType</code> <p>An Iceberg equivalent fixed type.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_fixed_type(self, avro_type: Dict[str, Any]) -&gt; FixedType:\n    \"\"\"\n    Convert Avro Type to the equivalent Iceberg fixed type.\n\n    - https://avro.apache.org/docs/current/spec.html#Fixed\n\n    Args:\n        avro_type: The Avro type.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; avro_fixed_type = {\n        ...     \"name\": \"md5\",\n        ...     \"type\": \"fixed\",\n        ...     \"size\": 16\n        ... }\n        &gt;&gt;&gt; FixedType(length=16) == AvroSchemaConversion()._convert_fixed_type(avro_fixed_type)\n        True\n\n    Returns:\n        An Iceberg equivalent fixed type.\n    \"\"\"\n    return FixedType(length=avro_type[\"size\"])\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_logical_decimal_type","title":"<code>_convert_logical_decimal_type(avro_type)</code>","text":"<p>Convert an avro type to an Iceberg DecimalType.</p> <p>Parameters:</p> Name Type Description Default <code>avro_type</code> <code>Dict[str, Any]</code> <p>The Avro type.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; avro_decimal_type = {\n...     \"type\": \"bytes\",\n...     \"logicalType\": \"decimal\",\n...     \"precision\": 19,\n...     \"scale\": 25\n... }\n&gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_decimal_type(avro_decimal_type)\n&gt;&gt;&gt; expected = DecimalType(\n...     precision=19,\n...     scale=25\n... )\n&gt;&gt;&gt; actual == expected\nTrue\n</code></pre> <p>Returns:</p> Type Description <code>DecimalType</code> <p>A Iceberg DecimalType.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_logical_decimal_type(self, avro_type: Dict[str, Any]) -&gt; DecimalType:\n    \"\"\"Convert an avro type to an Iceberg DecimalType.\n\n    Args:\n        avro_type: The Avro type.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; avro_decimal_type = {\n        ...     \"type\": \"bytes\",\n        ...     \"logicalType\": \"decimal\",\n        ...     \"precision\": 19,\n        ...     \"scale\": 25\n        ... }\n        &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_decimal_type(avro_decimal_type)\n        &gt;&gt;&gt; expected = DecimalType(\n        ...     precision=19,\n        ...     scale=25\n        ... )\n        &gt;&gt;&gt; actual == expected\n        True\n\n    Returns:\n        A Iceberg DecimalType.\n    \"\"\"\n    return DecimalType(precision=avro_type[\"precision\"], scale=avro_type[\"scale\"])\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_logical_map_type","title":"<code>_convert_logical_map_type(avro_type)</code>","text":"<p>Convert an avro map type to an Iceberg MapType.</p> <p>In the case where a map hasn't a key as a type you can use a logical map to still encode this in Avro.</p> <p>Parameters:</p> Name Type Description Default <code>avro_type</code> <code>Dict[str, Any]</code> <p>The Avro Type.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; avro_type = {\n...     \"type\": \"array\",\n...     \"logicalType\": \"map\",\n...     \"items\": {\n...         \"type\": \"record\",\n...         \"name\": \"k101_v102\",\n...         \"fields\": [\n...             {\"name\": \"key\", \"type\": \"int\", \"field-id\": 101},\n...             {\"name\": \"value\", \"type\": \"string\", \"field-id\": 102},\n...         ],\n...     },\n... }\n&gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_map_type(avro_type)\n&gt;&gt;&gt; expected = MapType(\n...         key_id=101,\n...         key_type=IntegerType(),\n...         value_id=102,\n...         value_type=StringType(),\n...         value_required=False\n... )\n&gt;&gt;&gt; actual == expected\nTrue\n</code></pre> <p>.. _Apache Iceberg specification:     https://iceberg.apache.org/spec/#appendix-a-format-specific-requirements</p> <p>Returns:</p> Type Description <code>MapType</code> <p>The logical map.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_logical_map_type(self, avro_type: Dict[str, Any]) -&gt; MapType:\n    \"\"\"Convert an avro map type to an Iceberg MapType.\n\n    In the case where a map hasn't a key as a type you can use a logical map to still encode this in Avro.\n\n    Args:\n        avro_type: The Avro Type.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; avro_type = {\n        ...     \"type\": \"array\",\n        ...     \"logicalType\": \"map\",\n        ...     \"items\": {\n        ...         \"type\": \"record\",\n        ...         \"name\": \"k101_v102\",\n        ...         \"fields\": [\n        ...             {\"name\": \"key\", \"type\": \"int\", \"field-id\": 101},\n        ...             {\"name\": \"value\", \"type\": \"string\", \"field-id\": 102},\n        ...         ],\n        ...     },\n        ... }\n        &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_map_type(avro_type)\n        &gt;&gt;&gt; expected = MapType(\n        ...         key_id=101,\n        ...         key_type=IntegerType(),\n        ...         value_id=102,\n        ...         value_type=StringType(),\n        ...         value_required=False\n        ... )\n        &gt;&gt;&gt; actual == expected\n        True\n\n    .. _Apache Iceberg specification:\n        https://iceberg.apache.org/spec/#appendix-a-format-specific-requirements\n\n    Returns:\n        The logical map.\n    \"\"\"\n    fields = avro_type[\"items\"][\"fields\"]\n    if len(fields) != 2:\n        raise ValueError(f\"Invalid key-value pair schema: {avro_type['items']}\")\n    key = self._convert_field(list(filter(lambda f: f[\"name\"] == \"key\", fields))[0])\n    value = self._convert_field(list(filter(lambda f: f[\"name\"] == \"value\", fields))[0])\n    return MapType(\n        key_id=key.field_id,\n        key_type=key.field_type,\n        value_id=value.field_id,\n        value_type=value.field_type,\n        value_required=value.required,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_logical_type","title":"<code>_convert_logical_type(avro_logical_type)</code>","text":"<p>Convert a schema with a logical type annotation into an IcebergType.</p> <p>For the decimal and map we need to fetch more keys from the dict, and for the simple ones we can just look it up in the mapping.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; avro_logical_type = {\n...     \"type\": \"int\",\n...     \"logicalType\": \"date\"\n... }\n&gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_type(avro_logical_type)\n&gt;&gt;&gt; actual == DateType()\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>avro_logical_type</code> <code>Dict[str, Any]</code> <p>The logical type.</p> required <p>Returns:</p> Type Description <code>IcebergType</code> <p>The converted logical type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the logical type is unknown.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_logical_type(self, avro_logical_type: Dict[str, Any]) -&gt; IcebergType:\n    \"\"\"Convert a schema with a logical type annotation into an IcebergType.\n\n    For the decimal and map we need to fetch more keys from the dict, and for\n    the simple ones we can just look it up in the mapping.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; avro_logical_type = {\n        ...     \"type\": \"int\",\n        ...     \"logicalType\": \"date\"\n        ... }\n        &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_type(avro_logical_type)\n        &gt;&gt;&gt; actual == DateType()\n        True\n\n    Args:\n        avro_logical_type: The logical type.\n\n    Returns:\n        The converted logical type.\n\n    Raises:\n        ValueError: When the logical type is unknown.\n    \"\"\"\n    logical_type = avro_logical_type[\"logicalType\"]\n    physical_type = avro_logical_type[\"type\"]\n    if logical_type == \"decimal\":\n        return self._convert_logical_decimal_type(avro_logical_type)\n    elif logical_type == \"map\":\n        return self._convert_logical_map_type(avro_logical_type)\n    elif logical_type == \"timestamp-micros\":\n        if avro_logical_type.get(\"adjust-to-utc\", False) is True:\n            return TimestamptzType()\n        else:\n            return TimestampType()\n    elif (logical_type, physical_type) in LOGICAL_FIELD_TYPE_MAPPING:\n        return LOGICAL_FIELD_TYPE_MAPPING[(logical_type, physical_type)]\n    else:\n        raise ValueError(f\"Unknown logical/physical type combination: {avro_logical_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_map_type","title":"<code>_convert_map_type(map_type)</code>","text":"<p>Convert an avro map type into an Iceberg MapType.</p> <p>Parameters:</p> Name Type Description Default <code>map_type</code> <code>Dict[str, Any]</code> <p>The dict that describes the Avro map type.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; avro_field = {\n...     \"type\": \"map\",\n...     \"values\": [\"null\", \"long\"],\n...     \"key-id\": 101,\n...     \"value-id\": 102,\n... }\n&gt;&gt;&gt; actual = AvroSchemaConversion()._convert_map_type(avro_field)\n&gt;&gt;&gt; expected = MapType(\n...     key_id=101,\n...     key_type=StringType(),\n...     value_id=102,\n...     value_type=LongType(),\n...     value_required=True\n... )\n&gt;&gt;&gt; actual == expected\nTrue\n</code></pre> <p>Returns: A MapType.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_map_type(self, map_type: Dict[str, Any]) -&gt; MapType:\n    \"\"\"Convert an avro map type into an Iceberg MapType.\n\n    Args:\n        map_type: The dict that describes the Avro map type.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; avro_field = {\n        ...     \"type\": \"map\",\n        ...     \"values\": [\"null\", \"long\"],\n        ...     \"key-id\": 101,\n        ...     \"value-id\": 102,\n        ... }\n        &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_map_type(avro_field)\n        &gt;&gt;&gt; expected = MapType(\n        ...     key_id=101,\n        ...     key_type=StringType(),\n        ...     value_id=102,\n        ...     value_type=LongType(),\n        ...     value_required=True\n        ... )\n        &gt;&gt;&gt; actual == expected\n        True\n\n    Returns: A MapType.\n    \"\"\"\n    value_type, value_required = self._resolve_union(map_type[\"values\"])\n    return MapType(\n        key_id=map_type[\"key-id\"],\n        # Avro only supports string keys\n        key_type=StringType(),\n        value_id=map_type[\"value-id\"],\n        value_type=self._convert_schema(value_type),\n        value_required=value_required,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_record_type","title":"<code>_convert_record_type(record_type)</code>","text":"<p>Convert the fields from a record into an Iceberg struct.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n&gt;&gt;&gt; record_type = {\n...     \"type\": \"record\",\n...     \"name\": \"r508\",\n...     \"fields\": [{\n...         \"name\": \"contains_null\",\n...         \"type\": \"boolean\",\n...         \"doc\": \"True if any file has a null partition value\",\n...         \"field-id\": 509,\n...      }, {\n...          \"name\": \"contains_nan\",\n...          \"type\": [\"null\", \"boolean\"],\n...          \"doc\": \"True if any file has a nan partition value\",\n...          \"default\": None,\n...          \"field-id\": 518,\n...      }],\n... }\n&gt;&gt;&gt; actual = AvroSchemaConversion()._convert_record_type(record_type)\n&gt;&gt;&gt; expected = StructType(\n...     fields=(\n...         NestedField(\n...             field_id=509,\n...             name=\"contains_null\",\n...             field_type=BooleanType(),\n...             required=False,\n...             doc=\"True if any file has a null partition value\",\n...         ),\n...         NestedField(\n...             field_id=518,\n...             name=\"contains_nan\",\n...             field_type=BooleanType(),\n...             required=True,\n...             doc=\"True if any file has a nan partition value\",\n...         ),\n...     )\n... )\n&gt;&gt;&gt; expected == actual\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>record_type</code> <code>Dict[str, Any]</code> <p>The record type itself.</p> required <p>Returns: A StructType.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_record_type(self, record_type: Dict[str, Any]) -&gt; StructType:\n    \"\"\"\n    Convert the fields from a record into an Iceberg struct.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n        &gt;&gt;&gt; record_type = {\n        ...     \"type\": \"record\",\n        ...     \"name\": \"r508\",\n        ...     \"fields\": [{\n        ...         \"name\": \"contains_null\",\n        ...         \"type\": \"boolean\",\n        ...         \"doc\": \"True if any file has a null partition value\",\n        ...         \"field-id\": 509,\n        ...      }, {\n        ...          \"name\": \"contains_nan\",\n        ...          \"type\": [\"null\", \"boolean\"],\n        ...          \"doc\": \"True if any file has a nan partition value\",\n        ...          \"default\": None,\n        ...          \"field-id\": 518,\n        ...      }],\n        ... }\n        &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_record_type(record_type)\n        &gt;&gt;&gt; expected = StructType(\n        ...     fields=(\n        ...         NestedField(\n        ...             field_id=509,\n        ...             name=\"contains_null\",\n        ...             field_type=BooleanType(),\n        ...             required=False,\n        ...             doc=\"True if any file has a null partition value\",\n        ...         ),\n        ...         NestedField(\n        ...             field_id=518,\n        ...             name=\"contains_nan\",\n        ...             field_type=BooleanType(),\n        ...             required=True,\n        ...             doc=\"True if any file has a nan partition value\",\n        ...         ),\n        ...     )\n        ... )\n        &gt;&gt;&gt; expected == actual\n        True\n\n    Args:\n        record_type: The record type itself.\n\n    Returns: A StructType.\n    \"\"\"\n    if record_type[\"type\"] != \"record\":\n        raise ValueError(f\"Expected record type, got: {record_type}\")\n\n    return StructType(*[self._convert_field(field) for field in record_type[\"fields\"]])\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._convert_schema","title":"<code>_convert_schema(avro_type)</code>","text":"<p>Resolve the Avro type.</p> <p>Parameters:</p> Name Type Description Default <code>avro_type</code> <code>Union[str, Dict[str, Any]]</code> <p>The Avro type, can be simple or complex.</p> required <p>Returns:</p> Type Description <code>IcebergType</code> <p>The equivalent IcebergType.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When there are unknown types</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _convert_schema(self, avro_type: Union[str, Dict[str, Any]]) -&gt; IcebergType:\n    \"\"\"\n    Resolve the Avro type.\n\n    Args:\n        avro_type: The Avro type, can be simple or complex.\n\n    Returns:\n        The equivalent IcebergType.\n\n    Raises:\n        ValueError: When there are unknown types\n    \"\"\"\n    if isinstance(avro_type, str) and avro_type in PRIMITIVE_FIELD_TYPE_MAPPING:\n        return PRIMITIVE_FIELD_TYPE_MAPPING[avro_type]\n    elif isinstance(avro_type, dict):\n        if \"logicalType\" in avro_type:\n            return self._convert_logical_type(avro_type)\n        else:\n            # Resolve potential nested types\n            while \"type\" in avro_type and isinstance(avro_type[\"type\"], dict):\n                avro_type = avro_type[\"type\"]\n            type_identifier = avro_type[\"type\"]\n            if type_identifier == \"record\":\n                return self._convert_record_type(avro_type)\n            elif type_identifier == \"array\":\n                return self._convert_array_type(avro_type)\n            elif type_identifier == \"map\":\n                return self._convert_map_type(avro_type)\n            elif type_identifier == \"fixed\":\n                return self._convert_fixed_type(avro_type)\n            elif isinstance(type_identifier, str) and type_identifier in PRIMITIVE_FIELD_TYPE_MAPPING:\n                return PRIMITIVE_FIELD_TYPE_MAPPING[type_identifier]\n            else:\n                raise TypeError(f\"Unknown type: {avro_type}\")\n    else:\n        raise TypeError(f\"Unknown type: {avro_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion._resolve_union","title":"<code>_resolve_union(type_union)</code>","text":"<p>Convert Unions into their type and resolves if the field is required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; AvroSchemaConversion()._resolve_union('str')\n('str', True)\n&gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', 'str'])\n('str', False)\n&gt;&gt;&gt; AvroSchemaConversion()._resolve_union([{'type': 'str'}])\n({'type': 'str'}, True)\n&gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', {'type': 'str'}])\n({'type': 'str'}, False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>type_union</code> <code>Union[Dict[str, str], List[Union[str, Dict[str, str]]], str]</code> <p>The field, can be a string 'str', list ['null', 'str'], or dict {\"type\": 'str'}.</p> required <p>Returns:</p> Type Description <code>Tuple[Union[str, Dict[str, Any]], bool]</code> <p>A tuple containing the type and if required.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case non-optional union types are encountered.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def _resolve_union(\n    self, type_union: Union[Dict[str, str], List[Union[str, Dict[str, str]]], str]\n) -&gt; Tuple[Union[str, Dict[str, Any]], bool]:\n    \"\"\"\n    Convert Unions into their type and resolves if the field is required.\n\n    Examples:\n        &gt;&gt;&gt; AvroSchemaConversion()._resolve_union('str')\n        ('str', True)\n        &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', 'str'])\n        ('str', False)\n        &gt;&gt;&gt; AvroSchemaConversion()._resolve_union([{'type': 'str'}])\n        ({'type': 'str'}, True)\n        &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', {'type': 'str'}])\n        ({'type': 'str'}, False)\n\n    Args:\n        type_union: The field, can be a string 'str', list ['null', 'str'], or dict {\"type\": 'str'}.\n\n    Returns:\n        A tuple containing the type and if required.\n\n    Raises:\n        TypeError: In the case non-optional union types are encountered.\n    \"\"\"\n    avro_types: Union[Dict[str, str], List[Union[Dict[str, str], str]]]\n    if isinstance(type_union, str):\n        # It is a primitive and required\n        return type_union, True\n    elif isinstance(type_union, dict):\n        # It is a context and required\n        return type_union, True\n    else:\n        avro_types = type_union\n\n    if len(avro_types) &gt; 2:\n        raise TypeError(f\"Non-optional types aren't part of the Iceberg specification: {avro_types}\")\n\n    # For the Iceberg spec it is required to set the default value to null\n    # From https://iceberg.apache.org/spec/#avro\n    # Optional fields must always set the Avro field default value to null.\n    #\n    # This means that null has to come first:\n    # https://avro.apache.org/docs/current/spec.html\n    # type of the default value must match the first element of the union.\n    if \"null\" != avro_types[0]:\n        raise TypeError(\"Only null-unions are supported\")\n\n    # Filter the null value and return the type\n    return list(filter(lambda t: t != \"null\", avro_types))[0], False\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion.avro_to_iceberg","title":"<code>avro_to_iceberg(avro_schema)</code>","text":"<p>Convert an Apache Avro into an Apache Iceberg schema equivalent.</p> <p>This expects to have field id's to be encoded in the Avro schema:</p> <pre><code>{\n    \"type\": \"record\",\n    \"name\": \"manifest_file\",\n    \"fields\": [\n        {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n        {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n    ]\n}\n</code></pre> Example <p>This converts an Avro schema into an Iceberg schema:</p> <p>avro_schema = AvroSchemaConversion().avro_to_iceberg({ ...     \"type\": \"record\", ...     \"name\": \"manifest_file\", ...     \"fields\": [ ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500}, ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501} ...     ] ... }) iceberg_schema = Schema( ...     NestedField( ...         field_id=500, name=\"manifest_path\", field_type=StringType(), required=False, doc=\"Location URI with FS scheme\" ...     ), ...     NestedField( ...         field_id=501, name=\"manifest_length\", field_type=LongType(), required=False, doc=\"Total file size in bytes\" ...     ), ...     schema_id=1 ... ) avro_schema == iceberg_schema True</p> <p>Parameters:</p> Name Type Description Default <code>avro_schema</code> <code>Dict[str, Any]</code> <p>The JSON decoded Avro schema.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Equivalent Iceberg schema.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def avro_to_iceberg(self, avro_schema: Dict[str, Any]) -&gt; Schema:\n    \"\"\"Convert an Apache Avro into an Apache Iceberg schema equivalent.\n\n    This expects to have field id's to be encoded in the Avro schema:\n\n        {\n            \"type\": \"record\",\n            \"name\": \"manifest_file\",\n            \"fields\": [\n                {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n                {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n            ]\n        }\n\n    Example:\n        This converts an Avro schema into an Iceberg schema:\n\n        &gt;&gt;&gt; avro_schema = AvroSchemaConversion().avro_to_iceberg({\n        ...     \"type\": \"record\",\n        ...     \"name\": \"manifest_file\",\n        ...     \"fields\": [\n        ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n        ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n        ...     ]\n        ... })\n        &gt;&gt;&gt; iceberg_schema = Schema(\n        ...     NestedField(\n        ...         field_id=500, name=\"manifest_path\", field_type=StringType(), required=False, doc=\"Location URI with FS scheme\"\n        ...     ),\n        ...     NestedField(\n        ...         field_id=501, name=\"manifest_length\", field_type=LongType(), required=False, doc=\"Total file size in bytes\"\n        ...     ),\n        ...     schema_id=1\n        ... )\n        &gt;&gt;&gt; avro_schema == iceberg_schema\n        True\n\n    Args:\n        avro_schema (Dict[str, Any]): The JSON decoded Avro schema.\n\n    Returns:\n        Equivalent Iceberg schema.\n    \"\"\"\n    return Schema(*[self._convert_field(field) for field in avro_schema[\"fields\"]], schema_id=1)\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion.iceberg_to_avro","title":"<code>iceberg_to_avro(schema, schema_name=None)</code>","text":"<p>Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def iceberg_to_avro(self, schema: Schema, schema_name: Optional[str] = None) -&gt; AvroType:\n    \"\"\"Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.\"\"\"\n    return visit(schema, ConvertSchemaToAvro(schema_name))\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.ConvertSchemaToAvro","title":"<code>ConvertSchemaToAvro</code>","text":"<p>               Bases: <code>SchemaVisitorPerPrimitiveType[AvroType]</code></p> <p>Convert an Iceberg schema to an Avro schema.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>class ConvertSchemaToAvro(SchemaVisitorPerPrimitiveType[AvroType]):\n    \"\"\"Convert an Iceberg schema to an Avro schema.\"\"\"\n\n    schema_name: Optional[str]\n    last_list_field_id: int\n    last_map_key_field_id: int\n    last_map_value_field_id: int\n\n    def __init__(self, schema_name: Optional[str]) -&gt; None:\n        \"\"\"Convert an Iceberg schema to an Avro schema.\n\n        Args:\n            schema_name: The name of the root record.\n        \"\"\"\n        self.schema_name = schema_name\n\n    def schema(self, schema: Schema, struct_result: AvroType) -&gt; AvroType:\n        if isinstance(struct_result, dict) and self.schema_name is not None:\n            struct_result[\"name\"] = self.schema_name\n        return struct_result\n\n    def before_list_element(self, element: NestedField) -&gt; None:\n        self.last_list_field_id = element.field_id\n\n    def before_map_key(self, key: NestedField) -&gt; None:\n        self.last_map_key_field_id = key.field_id\n\n    def before_map_value(self, value: NestedField) -&gt; None:\n        self.last_map_value_field_id = value.field_id\n\n    def struct(self, struct: StructType, field_results: List[AvroType]) -&gt; AvroType:\n        return {\"type\": \"record\", \"fields\": field_results}\n\n    def field(self, field: NestedField, field_result: AvroType) -&gt; AvroType:\n        # Sets the schema name\n        if isinstance(field_result, dict) and field_result.get(\"type\") == \"record\":\n            field_result[\"name\"] = f\"r{field.field_id}\"\n\n        result = {\n            \"name\": field.name,\n            \"field-id\": field.field_id,\n            \"type\": field_result if field.required else [\"null\", field_result],\n        }\n\n        if field.write_default is not None:\n            result[\"default\"] = field.write_default  # type: ignore\n        elif field.optional:\n            result[\"default\"] = None\n\n        if field.doc is not None:\n            result[\"doc\"] = field.doc\n\n        return result\n\n    def list(self, list_type: ListType, element_result: AvroType) -&gt; AvroType:\n        # Sets the schema name in case of a record\n        if isinstance(element_result, dict) and element_result.get(\"type\") == \"record\":\n            element_result[\"name\"] = f\"r{self.last_list_field_id}\"\n        return {\"type\": \"array\", \"element-id\": self.last_list_field_id, \"items\": element_result}\n\n    def map(self, map_type: MapType, key_result: AvroType, value_result: AvroType) -&gt; AvroType:\n        if isinstance(key_result, StringType):\n            # Avro Maps does not support other keys than a String,\n            return {\n                \"type\": \"map\",\n                \"values\": value_result,\n                \"key-id\": self.last_map_key_field_id,\n                \"value-id\": self.last_map_value_field_id,\n            }\n        else:\n            # Creates a logical map that's a list of schema's\n            # binary compatible\n            return {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"record\",\n                    \"name\": f\"k{self.last_map_key_field_id}_v{self.last_map_value_field_id}\",\n                    \"fields\": [\n                        {\"name\": \"key\", \"type\": key_result, \"field-id\": self.last_map_key_field_id},\n                        {\"name\": \"value\", \"type\": value_result, \"field-id\": self.last_map_value_field_id},\n                    ],\n                },\n                \"logicalType\": \"map\",\n            }\n\n    def visit_fixed(self, fixed_type: FixedType) -&gt; AvroType:\n        return {\"type\": \"fixed\", \"size\": len(fixed_type), \"name\": f\"fixed_{len(fixed_type)}\"}\n\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; AvroType:\n        return {\n            \"type\": \"fixed\",\n            \"size\": decimal_required_bytes(decimal_type.precision),\n            \"logicalType\": \"decimal\",\n            \"precision\": decimal_type.precision,\n            \"scale\": decimal_type.scale,\n            \"name\": f\"decimal_{decimal_type.precision}_{decimal_type.scale}\",\n        }\n\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; AvroType:\n        return \"boolean\"\n\n    def visit_integer(self, integer_type: IntegerType) -&gt; AvroType:\n        return \"int\"\n\n    def visit_long(self, long_type: LongType) -&gt; AvroType:\n        return \"long\"\n\n    def visit_float(self, float_type: FloatType) -&gt; AvroType:\n        return \"float\"\n\n    def visit_double(self, double_type: DoubleType) -&gt; AvroType:\n        return \"double\"\n\n    def visit_date(self, date_type: DateType) -&gt; AvroType:\n        return {\"type\": \"int\", \"logicalType\": \"date\"}\n\n    def visit_time(self, time_type: TimeType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"time-micros\"}\n\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; AvroType:\n        # Iceberg only supports micro's\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": False}\n\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; AvroType:\n        # Iceberg only supports micro's\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": True}\n\n    def visit_string(self, string_type: StringType) -&gt; AvroType:\n        return \"string\"\n\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; AvroType:\n        return {\"type\": \"fixed\", \"size\": 16, \"logicalType\": \"uuid\", \"name\": \"uuid_fixed\"}\n\n    def visit_binary(self, binary_type: BinaryType) -&gt; AvroType:\n        return \"bytes\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.ConvertSchemaToAvro.__init__","title":"<code>__init__(schema_name)</code>","text":"<p>Convert an Iceberg schema to an Avro schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>Optional[str]</code> <p>The name of the root record.</p> required Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def __init__(self, schema_name: Optional[str]) -&gt; None:\n    \"\"\"Convert an Iceberg schema to an Avro schema.\n\n    Args:\n        schema_name: The name of the root record.\n    \"\"\"\n    self.schema_name = schema_name\n</code></pre>"},{"location":"reference/pyiceberg/utils/singleton/","title":"singleton","text":"<p>This is a singleton metaclass that can be used to cache and reuse existing objects.</p> <p>In the Iceberg codebase we have a lot of objects that are stateless (for example Types such as StringType, BooleanType etc). FixedTypes have arguments (eg. Fixed[22]) that we also make part of the key when caching the newly created object.</p> <p>The Singleton uses a metaclass which essentially defines a new type. When the Type gets created, it will first evaluate the <code>__call__</code> method with all the arguments. If we already initialized a class earlier, we'll just return it.</p> <p>More information on metaclasses: https://docs.python.org/3/reference/datamodel.html#metaclasses</p>"},{"location":"reference/pyiceberg/utils/singleton/#pyiceberg.utils.singleton.Singleton","title":"<code>Singleton</code>","text":"Source code in <code>pyiceberg/utils/singleton.py</code> <pre><code>class Singleton:\n    _instances: ClassVar[Dict] = {}  # type: ignore\n\n    def __new__(cls, *args, **kwargs):  # type: ignore\n        key = (cls, tuple(args), _convert_to_hashable_type(kwargs))\n        if key not in cls._instances:\n            cls._instances[key] = super().__new__(cls)\n        return cls._instances[key]\n\n    def __deepcopy__(self, memo: Dict[int, Any]) -&gt; Any:\n        \"\"\"\n        Prevent deep copy operations for singletons.\n\n        The IcebergRootModel inherits from Pydantic RootModel,\n        which has its own implementation of deepcopy. When deepcopy\n        runs, it calls the RootModel __deepcopy__ method and ignores\n        that it's a Singleton. To handle this, the order of inheritance\n        is adjusted and a __deepcopy__ method is implemented for\n        singletons that simply returns itself.\n        \"\"\"\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/singleton/#pyiceberg.utils.singleton.Singleton.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Prevent deep copy operations for singletons.</p> <p>The IcebergRootModel inherits from Pydantic RootModel, which has its own implementation of deepcopy. When deepcopy runs, it calls the RootModel deepcopy method and ignores that it's a Singleton. To handle this, the order of inheritance is adjusted and a deepcopy method is implemented for singletons that simply returns itself.</p> Source code in <code>pyiceberg/utils/singleton.py</code> <pre><code>def __deepcopy__(self, memo: Dict[int, Any]) -&gt; Any:\n    \"\"\"\n    Prevent deep copy operations for singletons.\n\n    The IcebergRootModel inherits from Pydantic RootModel,\n    which has its own implementation of deepcopy. When deepcopy\n    runs, it calls the RootModel __deepcopy__ method and ignores\n    that it's a Singleton. To handle this, the order of inheritance\n    is adjusted and a __deepcopy__ method is implemented for\n    singletons that simply returns itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/truncate/","title":"truncate","text":""}]}